p_free_buffs = ( _cpp_buff_0 *)&pfile->free_buffs; 
result = ( _cpp_buff_0 *)&v6[( min_size + 7) & 0xFFFFFFFFFFFFFFF8LL]; 
v9 = ( tokenrun_0 *)xmalloc( 0x20uLL); 
v11 = ( cpp_token_0 *)xmalloc( 0x1770uLL); 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
v26 = gen_rtx_MEM( ( machine_mode)*( ( unsigned __int8 *)v26 + 2), fixed); 
tree_node *elements; // rdi 
fancy_abort( ( const char *)&a, 9320, "add_byte_size_attribute"); 
elements = result->vector.elements; 
if ( elements ) 
if ( host_integerp( elements, 1) ) 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
fancy_abort( ( const char *)&a, 8599, "add_data_member_location_attribute"); 
if ( v10 == reverse_condition( ( rtx_code)*( _WORD *)v8) 
*hv = ( __PAIR128__( h2, l2) + *( _OWORD *)&l1) >> 64; 
return ( ( ( ( __PAIR128__( h2, l2) + *( _OWORD *)&l1) >> 64) ^ v6) & ~( v6 ^ h2)) >> 63; 
v15 = label_num_59; 
v3 = label_58; 
++label_num_59; 
sprintf( label_58, "*.%s%u", "LCFI", v15); 
assemble_name( asm_out_file, label_58); 
insn[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)v2, ( __m128i)( unsigned __int64)before); 
fancy_abort( ( const char *)&a, 8989, "add_location_or_const_value_attribute"); 
if ( *( ( _DWORD *)&mode_class_0 + v20) != 2 ) 
fancy_abort( ( const char *)&a, 8722, "add_const_value_attribute"); 
fancy_abort( ( const char *)&a, 8713, "add_const_value_attribute"); 
fancy_abort( ( const char *)&a, 8757, "add_const_value_attribute"); 
fancy_abort( ( const char *)&a, 9026, "add_location_or_const_value_attribute"); 
rtx v10; // r14 
rtx v14; // r12 
rtx *slot_with_hash; // rax 
rtx *v24; // r15 
rtx v25; // rax 
rtx v31; // rax 
rtx v35; // rax 
rtx addra; // [rsp+8h] [rbp-50h] 
rtx v10; // rax 
v10 = gen_rtx_fmt_e0( MEM, v5, rtx); 
*( _QWORD *)&v10[1] = 0LL; 
memref = v10; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 8 | *( ( _BYTE *)v10 + 3) & 0xF7; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 8 | *( ( _BYTE *)v10 + 3) & 0xF7; 
*( ( _BYTE *)memref + 3) = ( _BYTE)v10; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 0x10 | ( unsigned __int8)v10 & 0xEF; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 0x10 | ( unsigned __int8)v10 & 0xEF; 
*( ( _BYTE *)memref + 3) = ( _BYTE)v10; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 0x80 | ( unsigned __int8)v10 & 0x7F; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 0x80 | ( unsigned __int8)v10 & 0x7F; 
*( ( _BYTE *)memref + 3) = ( _BYTE)v10; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 4 | ( unsigned __int8)v10 & 0xFB; 
LOBYTE( v10) = *( ( _BYTE *)v6 + 3) & 4 | ( unsigned __int8)v10 & 0xFB; 
*( ( _BYTE *)memref + 3) = ( _BYTE)v10; 
( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 
return gen_rtx_fmt_ee( EXPR_LIST, ( machine_mode)kind, val, next); 
*(  struct rtx_def *)&result->fld[0].rtwint = ( rtx_def)v4; 
*(  struct rtx_def *)&result->fld[0].rtwint = ( rtx_def)v4; 
rtx v20; // rax 
rtx v29; // rax 
v6 = convert_to_mode( ( machine_mode)v8, v6, 1); 
v20 = force_operand( v19, 0LL), 
v6 = v20, 
v3 = gen_reg_rtx( ( machine_mode)v11); 
( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 
emit_cmp_and_jump_insns( v37, v6, GEU, 0LL, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 1, v36); 
( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 
v27 = expand_divmod( 0, TRUNC_DIV_EXPR, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v25, v26, 0LL, 1); 
v3 = expand_mult( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v27, v28, 0LL, 1); 
v21 = *( ( unsigned __int16 *)&insn_data_0[1234].operand[1] + 8); 
predicate = insn_data_0[1234].operand[1].predicate; 
v12 = assign_stack_local( ( machine_mode)v8, v10, -( v7 > v9)); 
v12 = assign_stack_local( ( machine_mode)v8, v10, -( v10 != v9)); 
v18 = adjust_address_1( v12, ( machine_mode)v8, 0LL, 0, 1); 
result = simplify_subreg( v5, v3.rtx, ( machine_mode)*( unsigned __int8 *)( v3.rtwint + 2), v4); 
rtx base_term; // rax 
base_term = find_base_term( addr_0); 
if ( base_term ) 
if ( *( _WORD *)base_term == 67 || ( *( _DWORD *)base_term & 0x400FFFF) == 67108932 ) 
if ( *( _WORD *)base_term == 67 || ( *( _DWORD *)base_term & 0x400FFFF) == 67108932 ) 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( ( unsigned __int8 *)mem + 2)) ) 
result = size_11; 
if ( size_11 < 0 ) 
size_11 = v1; 
size_11 = 2 * v1; 
return size_11; 
if ( v15 && ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v3) - 5) >= 2 ) 
while ( !ix86_hard_regno_mode_ok( i, ( machine_mode)v12) ) 
if ( have_insn_for( SET, ( machine_mode)v12) ) 
while ( !ix86_hard_regno_mode_ok( i, ( machine_mode)v13) ) 
if ( have_insn_for( SET, ( machine_mode)v13) ) 
while ( !ix86_hard_regno_mode_ok( i, ( machine_mode)v14) ) 
if ( have_insn_for( SET, ( machine_mode)v14) ) 
v10 = size_11; 
rtx v19; // rax 
v19 = gen_rtx_fmt_E( PARALLEL, VOIDmode, v18); 
rtwint = ( int *)v19->fld[0].rtwint; 
rtx = v19; 
while ( ( change_t_0 *)v11 != v10 ); 
v7 = legitimate_address_p( ( machine_mode)*( ( unsigned __int8 *)object + 2), v6, 0); 
old_reg = gen_reg_rtx( ( machine_mode)v4); 
rtx v5; // rbx 
rtx v6; // r13 
rtx result; // rax 
if ( ( *( &mode_class_0 + *( ( unsigned __int8 *)x + 2)) & 0xFFFFFFFB) == 2 ) 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)x + 2)) == 8 ) 
v5 = expand_compound_operation( rtx); 
v6 = expand_compound_operation( v4); 
v7 = *( _WORD *)v5; 
if ( ( _WORD)v7 != *( _WORD *)v6 ) 
if ( *( _BYTE *)( v5->fld[0].rtwint + 2) != *( _BYTE *)( v6->fld[0].rtwint + 2) ) 
if ( *( _BYTE *)( v5->fld[0].rtwint + 2) != *( _BYTE *)( v6->fld[0].rtwint + 2) ) 
if ( v5[1] != v6[1] ) 
result = size_5; 
if ( size_5 < 0 ) 
size_5 = 0; 
if ( ix86_hard_regno_mode_ok( i, ( machine_mode)v2) ) 
while ( !ix86_hard_regno_mode_ok( i, ( machine_mode)v6) ) 
if ( have_insn_for( SET, ( machine_mode)v6) ) 
while ( !ix86_hard_regno_mode_ok( i, ( machine_mode)v7) ) 
if ( have_insn_for( SET, ( machine_mode)v7) ) 
while ( !ix86_hard_regno_mode_ok( i, ( machine_mode)v8) ) 
if ( have_insn_for( SET, ( machine_mode)v8) ) 
v5 = size_5; 
if ( size_5 % ( int)( mode_alignment >> 3) ) 
v5 = ( mode_alignment >> 3) * ( ( int)( size_5 + ( mode_alignment >> 3) - 1) / ( int)( mode_alignment >> 3)); 
size_5 = mode_size[v3] + v5; 
if ( ( sch_istable[v7] & 4) == 0 ) 
while ( ( sch_istable[v9] & 4) != 0 ); 
rtx start; // rbp 
rtx nonnote_insn; // rax 
rtx v9; // rax 
start = loop->start; 
nonnote_insn = prev_nonnote_insn( end); 
v6 = nonnote_insn; 
if ( *( _WORD *)nonnote_insn == 35 ) 
v6 = (  struct rtx_def *)nonnote_insn[1]; 
if ( start != rtx ) 
if ( start == v8 ) 
v9 = start; 
v9 = start; 
v9 = v9[1].fld[0].rtx; 
v9 = v9[1].fld[0].rtx; 
if ( v9 == rtx ) 
return *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)op + 2)) == 2; 
return *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)op + 2)) == 2; 
*( _OWORD *)&head->first = 0LL; 
if ( *( _OWORD *)v6->bits != 0LL ) 
*( _OWORD *)&to->first = 0LL; 
*( _OWORD *)( object_base + 24) = 0LL; 
*( _OWORD *)object_base = v7; 
*( _OWORD *)( object_base + 24) = 0LL; 
*( _OWORD *)object_base = 0LL; 
*( _OWORD *)&head->first = 0LL; 
*( _OWORD *)&to->first = 0LL; 
*( _OWORD *)&to->first = 0LL; 
*( _OWORD *)( object_base + 24) = 0LL; 
*( _OWORD *)object_base = 0LL; 
*( _OWORD *)&to->first = 0LL; 
*( _OWORD *)( object_base + 24) = 0LL; 
*( _OWORD *)object_base = 0LL; 
induction_1 *biv; // rbx 
induction_1 *biv; // rbx 
biv = bl_0->biv; 
if ( biv ) 
for ( i = *( ( _BYTE *)biv + 100); ( i & 8) != 0; i = *( ( _BYTE *)biv + 100) ) 
for ( i = *( ( _BYTE *)biv + 100); ( i & 8) != 0; i = *( ( _BYTE *)biv + 100) ) 
if ( biv->mult_val != const_int_rtx[65] || ( i & 0x20) != 0 ) 
mode = biv->mode; 
add_val = biv->add_val; 
biv = biv->next_iv; 
biv = biv->next_iv; 
if ( biv ) 
biv = biv->next_iv; 
biv = biv->next_iv; 
if ( !biv ) 
*( _OWORD *)&result->common.chain = 0LL; 
v7 = ( tree_node *)ggc_alloc( v6); 
v7 = ( tree_node *)ggc_alloc( v20); 
v15 = ( tree_node *)ggc_alloc( 0xD0uLL); 
v16 = ( tree_node *)ggc_alloc( 0xA8uLL); 
v7 = ( tree_node *)ggc_alloc( v19); 
v6 = ( tree_node *)ggc_alloc( v5); 
v4 = ( tree_node *)ggc_alloc( v3); 
v7 = ( tree_node *)ggc_alloc( 0x30uLL); 
*( ( _OWORD *)&v7->block.common + 1) = 0LL; 
*( _OWORD *)&v7->common.chain = 0LL; 
*( _OWORD *)&v7->block.vars = 0LL; 
v8 = ( tree_node *)ggc_alloc( 0xD0uLL); 
v13 = ( tree_node *)ggc_alloc( 0xA8uLL); 
v4 = ( tree_node *)ggc_alloc( v3); 
v4 = ( tree_node *)ggc_alloc( v14); 
v13 = ( tree_node *)ggc_alloc( 0xD0uLL); 
v10 = ( tree_node *)ggc_alloc( 0xA8uLL); 
v4 = ( tree_node *)ggc_alloc( v9); 
*( _OWORD *)&result->block.fragment_chain = 0LL; 
v4 = ( tree_node *)ggc_alloc( v3); 
v4 = ( tree_node *)ggc_alloc( v15); 
v11 = ( tree_node *)ggc_alloc( 0xD0uLL); 
v12 = ( tree_node *)ggc_alloc( 0xA8uLL); 
v4 = ( tree_node *)ggc_alloc( v10); 
rtx last_value; // rax 
rtx v21; // rbp 
rtx v25; // rax 
rtx v27; // rax 
rtx pinsn[8]; // [rsp+8h] [rbp-40h] BYREF 
pinsn[0] = a2; 
v8 = ( unsigned __int16 *)pinsn[0][1]; 
pinsn[0] = ( rtx)v8; 
return general_operand( op, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4)); 
return general_operand( op, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4)); 
v8 = ( tree_node *)v10[6]; 
while ( v1 != ( change_t_0 *)v2 ); 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)&v27[2 * v30 + 2] + 2LL)); 
|| !fixed_regs[rtuint] && *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)v2 + 2)) != 4) ) 
|| insn_data_0[rtint].n_dups > 0) ) 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)x + 2), v5, v6); 
if ( !base_alias_check( rtx, mem_addr, ( machine_mode)*( ( unsigned __int8 *)x + 2), mem_mode) ) 
rtx v9; // r12 
rtx nonnote_insn; // rax 
rtx v18; // rax 
rtx x; // [rsp+8h] [rbp-50h] 
rtx xa; // [rsp+8h] [rbp-50h] 
v9 = ( rtx)cond[1]; 
if ( rtx_class[v10] == 60 && const_tiny_rtx[0][*( ( unsigned __int8 *)rtx + 2)] == v9 && rtx != want_reg ) 
v9 = ( rtx)rtx[1]; 
nonnote_insn = prev_nonnote_insn( v5); 
v5 = nonnote_insn; 
if ( !nonnote_insn || *( _WORD *)nonnote_insn != 32 ) 
rtx v6; // r12 
rtx v7; // rax 
rtx v11; // rax 
if ( !rtx_equal_p( rtx, memref->fld[0].rtx) || ( v5 = *( ( unsigned __int8 *)memref + 2), v6 = memref, v3 != ( _DWORD)v5) ) 
v7 = gen_rtx_fmt_e0( MEM, v3, rtx); 
*( _QWORD *)&v7[1] = 0LL; 
v6 = v7; 
v6 = v7; 
v5 = *( ( unsigned __int8 *)v7 + 2); 
LOBYTE( v7) = *( ( _BYTE *)memref + 3) & 8 | *( ( _BYTE *)v7 + 3) & 0xF7; 
LOBYTE( v7) = *( ( _BYTE *)memref + 3) & 8 | *( ( _BYTE *)v7 + 3) & 0xF7; 
*( ( _BYTE *)v6 + 3) = ( _BYTE)v7; 
*( ( _BYTE *)v6 + 3) = ( _BYTE)v7; 
LOBYTE( v7) = *( ( _BYTE *)memref + 3) & 0x10 | ( unsigned __int8)v7 & 0xEF; 
if ( ( sch_istable[( unsigned __int8)v12] & 4) != 0 && !v11[1] ) 
rtx v12; // rax 
rtx v18; // rsi 
rtx v20; // rax 
rtx insna; // [rsp+10h] [rbp-C0h] 
n_operands = recog_data_0.n_operands; 
n_outputs = recog_data_0.n_operands - asm_operand_n_inputs; 
if ( recog_data_0.n_operands > 0 ) 
v7 = recog_data_0.operand[v6]; 
recog_data_0.operand[v6++] = rtx; 
v12 = recog_data_0.operand[v10]; 
v12 = recog_data_0.operand[v10]; 
if ( *( _WORD *)v12 != 61 ) 
rtuint = v12->fld[0].rtuint; 
v12 = recog_data_0.operand[v10]; 
induction_1 *v24; // r15 
induction_1 *v24; // r15 
rtx src_reg; // [rsp+18h] [rbp-60h] BYREF 
rtx add_val; // [rsp+20h] [rbp-58h] BYREF 
rtx mult_val; // [rsp+28h] [rbp-50h] BYREF 
rtx ext_val; // [rsp+30h] [rbp-48h] BYREF 
rtx last_consec_insn[8]; // [rsp+38h] [rbp-40h] BYREF 
if ( general_induction_var( loop, *( rtx *)&v19[1], &src_reg, &add_val, &mult_val, &ext_val, 0, &benefit, VOIDmode) 
if ( general_induction_var( loop, *( rtx *)&v19[1], &src_reg, &add_val, &mult_val, &ext_val, 0, &benefit, VOIDmode) 
mark_reload_reg_in_use( *( _DWORD *)( v15 + 8), v17, v16, *( machine_mode *)( v13 - 104)); 
v10 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) < 2) + 1; 
v23 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v20) - 5) < 2) + 1; 
tree v21; // r12 
tree v27; // rax 
tree v29; // rax 
tree v38; // rdi 
mode_alignment = get_mode_alignment( ( machine_mode)*( ( unsigned __int8 *)object + 2)); 
rtx v55; // [rsp+10h] [rbp-88h] 
rtx x; // [rsp+28h] [rbp-70h] 
rtx in; // [rsp+48h] [rbp-50h] 
x = *loc; 
v41 = expand_field_assignment( x); 
if ( rtuint <= 0x34 && !ix86_hard_regno_mode_ok( rtuint, ( machine_mode)*( ( unsigned __int8 *)v45 + 2)) ) 
rtwint = ( int *)x->fld[0].rtwint; 
v55 = *( rtx *)&rtwint[2 * ( int)v11 + 2]; 
if ( *( _WORD *)v55 != 47 ) 
v36 = expand_field_assignment( v55); 
if ( v49 <= 0x34 && !ix86_hard_regno_mode_ok( v49, ( machine_mode)*( ( unsigned __int8 *)v40 + 2)) ) 
rtx v131; // r14 
rtx v132; // r13 
rtx v138; // rax 
v11 = lang_hooks_0.expand_constant( v3); 
rtx *true_reg; // rbx 
rtx v10; // r13 
rtx regno_note; // r14 
rtx v13; // rax 
rtx *rtwint; // r9 
rtx *v37; // rax 
rtx *src2; // [rsp+0h] [rbp-48h] 
rtx *pata; // [rsp+8h] [rbp-40h] 
true_reg = get_true_reg( ( rtx *)pat_src->fld); 
src2 = get_true_reg( v4); 
if ( *( _WORD *)*true_reg == 61 ) 
rtx v18; // rax 
reg_set_0 *v32; // rdx 
reg_set_0 *v32; // rdx 
rtx insn; // rcx 
reg_set_0 *v36; // rdx 
reg_set_0 *v36; // rdx 
rtx v38; // rcx 
v32 = reg_set_table[rtuint]; 
if ( v32 ) 
insn = v32->insn; 
insn = v32->insn; 
v32 = v32->next; 
v32 = v32->next; 
v35 = bmap[*( int *)( v33->data.l[insn->fld[0].rtint] + 88)]; 
while ( v32 ); 
v36 = reg_set_table[rtuint]; 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v14) - 5) > 1 ) 
rtx v30; // rax 
rtx *add_vala; // [rsp+AAh] [rbp-58h] 
rtx v40; // [rsp+B2h] [rbp-50h] 
rtx src_rega[2]; // [rsp+BAh] [rbp-48h] BYREF 
src_rega[0] = src_reg; 
add_vala = add_val; 
v40 = v15; 
src_rega, 
add_vala, 
|| ( v30 = find_reg_note( p, REG_EQUAL, 0LL)) != 0LL 
v30->fld[0].rtx, 
for ( k = ( tree_node *)j[4]; k; k = ( tree_node *)j[4] ) 
for ( k = ( tree_node *)j[4]; k; k = ( tree_node *)j[4] ) 
rtx v20; // rsi 
rtx v21; // rdi 
n_operands = recog_data_0.n_operands; 
if ( !n_operands || !recog_data_0.n_alternatives || n_operands <= 0 ) 
*( const char **)( ( char *)&constraints[-1] + ( unsigned int)( 8 * n_operands)) = *( const char **)( ( char *)&recog_data_0.constraints[-1] 
qmemcpy( constraints, recog_data_0.constraints, 8LL * ( ( unsigned int)( 8 * n_operands - 1) >> 3)); 
rtx = recog_data_0.operand[v6]; 
( machine_mode)*( unsigned __int8 *)( v28.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)rtx + 2)); 
v20 = recog_data_0.operand[v6]; 
v20 = recog_data_0.operand[v6]; 
v21 = recog_data_0.operand[v18]; 
if ( *( ( _DWORD *)&mode_class_0 + ( int)mode) != 1 ) 
if ( *( ( _DWORD *)&mode_class_0 + ( int)mode) != 1 ) 
if ( *( ( _DWORD *)&mode_class_0 + ( int)oldmode) == 1 
v85 = *( ( _DWORD *)&mode_class_0 + v87); 
frome = *( ( _DWORD *)&mode_class_0 + ( unsigned __int8)v6) == 2; 
v83 = *( ( _DWORD *)&mode_class_0 + ( unsigned __int8)v6); 
v8 = gen_lowpart( ( machine_mode)v87, ( rtx)v7); 
v8 = simplify_gen_subreg( ( machine_mode)v87, v8, ( machine_mode)*( ( unsigned __int8 *)v8 + 2), 0); 
v8 = simplify_gen_subreg( ( machine_mode)v87, v8, ( machine_mode)*( ( unsigned __int8 *)v8 + 2), 0); 
to = simplify_gen_subreg( ( machine_mode)v6, to, ( machine_mode)*( ( unsigned __int8 *)to + 2), 0); 
to = simplify_gen_subreg( ( machine_mode)v6, to, ( machine_mode)*( ( unsigned __int8 *)to + 2), 0); 
v36 = can_extend_p( ( machine_mode)v87, ( machine_mode)v6, unsignedp); 
v36 = can_extend_p( ( machine_mode)v87, ( machine_mode)v6, unsignedp); 
|| ( v41 = can_extend_p( ( machine_mode)v87, *(short *)0xmode, unsignedp), 
if ( *( ( _DWORD *)&mode_class_0 + ( int)mode) != 1 ) 
if ( *( ( _DWORD *)&mode_class_0 + ( int)mode) != 1 ) 
if ( *( ( _DWORD *)&mode_class_0 + v5) == 1 
v4 = gen_reg_rtx( ( machine_mode)v2); 
copy_to_mode_reg_0( ( machine_mode)x, v3); 
( tree_code)*( ( unsigned __int8 *)&exp->block.common + 16), 
return build1( ( tree_code)*( ( unsigned __int8 *)&exp->block.common + 16), exp->common.type, v5); 
rtx v20; // r15 
rtx v30; // r15 
rtx v35; // rax 
rtx v40; // rax 
rtx v50; // rax 
rtx sequence[3]; // [rsp+10h] [rbp-58h] 
v20 = 0LL; 
v20 = ( rtx)v3[2]; 
if ( ( *( _DWORD *)v20->fld[0].rtwint & 0x4000FFFF) != 1073741885 ) 
if ( volatile_refs_p( *( rtx *)&v20[1]) ) 
|| ( v30 = ( rtx)rtx[2], *( _WORD *)v30 != 47) && ( v30 = single_set_2( rtx, *( rtx *)&rtx[2])) == 0LL ) 
rtx v12; // rbp 
rtx v13; // r14 
rtx v16; // rax 
rtx note; // [rsp+18h] [rbp-40h] BYREF 
note = copy_rtx_and_substitute( rtx, map, 0); 
subst_constants( &note, 0LL, map, 0); 
v12 = note; 
v12 = note; 
v7[3].fld[0].rtwint = ( __int64)note; 
if ( !v12 ) 
v13 = v12; 
v13 = v12; 
v12 = ( rtx)v12[1]; 
v14 = *( ( _BYTE *)v13 + 2); 
rtx v13; // rbp 
rtx v15; // rax 
rtx *insn_map; // rdx 
rtx v17; // rsi 
rtx v18; // rax 
rtx v22; // rcx 
rtx v24; // rax 
rtx v26; // r14 
rtx v27; // rax 
v4 = ( tree_node *)memcpy( v3, node, v2); 
rtx v33; // r12 
rtx v34; // rax 
rtx v40; // rax 
rtx v42; // r12 
rtx v43; // rax 
rtx v57; // rax 
rtx v63; // rax 
v2 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)x + 2)); 
free_buffs = ( _cpp_buff_0 *)&v16[v15]; 
rtx v17; // rdx 
v17 = head; 
v17 = ( rtx)head[1]; 
if ( v17 != bb_note && v17[1].fld[0].rtx != bb_note ) 
if ( v17 != bb_note && v17[1].fld[0].rtx != bb_note ) 
reorder_insns( bb_note, bb_note, v17); 
v21 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v4) - 5) < 2) + 1; 
v12 = new_cselib_val( ++next_unknown_value, ( machine_mode)v6); 
v26 = hash_table_0; 
slot_with_hash = htab_find_slot_with_hash( ( __int64)hash_table_0, ( __int64)v9, v8, create != 0); 
if ( ( *( &mode_class_0 + *( ( unsigned __int8 *)x + 2)) & 0xFFFFFFFB) == 2 
|| *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)x + 2)) == 8 ) 
*htab_find_slot_with_hash( ( __int64)hash_table_0, ( __int64)v3, value, 1) = ( __int64)elt; 
rtvec v19; // rax 
rtvec jc; // [rsp+0h] [rbp-58h] 
v19 = rtvec_alloc( *rtwint); 
rtvec = v19; 
v3->fld[v25].rtwint = ( __int64)v19; 
jc = v19; 
jc = v19; 
memmove( v19->elem, rtwint + 2, 8 * v14); 
rtvec = jc; 
v3 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v6) - 5) < 2) + ( _DWORD)rtuint; 
v19 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v18) - 5) < 2) + 1; 
v29 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v26) - 5) < 2) + 1; 
clear_reload_reg_in_use( v4, *( ( _DWORD *)v2 + 18), *( ( reload_type *)v2 + 23), *( ( machine_mode *)v2 + 7)); 
clear_reload_reg_in_use( v4, *( ( _DWORD *)v2 + 18), *( ( reload_type *)v2 + 23), *( ( machine_mode *)v2 + 7)); 
fprintf( file, off_6B8A7E, ( unsigned int)( v6 + v4 + ( i->indx << 7))); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x748370); 
v171 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x748360); 
v172 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6B6D50); 
while ( ( sch_istable[( unsigned __int8)v2[v3]] & 4) != 0 ) 
if ( !strcmp( v2, table_21[i].name) ) 
return table_21[( int)i].number; 
*( _OWORD *)&v8->dw_cfi_oprnd1.dw_cfi_reg_num = 0LL; 
rtx v10; // rsi 
v10 = ( rtx)dead_insn[2]; 
if ( *( _WORD *)v10 == 47 || ( v10 = single_set_2( dead_insn, v10)) != 0LL ) 
if ( *( _WORD *)v10 == 47 || ( v10 = single_set_2( dead_insn, v10)) != 0LL ) 
if ( *( _WORD *)v10 == 47 || ( v10 = single_set_2( dead_insn, v10)) != 0LL ) 
v11.rtwint = ( __int64)v10->fld[0]; 
if ( reg_note && ( v4.rtwint = ( __int64)reg_note->fld[0], *( _WORD *)v4.rtwint == 36) ) 
if ( reg_note && ( v4.rtwint = ( __int64)reg_note->fld[0], *( _WORD *)v4.rtwint == 36) ) 
rtx nonnote_insn; // rax 
nonnote_insn = next_nonnote_insn( ( rtx)v12); 
if ( nonnote_insn && *( _WORD *)nonnote_insn == 33 && ( unsigned __int16)( **( _WORD **)&nonnote_insn[2] - 44) <= 1u ) 
if ( nonnote_insn && *( _WORD *)nonnote_insn == 33 && ( unsigned __int16)( **( _WORD **)&nonnote_insn[2] - 44) <= 1u ) 
if ( nonnote_insn && *( _WORD *)nonnote_insn == 33 && ( unsigned __int16)( **( _WORD **)&nonnote_insn[2] - 44) <= 1u ) 
delete_related_insns( nonnote_insn); 
rtx v33; // rax 
rtx insna; // [rsp+8h] [rbp-50h] BYREF 
rtx v54; // [rsp+18h] [rbp-40h] 
insna = insn; 
v23 = _mm_loadh_ps( ( const double *)&insna); 
v17 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v14) - 5) < 2) + 1; 
v33 = gen_rtx_REG( reg_raw_mode[i], i); 
v54 = v33; 
v54 = v33; 
v29 = _mm_loadh_ps( ( const double *)&insna); 
rtx v12; // rax 
v12 = added_links_insn; 
if ( v12 ) 
for ( i = v12->fld[0].rtint; max_uid_cuid < ( int)i; i = v12->fld[0].rtint ) 
for ( i = v12->fld[0].rtint; max_uid_cuid < ( int)i; i = v12->fld[0].rtint ) 
if ( *( _WORD *)v12 != 32 || **( _WORD **)&v12[2] != 48 ) 
if ( *( _WORD *)v12 != 32 || **( _WORD **)&v12[2] != 48 ) 
v12 = v12[1].fld[0].rtx; 
v12 = v12[1].fld[0].rtx; 
if ( !v12 ) 
rtx v17; // r14 
rtx v19; // r14 
rtx real_insn; // rax 
rtx v30; // rax 
rtx v33; // rdx 
rtx v42; // rax 
rtx v47; // rax 
rtx v56; // rbp 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)v2 + 2)) == 1 && *( _WORD *)newval == 54 ) 
if ( rtwint != trunc_int_for_mode( rtwint, ( machine_mode)*( ( unsigned __int8 *)v2 + 2)) ) 
frees = ( __m128i *)undobuf_0.frees; 
if ( undobuf_0.frees ) 
undobuf_0.frees = undobuf_0.frees->next; 
undobuf_0.frees = undobuf_0.frees->next; 
undos = undobuf_0.undos; 
undobuf_0.undos = (  struct undo *)frees; 
( machine_mode)*( unsigned __int8 *)( v12.rtwint + 2), 
v18 = swap_condition( ( rtx_code)aux[22]); 
v66 = expand_simple_binop( ( machine_mode)*( ( unsigned __int8 *)v56 + 2), LSHIFTRT, v56, v65, 0LL, 1, OPTAB_LIB_WIDEN); 
( machine_mode)*( ( unsigned __int8 *)doloop_regb + 2), 
emit_cmp_and_jump_insns( v68, v70, ( rtx_code)( v69 == 0 ? EQ : LEU), 0LL, v88, 0, op1a); 
( machine_mode)*( ( unsigned __int8 *)doloop_regb + 2), 
fwrite( &unk_69837D, 1uLL, 0x11uLL, outf); 
fprintf( file, off_698152, ( unsigned int)dest->index); 
fprintf( file, &off_698152[1], ( unsigned int)v7++); 
v10 = ( &bitnames_8)[v7++]; 
fprintf( outf, off_698152, ( unsigned int)v5); 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
if ( ( unsigned int)debug_info_level_0 > DINFO_LEVEL_TERSE ) 
if ( ( unsigned int)debug_info_level_0 <= DINFO_LEVEL_TERSE ) 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
&& ( unsigned int)debug_info_level_0 > DINFO_LEVEL_TERSE ) 
fancy_abort( ( const char *)&a, 1309, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1603, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1479, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1549, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1513, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1555, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1490, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1495, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1561, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1528, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1538, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1588, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1457, "dwarf2out_frame_debug_expr"); 
fancy_abort( ( const char *)&a, 1386, "dwarf2out_frame_debug_expr"); 
*( _OWORD *)&v11[1] = 0LL; 
*( _OWORD *)v34 = 0LL; 
*( _OWORD *)&v34[2] = 0LL; 
*( _OWORD *)equot = 0LL; 
rtx v47; // rdi 
rtx v49; // rax 
rtx v51; // rax 
rtx v53; // rax 
rtx v55; // rax 
rtx v56; // rsi 
rtx v62; // rax 
rtvec v64; // rax 
rtx v16; // rax 
rtx v27; // r11 
rtx v32; // rax 
v16 = v3; 
v16 = v3->fld[0].rtx; 
v4 = *( _WORD *)v16; 
if ( *( _WORD *)v16 != 61 ) 
if ( mode_size[*( ( unsigned __int8 *)v3 + 2)] <= mode_size[*( ( unsigned __int8 *)v16 + 2)] 
&& reg_equiv_memory_loc[v16->fld[0].rtuint] ) 
rtint = v16->fld[0].rtint; 
*( _OWORD *)( s + 1) = 0LL; 
*( _OWORD *)rbit = 0LL; 
result[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)( unsigned __int64)after, ( __m128i)( unsigned __int64)v5.rtwint); 
tree v25; // r12 
tree v32; // rax 
tree v34; // rax 
tree v43; // rdi 
v11 = *( ( _DWORD *)&mode_class_0 + ( int)mode); 
mode = *( ( unsigned __int16 *)insn_data_0[1203].operand + 8); 
if ( !insn_data_0[1159].operand->predicate( loc, ( ( BYTE3( target_flags) & 2) != 0) + 4) ) 
v1 = copy_to_mode_reg( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), loc); 
v3[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)( unsigned __int64)v6, ( __m128i)( unsigned __int64)v7.rtwint); 
rtx v3; // r12 
rtx *v7; // r13 
v3 = rtx; 
v3[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)v4, ( __m128i)( unsigned __int64)before); 
v3[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)v4, ( __m128i)( unsigned __int64)before); 
*( _QWORD *)( v4 + 24) = v3; 
*( _QWORD *)( *( _QWORD *)( *( _QWORD *)( v12 + 8) + 8LL * ( **( _DWORD **)( v12 + 8) - 1) + 8) + 24LL) = v3; 
*( _QWORD *)( v10 + 8) = v3; 
*( _QWORD *)v11 = v3; 
v7 = ( rtx *)basic_block_for_insn->data.l[( int)rtuint]; 
if ( v7 ) 
set_block_for_insn( v3, basic_block_for_insn->data.bb[( int)rtuint]); 
if ( *v7 == v3 && *( _WORD *)v3 != 35 && ( *( _WORD *)v3 != 37 || v3[2].fld[0].rtint != -80) ) 
if ( *v7 == v3 && *( _WORD *)v3 != 35 && ( *( _WORD *)v3 != 37 || v3[2].fld[0].rtint != -80) ) 
v3[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)( unsigned __int64)v10, ( __m128i)( unsigned __int64)v11.rtwint); 
label[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)( unsigned __int64)after, ( __m128i)( unsigned __int64)v7.rtwint); 
label[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)v7, ( __m128i)( unsigned __int64)before); 
v30 = gen_reg_rtx( ( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
v10 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)y + 2)); 
v16 = gen_reg_rtx( ( machine_mode)*( unsigned __int8 *)( y->fld[0].rtwint + 2)); 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v3 + 2), v3->fld[0].rtx) 
&& !push_operand( v3, ( machine_mode)*( ( unsigned __int8 *)v3 + 2)) 
result[1] = ( rtx_def)_mm_unpacklo_epi64( ( __m128i)v7, ( __m128i)( unsigned __int64)before); 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)reg + 2)) - 5) <= 1 ) 
induction_1 **p_giv; // r15 
induction_1 **p_giv; // r15 
address = simplify_gen_binary( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v13, address); 
p_giv = &info[0].giv; 
fprintf( v42, "Prefetch insn %i address: ", ( *p_giv)->insn->fld[0].rtuint); 
print_rtl( loop_dump_stream, ( rtx)p_giv[1]); 
fprintf( loop_dump_stream, "%lld", p_giv[2]); 
v48 = ( __int64)p_giv[3]; 
v46 = ( unsigned int)( 100 * *( ( _DWORD *)p_giv + 8)); 
v47 = *( ( unsigned int *)p_giv + 9); 
p_giv += 7; 
v52 = ( unsigned int)( 100 * *( ( _DWORD *)p_giv + 8)); 
v53 = *( ( _DWORD *)p_giv + 9); 
p_giv += 7; 
if ( memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v4 + 2), rtx) ) 
v13 = assign_stack_local( ( machine_mode)v5, mode_size[v5], 0); 
v13 = gen_reg_rtx( ( machine_mode)v5); 
if ( memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v4 + 2), rtx) ) 
if ( memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v4 + 2), v9) ) 
( machine_mode)*( ( unsigned __int8 *)dest_reg + 2), 
v2 = rndprc; 
rndprc = 24; 
rndprc = v2; 
*( _QWORD *)e = *( _QWORD *)XFlittlenan; 
e[4] = XFlittlenan[4]; 
*( _QWORD *)( low + 144) = adjust_address_1( rtl, ( machine_mode)v10, 0LL, 0, 1); 
fancy_abort( ( const char *)&label, 4259, "expand_anon_union_decl"); 
*( _QWORD *)( low + 144) = gen_lowpart_SUBREG( ( machine_mode)v10, rtl); 
v18 = gen_rtx_fmt_e( USE, ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v1->fld[0].rtx); 
v12 = nonzero_bits( v1->fld[0].rtx, ( machine_mode)*( unsigned __int8 *)( v1->fld[0].rtwint + 2)); 
v1 = gen_rtx_fmt_e( ZERO_EXTEND, ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v13.rtx); 
v28 = nonzero_bits( *( rtx *)( v23.rtwint + 8), ( machine_mode)( unsigned __int8)v9); 
v25 = simplify_shift_const( 0LL, ASHIFT, ( machine_mode)v11, v1->fld[0].rtx, v10 - ( v7 + v8)); 
v21 = simplify_shift_const( 0LL, ( rtx_code)( i + 89), ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v25, v24); 
rtx v20; // rbx 
rtx v30; // rax 
rtx v32; // rax 
rtx v37; // rax 
v37 = decl->decl.rtl; 
if ( !v37 ) 
v37 = decl->decl.rtl; 
*( _QWORD *)( v36 + 8LL * *( unsigned int *)( *( _QWORD *)&v37[1] + 8LL)) = decl; 
emit_stack_save( ( save_level)( *( _QWORD *)( v23 + 8) == 0LL), ( rtx *)( v23 + 40), *( rtx *)( v23 + 48)); 
v11 = build( COND_EXPR, ( tree_node *)*( &global_trees + 27), ( __int64)v9, ( __int64)cleanup, v8, v10, v21, v22); 
v5 = ( tree_node *)gen_label_rtx( ); 
v19 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v18); 
v26 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v25); 
rtx v57; // r12 
rtx v58; // r12 
rtx v65; // rbx 
rtx v71; // rbx 
fancy_abort( ( const char *)&label, 2733, "expand_end_loop"); 
rtx v76; // rax 
rtx v78; // r14 
rtx v79; // r13 
( const char *)&label.block.vars + 2); 
v10 = *( ( _DWORD *)&mode_class_0 + v9); 
rtx = gen_rtx_fmt_e( USE, ( machine_mode)*( unsigned __int8 *)( v2.rtwint + 2), *( rtx *)( v2.rtwint + 8)); 
v29 = gen_lowpart_for_combine( ( machine_mode)v25, *( rtx *)&x[1]); 
v11 = ( tree_node *)v8[8]; 
v22 = ( tree_node *)v15[8]; 
v25 = ( tree_node *)v15[9]; 
( machine_mode)*( ( unsigned __int8 *)target + 2), 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)op1 + 2)) != 1 ) 
variant = basic_variant; 
variant = basic_variant; 
variant = negate_variant; 
variant = negate_variant; 
variant = add_variant; 
variant = add_variant; 
v66 = expand_shift_1( ( tree_code)mode, temb, v74, 0LL, 0LL, v75); 
v70 = expand_shift_1( ( tree_code)mode, temb, v72, 0LL, 0LL, v73); 
v12 = expand_shift_1( ( tree_code)mode, v12, v31, 0LL, 0LL, v32); 
v66 = expand_shift_1( ( tree_code)mode, temb, v64, 0LL, 0LL, v65); 
tree v14; // r13 
tree v15; // rax 
tree v19; // rax 
v12 = type_for_mode( ( machine_mode)v11, unsignedp); 
v14 = make_tree( v10, mult); 
v15 = make_tree( v10, x); 
v18 = build( MULT_EXPR, v10, ( __int64)v15, ( __int64)v14, v16, v17, v25, v27); 
v18 = build( MULT_EXPR, v10, ( __int64)v15, ( __int64)v14, v16, v17, v25, v27); 
v19 = fold( v18); 
v22 = build( PLUS_EXPR, v10, ( __int64)v19, ( __int64)tree, v20, v21, v26, v28); 
fancy_abort( ( const char *)&label, 3395, "expand_start_bindings_and_block"); 
if ( *v12 && ( ( v14 = ( tree_node *)*( ( _QWORD *)v13 + 8), v15 = ( tree_node *)*( ( _QWORD *)v13 + 9), v14) || v15) ) 
if ( *v12 && ( ( v14 = ( tree_node *)*( ( _QWORD *)v13 + 8), v15 = ( tree_node *)*( ( _QWORD *)v13 + 9), v14) || v15) ) 
*( _OWORD *)( object_base + 40) = 0LL; 
if ( *v8 && ( ( v10 = ( tree_node *)*( ( _QWORD *)v9 + 8), v11 = ( tree_node *)*( ( _QWORD *)v9 + 9), v10) || v11) ) 
if ( *v8 && ( ( v10 = ( tree_node *)*( ( _QWORD *)v9 + 8), v11 = ( tree_node *)*( ( _QWORD *)v9 + 9), v10) || v11) ) 
if ( elements != ( tree_node *)global_trees ) 
if ( !explained_1 ) 
explained_1 = 1; 
rtx g1_add_val; // [rsp+8h] [rbp-30h] 
g1_add_val = g1->add_val; 
v14 = gen_rtx_fmt_ee( MULT, ( machine_mode)*( ( unsigned __int8 *)g1_add_val + 2), g1_add_val->fld[0].rtx, v22); 
v14 = gen_rtx_fmt_ee( MULT, ( machine_mode)*( ( unsigned __int8 *)g1_add_val + 2), g1_add_val->fld[0].rtx, v22); 
v14 = gen_rtx_fmt_ee( MULT, ( machine_mode)*( ( unsigned __int8 *)g1_add_val + 2), g1_add_val->fld[0].rtx, v22); 
v14 = gen_rtx_fmt_ee( MULT, ( machine_mode)*( ( unsigned __int8 *)v12 + 2), v12, v6); 
( rtx_code)*( _WORD *)ext_dependent, 
( machine_mode)*( ( unsigned __int8 *)ext_dependent + 2), 
if ( insn != recog_data_0.insn || insn[2].fld[0].rtint < 0 ) 
recog_data_0.insn = insn; 
fatal_insn_not_found( insn, "recog.c", 2063, "extract_constrain_insn_cached"); 
v42 = ( tree_node *)target; 
v43 = expand_shift_0( ( tree_code)v15, ( machine_mode)v11, v41, v42, ( rtx)1, v46); 
v19 = immed_double_const( v17, v18, ( machine_mode)*( ( unsigned __int8 *)v11 + 2)); 
( machine_mode)*( ( unsigned __int8 *)v11 + 2), 
v29 = force_reg( ( machine_mode)v15, v11); 
v38 = ( tree_node *)target; 
LODWORD( v32) = ( unsigned int)expand_shift_1( ( tree_code)v30, ( machine_mode)v32, v36, v38, ( rtx)1, v37); 
recog_data_0.n_alternatives = 0; 
*( _WORD *)&recog_data_0.n_operands = 0; 
recog_data_0.insn = 0LL; 
recog_data_0.n_operands = n_operands; 
fatal_insn_not_found( insn, "recog.c", 2139, "extract_insn"); 
recog_data_0.operand, 
recog_data_0.operand_loc, 
recog_data_0.constraints, 
recog_data_0.operand_mode); 
v19 = recog_data_0.constraints[0]; 
recog_data_0.n_alternatives = 1; 
for ( i = *recog_data_0.constraints[0]; *v19; i = *v19 ) 
recog_data_0.n_alternatives = v13; 
if ( recog_data_0.n_alternatives <= 30 ) 
if ( recog_data_0.insn != insn || insn[2].fld[0].rtint < 0 ) 
recog_data_0.insn = insn; 
( machine_mode)*( ( unsigned __int8 *)rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
fancy_abort( ( const char *)&a, 8430, "field_byte_offset"); 
imag = ( tree_node *)*( &global_trees + 17); 
induction_1 *biv; // rax 
induction_1 *biv; // rax 
biv = bl_0->biv; 
if ( *( ( _DWORD *)&mode_class_0 + ( int)biv->mode) != 1 ) 
if ( *( ( _DWORD *)&mode_class_0 + ( int)biv->mode) != 1 ) 
src_reg = biv->src_reg; 
biv = bl_0->biv; 
biv = bl_0->biv; 
src_reg = biv->src_reg; 
v9 = gen_reg_rtx( biv->mode); 
( machine_mode)*( ( unsigned __int8 *)v8 + 2), 
rtx v6; // rax 
v6 = find_base_term( x); 
if ( !v6 ) 
base_term = v6; 
if ( ( unsigned __int16)( *( _WORD *)v6 - 67) > 1u && ( *( _WORD *)v6 != 25 || !*( ( _BYTE *)v6 + 2)) ) 
if ( ( unsigned __int16)( *( _WORD *)v6 - 67) > 1u && ( *( _WORD *)v6 != 25 || !*( ( _BYTE *)v6 + 2)) ) 
if ( ( unsigned __int16)( *( _WORD *)v6 - 67) > 1u && ( *( _WORD *)v6 != 25 || !*( ( _BYTE *)v6 + 2)) ) 
rtx base_value; // rax 
rtx v11; // rax 
base_value = find_base_value( rtx->fld[0].rtx); 
if ( base_value ) 
v1 = base_value; 
v11 = find_base_value( v3); 
if ( v11 ) 
v3 = v11; 
rtx v7; // r13 
v7 = *parg2; 
if ( ( _WORD)v9 == COMPARE && const_int_rtx[64] == v7 ) 
v7 = ( rtx)exp[1]; 
if ( const_tiny_rtx[0][v8] != v7 ) 
v13 = canon_hash( rtx, ( machine_mode)v8); 
rtx v21; // rbx 
rtx v22; // r8 
rtx v27; // r8 
rtx v39; // rax 
rtx v56; // rax 
rtx v17; // rax 
rtx v23; // rdi 
rtx v30; // rax 
rtx v56; // rax 
rtx *v63; // rax 
rtx v66; // rdx 
rtx *v95; // rcx 
rtx v98; // rax 
rtx ina; // [rsp+20h] [rbp-68h] 
if ( ( *( _BYTE *)( *( _QWORD *)&cfun + 425LL) & 1) != 0 && qty_0[v30].n_calls_crossed > 0 ) 
if ( !qty_0[v30].n_calls_crossed ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) > 1 ) 
if ( &unk_9ABB74 == ( _UNKNOWN *)++v16 ) 
while ( &unk_9ABB74 != ( _UNKNOWN *)v16 ); 
n_calls_crossed = qty_0[v30].n_calls_crossed; 
if ( !n_calls_crossed || 4 * n_calls_crossed >= qty_0[v30].n_refs ) 
induction_1 *v12; // rbp 
induction_1 *v12; // rbp 
( machine_mode)*( ( unsigned __int8 *)x + 2)) ) 
v12 = ( induction_1 *)xmalloc( 0xA8uLL); 
v12 = ( induction_1 *)xmalloc( 0xA8uLL); 
v12, 
v12->mem = x; 
v6 = *( int *)( ( char *)&allocno_0->reg + v5); 
v8 = ( char *)allocno_0 + v5; 
+ 8LL * *( int *)( ( char *)&allocno_0->reg + v5)) 
if ( ix86_hard_regno_mode_ok( v15, ( machine_mode)v9) ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v9) - 5) > 1 ) 
v21 = allocno_0; 
v22 = ( char *)allocno_0 + v97; 
v24 = *( HARD_REG_ELT_TYPE *)( ( _BYTE *)&allocno_0->hard_reg_copy_preferences + v97) & ~v13; 
*( HARD_REG_ELT_TYPE *)( ( char *)&allocno_0->hard_reg_copy_preferences + v97) = v24; 
v11 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v9) - 5) < 2) + 1; 
if ( ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v16) - 5) < 2) + 1 + ( unsigned int)v14 > regno ) 
v11 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v10) - 5) < 2) + 1; 
rtx memloc; // rax 
rtx v30; // rdi 
rtx v31; // rax 
rtx v33; // r12 
rtx v46; // rax 
rtx v47; // r12 
rtx v48; // rax 
rtx tem; // [rsp+8h] [rbp-40h] BYREF 
tem = force_const_mem( mode, x); 
rtx = tem->fld[0].rtx; 
v11 = tem; 
fld = tem->fld; 
find_reloads_address( mode, &tem, rtx, ( rtx *)fld, opnum, type, ind_levels, 0LL); 
v20 = force_const_mem( ( machine_mode)*( ( unsigned __int8 *)v11 + 2), *( rtx *)&v11[1]); 
tem = v20; 
rtx = tem->fld[0].rtx; 
fld = tem->fld; 
rtx v14; // r15 
rtx v15; // rax 
rtx v17; // rax 
rtx result; // rax 
rtx tem; // [rsp+18h] [rbp-40h] BYREF 
tem = make_memloc( rtx, v13); 
v14 = tem; 
v14 = tem; 
tem = make_memloc( x->fld[0].rtx, v13); 
v20 = rtx_equal_p( tem, reg_equiv_mem[v21]); 
result = x; 
return result; 
if ( replace_reloads && recog_data_0.operand[opnum] != arg0 ) 
( machine_mode)*( ( unsigned __int8 *)v33 + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
result = gen_lowpart_common( ( machine_mode)*( ( unsigned __int8 *)arg0 + 2), v43); 
( machine_mode)*( unsigned __int8 *)( arg0->fld[0].rtwint + 2)); 
v45 = legitimate_address_p( ( machine_mode)*( ( unsigned __int8 *)result + 2), v44->fld[0].rtx, 1); 
result = gen_lowpart_common( ( machine_mode)*( ( unsigned __int8 *)arg0 + 2), v41); 
return gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)v3, ( machine_mode)*( ( unsigned __int8 *)v3 + 2), replacement, v10); 
return gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)v3, ( machine_mode)*( ( unsigned __int8 *)v3 + 2), replacement, v10); 
( machine_mode)*( unsigned __int8 *)( v7->fld[0].rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)v7 + 2)); 
return gen_rtx_REG( ( machine_mode)*( ( unsigned __int8 *)*loc + 2), rtint + v12); 
( machine_mode)( unsigned __int8)v8, 
rtx *single_use; // rax 
rtx v36; // rcx 
rtx v79; // rax 
rtx v80; // r13 
rtx v90; // rsi 
rtx v93; // rax 
rtx desta; // [rsp+0h] [rbp-58h] 
v7 = ix86_register_move_cost( m1, ( reg_class)v3, v4); 
best_cost = ix86_register_move_cost( m1, ( reg_class)v3, v4); 
while ( ( tree_node *)v8[3] != v3 ) 
v13 = memory_address( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), fixed); 
v14 = gen_rtx_MEM( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v13); 
if ( **( _WORD **)( v7.rtwint + 16) != 54 || memory_address_p( ( machine_mode)*( ( unsigned __int8 *)x + 2), v7.rtx) ) 
rtx *v26; // r10 
rtx v30; // rax 
rtx v31; // r10 
rtx v32; // rbp 
rtx v38; // rax 
rtx v41; // r15 
rtx v50; // rbp 
rtx v53; // r15 
rtx v55; // rdx 
*( _QWORD *)( v11.rtwint + 8) = walk_fixup_memory_subreg( *( rtx *)( v11.rtwint + 8), insn, ( machine_mode)v13, v14); 
rtx seq; // [rsp+8h] [rbp-50h] 
seq = ( rtx)*( ( _QWORD *)v11 + 4); 
seq->fld[v16].rtwint = ( __int64)get_insns( ); 
add_dependence_2( insn, v12->fld[0].rtx, ( reg_note)for_read); 
add_dependence( insn, v7->fld[0].rtx, ( reg_note)v6); 
add_dependence( insn, v9->fld[0].rtx, ( reg_note)v6); 
rtx v57; // rax 
rtx v58; // rdx 
rtx v65; // rax 
rtx *v67; // r12 
rtx const_rtx; // r10 
rtx v98; // r13 
rtx v106; // rax 
sprintf( label, "*.%s%u", ( const char *)&a.dw_attr_val, v16); 
v22 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v21); 
rtx v31; // rbp 
rtx v34; // rdi 
rtx v37; // rax 
edge succ; // rax 
edge v51; // rdx 
succ = entry_exit_blocks[0].succ; 
if ( succ ) 
if ( e == succ ) 
v51 = succ; 
v51 = succ; 
succ = succ->succ_next; 
succ = succ->succ_next; 
if ( !succ ) 
if ( e == succ ) 
rtx v27; // rax 
v4 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)value + 2)); 
return expand_binop( ( machine_mode)*( ( unsigned __int8 *)value + 2), v6, v19, v20, target, 0, OPTAB_LIB_WIDEN); 
v27 = negate_rtx( ( machine_mode)*( ( unsigned __int8 *)value + 2), *( rtx *)&value[1]); 
v27 = negate_rtx( ( machine_mode)*( ( unsigned __int8 *)value + 2), *( rtx *)&value[1]); 
v7 = v27; 
v11 = expand_binop( ( machine_mode)*( ( unsigned __int8 *)value + 2), v6, v10, v7, v4, 0, OPTAB_LIB_WIDEN); 
return expand_binop( ( machine_mode)*( ( unsigned __int8 *)value + 2), v6, v11, v12, target, 0, OPTAB_LIB_WIDEN); 
v18 = force_reg( ( machine_mode)*( unsigned __int8 *)( value->fld[0].rtwint + 2), v17); 
return simplify_gen_subreg( ( machine_mode)*( ( unsigned __int8 *)value + 2), v18, ( machine_mode)v15, v16); 
return simplify_gen_subreg( ( machine_mode)*( ( unsigned __int8 *)value + 2), v18, ( machine_mode)v15, v16); 
return expand_mult( ( machine_mode)*( ( unsigned __int8 *)value + 2), v24, v25, target, 1); 
rtx v27; // rax 
rtx v33; // rax 
rtx v36; // rax 
rtx v57; // rax 
rtx v75; // rax 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)v4, y, x); 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)v4, y, x); 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)v4, y, x); 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)v4, y, x); 
v17 = gen_rtx_fmt_ee( PLUS, ( machine_mode)v4, y, x); 
else if ( in_section_0 != in_text ) 
in_section_0 = in_text; 
immediate_operand( recog_data_0.operand[1], VOIDmode); 
v17 = immediate_operand( recog_data_0.operand[1], VOIDmode); 
v22 = general_operand( recog_data_0.operand[0], QImode); 
v2 = &insn_data_0[optab_table[0]->handlers[*( ( unsigned __int8 *)x + 2)].insn_code]; 
if ( !operand->predicate( x, ( machine_mode)*( ( unsigned __int16 *)operand + 8)) 
|| !operand[1].predicate( x, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) 
|| !operand[2].predicate( y, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
v4 = &insn_data_0[insn_code]; 
if ( operand->predicate( r0, ( machine_mode)*( ( unsigned __int16 *)operand + 8)) 
&& operand[1].predicate( r1, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) 
&& operand[2].predicate( c, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
fancy_abort( ( const char *)&a, 5026, "add_child_die"); 
v16 = ( tree_node *)rtl[7]; 
rtx result; // rax 
result = simplify_binary_operation( code, mode, v5, rtx); 
if ( result ) 
return result; 
result = simplify_binary_operation( code, mode, rtx, v5); 
if ( result ) 
return result; 
result = simplify_binary_operation( code, mode, op0, op1); 
if ( result ) 
return result; 
result = simplify_relational_operation( code, v8, rtx, v5); 
if ( !result ) 
return result; 
rtx v11; // rax 
if ( !call_insn_operand( rtx, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4)) ) 
( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 
v11 = emit_call_insn( v10); 
use_reg( ( rtx *)&v11[4], v9); 
rtx v15; // rax 
if ( !call_insn_operand( rtx, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4)) ) 
( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 
v15 = emit_call_insn( v14); 
use_reg( ( rtx *)&v15[4], v12); 
if ( ( unsigned int)debug_info_level_0 <= DINFO_LEVEL_TERSE ) 
if ( ( unsigned int)debug_info_level_0 <= DINFO_LEVEL_TERSE ) 
fancy_abort( ( const char *)&a, 10568, "gen_label_die"); 
if ( ( unsigned int)debug_info_level_0 <= DINFO_LEVEL_TERSE ) 
fancy_abort( ( const char *)&a, 10025, "gen_formal_parameter_die"); 
fancy_abort( ( const char *)&a, 9459, "add_abstract_origin_attribute"); 
x = force_reg( ( machine_mode)*( ( unsigned __int8 *)x + 2), x); 
rtx result; // rax 
rtx *slot_with_hash; // rbx 
__m256i u; // [rsp+20h] [rbp-48h] OVERLAPPED BYREF 
v10 = *( ( unsigned int *)&mode_class_0 + v2); 
result = rtx; 
return result; 
return result; 
u.m256i_i64[0] = v29; 
slot_with_hash = ( rtx *)htab_find_slot_with_hash( ( __int64)const_int_htab, ( __int64)&u, v29, 1); 
if ( !*slot_with_hash ) 
*slot_with_hash = gen_rtx_fmt_w( CONST_INT, VOIDmode, u.m256i_i64[0]); 
*slot_with_hash = gen_rtx_fmt_w( CONST_INT, VOIDmode, u.m256i_i64[0]); 
return *slot_with_hash; 
rtx result; // rax 
result = rtx->fld[0].rtx; 
if ( *( _WORD *)result == 66 ) 
if ( *( ( unsigned __int8 *)result + 2) == mode ) 
return result; 
result = gen_lowpart_common( mode, rtx); 
if ( result ) 
return result; 
return gen_rtx_fmt_ee( ( rtx_code)( unsigned __int16)v6, mode, rtx->fld[0].rtx, *( rtx *)&rtx[1]); 
result = simplify_gen_subreg( mode, rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v8); 
result = simplify_gen_subreg( mode, rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v8); 
if ( !result ) 
return result; 
if ( *( ( _DWORD *)&mode_class_0 + v3) == 4 && optab_table[30]->handlers[v3].insn_code == CODE_FOR_nothing ) 
return insn_data_0[optab_table[30]->handlers[v6].insn_code].genfun( v8, v9); 
operands[4] = gen_rtx_REG( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), 17); 
if ( generating_concat_p && ( v9 = *( ( _DWORD *)&mode_class_0 + ( int)mode), ( unsigned int)( v9 - 5) <= 1) ) 
rtx last_insn; // rbx 
rtx v15; // rax 
rtx v20; // r15 
rtx v23; // rax 
rtx v29; // r9 
rtx v31; // r12 
rtx v37; // rax 
rtx secondary_mem; // rax 
rtx v40; // r14 
*(  struct rtx_def *)&result->fld[0].rtwint = ( rtx_def)si128; 
*(  struct rtx_def *)&result->fld[0].rtwint = ( rtx_def)si128; 
*(  struct rtx_def *)&result->fld[0].rtwint = ( rtx_def)v12; 
result[2] = ( rtx_def)si128; 
*(  struct rtx_def *)&result->fld[0].rtwint = ( rtx_def)si128; 
if ( MEMORY[0x1009DF8DF] ) 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v5 = trunc_int_for_mode( 1 << operands[2]->fld[0].rtwint, ( machine_mode)( !v2 + 4)); 
v8 = gen_rtx_fmt_ee( MULT, ( machine_mode)( !v2 + 4), v7, v6); 
v1 = gen_lowpart( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), operands[1]); 
v3 = trunc_int_for_mode( 1 << operands[2]->fld[0].rtwint, ( machine_mode)( !v2 + 4)); 
ix86_split_fp_branch( ( rtx_code)*( _WORD *)*operands, operands[1], operands[2], operands[3], operands[4], 0LL); 
ix86_split_fp_branch( ( rtx_code)*( _WORD *)*operands, operands[1], operands[2], operands[3], operands[4], operands[5]); 
v1 = ix86_force_to_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2), operands[1]); 
v2 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v1); 
v4 = gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)operands[3], ( machine_mode)*( ( unsigned __int8 *)operands[3] + 2), v2, v3); 
v4 = gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)operands[3], ( machine_mode)*( ( unsigned __int8 *)operands[3] + 2), v2, v3); 
ix86_free_from_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2)); 
v1 = ix86_force_to_memory( ( machine_mode)*( ( unsigned __int8 *)operands[2] + 2), operands[2]); 
v2 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v1); 
v4 = gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)operands[3], ( machine_mode)*( ( unsigned __int8 *)operands[3] + 2), v3, v2); 
v4 = gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)operands[3], ( machine_mode)*( ( unsigned __int8 *)operands[3] + 2), v3, v2); 
ix86_free_from_memory( ( machine_mode)*( ( unsigned __int8 *)operands[2] + 2)); 
v7 = gen_rtx( ( rtx_code)*( _WORD *)v1, ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v2, const_int_rtx[64]); 
v7 = gen_rtx( ( rtx_code)*( _WORD *)v1, ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v2, const_int_rtx[64]); 
v12 = gen_rtx( ( rtx_code)*( _WORD *)v1, ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v11, v10); 
v12 = gen_rtx( ( rtx_code)*( _WORD *)v1, ( machine_mode)*( ( unsigned __int8 *)v1 + 2), v11, v10); 
v10 = gen_rtx( ( rtx_code)*( _WORD *)operand1, ( machine_mode)*( ( unsigned __int8 *)operand1 + 2), v9, v8); 
v10 = gen_rtx( ( rtx_code)*( _WORD *)operand1, ( machine_mode)*( ( unsigned __int8 *)operand1 + 2), v9, v8); 
*( _WORD *)operands[1] = swap_condition( ( rtx_code)*( _WORD *)v21); 
if ( const0_operand( operands[2], ( machine_mode)*( ( unsigned __int8 *)*operands + 2)) ) 
v10 = gen_rtx( ( rtx_code)*( _WORD *)v6, ( machine_mode)*( ( unsigned __int8 *)v6 + 2), v9, v7); 
v10 = gen_rtx( ( rtx_code)*( _WORD *)v6, ( machine_mode)*( ( unsigned __int8 *)v6 + 2), v9, v7); 
v16 = gen_rtx( ( rtx_code)*( _WORD *)v13, ( machine_mode)*( ( unsigned __int8 *)v13 + 2), v11, v12); 
v16 = gen_rtx( ( rtx_code)*( _WORD *)v13, ( machine_mode)*( ( unsigned __int8 *)v13 + 2), v11, v12); 
v1 = ix86_force_to_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2), operands[1]); 
v2 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v1); 
ix86_free_from_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2)); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v6 = gen_lowpart( ( machine_mode)( !v2 + 4), v5); 
v8 = gen_lowpart( ( machine_mode)( !v2 + 4), v7); 
v12 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( !v2 + 4), v10, v9); 
v13 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v12, v11); 
v1 = gen_lowpart( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), operands[1]); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v6 = gen_lowpart( ( machine_mode)( !v2 + 4), v5); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v6 = gen_lowpart( ( machine_mode)( !v2 + 4), v5); 
v10 = gen_rtx_fmt_ee( MULT, ( machine_mode)( !v2 + 4), v8, v7); 
v11 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v10, v9); 
v1 = gen_lowpart( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), operands[1]); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v6 = gen_lowpart( ( machine_mode)( !v2 + 4), v5); 
v8 = gen_lowpart( ( machine_mode)( !v2 + 4), v7); 
v13 = gen_rtx_fmt_ee( MULT, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v10, v9); 
v14 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v13, v11); 
v15 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v14, v12); 
v1 = gen_lowpart( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), operands[1]); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v6 = gen_lowpart( ( machine_mode)( !v2 + 4), v5); 
v5 = gen_lowpart( ( machine_mode)v4, operands[1]); 
v8 = gen_lowpart( ( machine_mode)( !v6 + 4), v7); 
v12 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( !v6 + 4), v11, v10); 
v1 = gen_lowpart( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), operands[1]); 
v4 = gen_lowpart( ( machine_mode)( !v2 + 4), v3); 
v19 = gen_rtx( ( rtx_code)*( _WORD *)operand1, VOIDmode, operand4, operand5); 
v19 = gen_rtx( ( rtx_code)*( _WORD *)operand1, VOIDmode, operand4, operand5); 
v4 = &insn_data_0[insn_code]; 
if ( operand->predicate( r0, ( machine_mode)*( ( unsigned __int16 *)operand + 8)) 
&& operand[1].predicate( r1, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) 
&& operand[2].predicate( c, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
if ( ( unsigned int)debug_info_level_0 > DINFO_LEVEL_TERSE ) 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
if ( ( unsigned int)debug_info_level_0 > DINFO_LEVEL_TERSE ) 
sprintf( label_id, "*.%s%u", ( const char *)&a.dw_attr_next + 4, v77); 
if ( ( unsigned int)debug_info_level_0 <= DINFO_LEVEL_TERSE ) 
fancy_abort( ( const char *)&a, 9459, "add_abstract_origin_attribute"); 
v7 = ( tree_node *)xcalloc( 1uLL, 0x48uLL); 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 
fancy_abort( ( const char *)&a, 10121, "gen_type_die_for_member"); 
fancy_abort( ( const char *)&a, 9459, "add_abstract_origin_attribute"); 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
fancy_abort( ( const char *)&a, 4387, "AT_flag"); 
if ( !( _BYTE)v5 && ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFD) != 1 ) 
if ( trunc_int_for_mode( op->fld[0].rtwint, ( machine_mode)v3) != op->fld[0].rtwint ) 
if ( *( ( _DWORD *)&mode_class_0 + v5) == 2 && mode_size[v5] > mode_size[*( ( unsigned __int8 *)rtwint + 2)] ) 
if ( *( _WORD *)v10 != 70 && !legitimate_address_p( ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v10, 0) ) 
result = lang_hooks_0.get_alias_set( i); 
v19 = ++last_alias_set_5; 
result = lang_hooks_0.get_alias_set( section_name); 
result = ++last_alias_set_5; 
fatal_insn_not_found( insn, "insn-attrtab.c", 12189, "get_attr_athlon_decode"); 
LODWORD( v3) = memory_operand( recog_data_0.operand[1], VOIDmode) != 0; 
fatal_insn_not_found( insn, "insn-attrtab.c", 11973, "get_attr_athlon_fpunits"); 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) || immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( mult_operator( recog_data_0.operand[3], SFmode) ) 
if ( !mult_operator( recog_data_0.operand[3], SFmode) ) 
if ( mult_operator( recog_data_0.operand[3], SFmode) ) 
if ( !mult_operator( recog_data_0.operand[3], XFmode) ) 
if ( mult_operator( recog_data_0.operand[3], TFmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 13438, "get_attr_i387"); 
if ( mult_operator( recog_data_0.operand[3], SFmode) ) 
if ( mult_operator( recog_data_0.operand[3], DFmode) ) 
if ( get_attr_type( insn) != TYPE_FOP && !mult_operator( recog_data_0.operand[3], XFmode) ) 
if ( get_attr_type( insn) != TYPE_FOP && !mult_operator( recog_data_0.operand[3], TFmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 13072, "get_attr_imm_disp"); 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( !memory_displacement_operand( recog_data_0.operand[0], VOIDmode) ) 
return immediate_operand( recog_data_0.operand[1], VOIDmode) != 0; 
if ( !q_regs_operand( recog_data_0.operand[0], QImode) ) 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], DImode) ) 
v3 = incdec_operand( recog_data_0.operand[2], DImode); 
if ( !incdec_operand( recog_data_0.operand[2], SImode) ) 
if ( !incdec_operand( recog_data_0.operand[2], HImode) ) 
if ( !incdec_operand( recog_data_0.operand[2], QImode) ) 
if ( const1_operand( recog_data_0.operand[2], VOIDmode) 
|| !const1_operand( recog_data_0.operand[2], VOIDmode) ) 
if ( const0_operand( recog_data_0.operand[2], SImode) 
&& memory_displacement_operand( recog_data_0.operand[0], VOIDmode) 
&& immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( !const0_operand( recog_data_0.operand[2], DImode) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) && memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) && memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
v5 = recog_data_0.operand[1]; 
fatal_insn_not_found( insn, "insn-attrtab.c", 15861, "get_attr_memory"); 
if ( !memory_operand( recog_data_0.operand[0], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
&& symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) && memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) && memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 17669, "get_attr_mode"); 
v18 = aligned_operand( recog_data_0.operand[1], HImode) == 0; 
if ( !aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( q_regs_operand( recog_data_0.operand[0], QImode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 19258, "get_attr_pent_pair"); 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( !q_regs_operand( recog_data_0.operand[0], QImode) ) 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], DImode) ) 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], DImode) ) 
if ( !memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( !pic_symbolic_operand( recog_data_0.operand[2], DImode) ) 
if ( pic_symbolic_operand( recog_data_0.operand[2], SImode) ) 
v9 = incdec_operand( recog_data_0.operand[2], HImode) == 0; 
if ( !incdec_operand( recog_data_0.operand[2], HImode) ) 
|| !incdec_operand( recog_data_0.operand[2], QImode) 
if ( !incdec_operand( recog_data_0.operand[2], QImode) ) 
rtx v9; // rdx 
fatal_insn_not_found( insn, "insn-attrtab.c", 21978, "get_attr_type"); 
if ( flag_pic && symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( register_operand( recog_data_0.operand[0], QImode) ) 
if ( !q_regs_operand( recog_data_0.operand[0], QImode) ) 
if ( !flag_pic || !symbolic_operand( recog_data_0.operand[1], DImode) ) 
if ( ( _DWORD)which_alternative == 2 || pic_symbolic_operand( recog_data_0.operand[2], DImode) ) 
if ( !incdec_operand( recog_data_0.operand[2], DImode) ) 
if ( pic_symbolic_operand( recog_data_0.operand[2], SImode) ) 
if ( incdec_operand( recog_data_0.operand[2], SImode) ) 
if ( !incdec_operand( recog_data_0.operand[2], HImode) ) 
if ( !incdec_operand( recog_data_0.operand[2], QImode) ) 
if ( !register_operand( recog_data_0.operand[0], VOIDmode) ) 
v12 = swap_condition( ( rtx_code)*v9); 
v9 = ( tree_node *)*( &global_trees + 15); 
v10 = ( tree_node *)*( &global_trees + 17); 
imag = *( tree_node **)( high + 40); 
rtx loc[2]; // [rsp+8h] [rbp-10h] BYREF 
loc[0] = last_value; 
return gen_lowpart_for_combine( ( machine_mode)*( ( unsigned __int8 *)v1 + 2), last_value); 
loc[0] = reg_last_set_value[v3]; 
if ( !loc[0] ) 
&& ( get_last_value_validate( loc, v4, *( ( _DWORD *)reg_last_set_label + v3), 0) 
|| ( loc[0] = copy_rtx( loc[0]), 
|| ( loc[0] = copy_rtx( loc[0]), 
get_last_value_validate( loc, reg_last_set[v3], *( ( _DWORD *)reg_last_set_label + v3), 1))) ) 
return loc[0]; 
v15 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v27) - 5) < 2) + 1; 
*loc = gen_rtx_fmt_e( CLOBBER, ( machine_mode)*( ( unsigned __int8 *)v6 + 2), const_int_rtx[64]); 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) <= 1 ) 
v8 = *( ( _DWORD *)&mode_class_0 + ( int)mode); 
if ( ( *( &mode_class_0 + v6) & 0xFFFFFFF9) == 1 ) 
find_reloads_address( v7, 0LL, v10->fld[0].rtx, ( rtx *)v10->fld, opnum, ( reload_type)v13, 0, 0LL); 
rtx v6; // r12 
v6 = rtx; 
v6 = rtx; 
( machine_mode)*( unsigned __int8 *)( v5.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)rtx + 2))] 
v23 = ( page_entry_0 *)( &G + 2640); 
v28 = ( page_group_0 *)entry; 
v28 = ( page_group_0 *)&v47[v44 - v50]; 
if ( length == 1 && ( sch_istable[*( unsigned __int8 *)contents] & 4) != 0 ) 
v17 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v15) - 5) < 2) + 1; 
v13 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v11) - 5) < 2) + 1; 
v23 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v22) - 5) < 2) + 1; 
v30 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v29) - 5) < 2) + 1; 
v38 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v37) - 5) < 2) + 1; 
v47 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v46) - 5) < 2) + 1; 
rtx v19; // rax 
v19 = rtx; 
v21 = ( __m128)_mm_loadu_si128( ( const __m128i *)v19[1].fld); 
v22 = ( __m128)_mm_loadu_si128( ( const __m128i *)v19[2].fld); 
v19 += 2; 
while ( &rtx[2 * ( v18 >> 2)] != v19 ); 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)&v12[2 * v15 + 2] + 2LL), 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)&rtwint[2 * v26 + 2] + 2LL), 
v13 = cselib_lookup( x, ( machine_mode)*( ( unsigned __int8 *)x + 2), create); 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)( 5 - ( ( ( unsigned int)target_flags & 0x2000000) == 0)), v8, result); 
rtx last_value; // rax 
rtx v52; // r11 
rtx v59; // rax 
rtx v60; // rax 
rtx v63; // rax 
if ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFD) == 1 ) 
if ( !target_isnan( d) && *( _OWORD *)d.r == *( _OWORD *)dconst0.r && dconst0.r[2] == d.r[2] ) 
while ( *( _OWORD *)u.d.r != *( _OWORD *)&result[1] || result[2] != u.d.r[2] ) 
result[1] = ( rtx_def)v3; 
if ( !*( ( _BYTE *)op + 2) && mode && ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFD) != 1 ) 
rtx v21; // rax 
v21 = canon_rtx( rtx); 
v22 = v21; 
if ( ( *( ( _BYTE *)v21 + 3) & 4) == 0 ) 
v23.rtwint = ( __int64)v21->fld[0]; 
v2 = rtx_alloc( ( rtx_code)*( _WORD *)notes); 
v5 = rtx_alloc( ( rtx_code)*v4); 
v8 = rtx_alloc( ( rtx_code)*v7); 
v10 = rtx_alloc( ( rtx_code)*v9); 
v12 = rtx_alloc( ( rtx_code)*v11); 
v42 = rtx_alloc( ( rtx_code)*v15); 
v44 = rtx_alloc( ( rtx_code)*v18); 
v46 = rtx_alloc( ( rtx_code)*v21); 
v50 = rtx_alloc( ( rtx_code)*v24); 
v3 = lang_hooks_0.expand_constant( value); 
if ( ( *( &mode_class_0 + v28) & 0xFFFFFFF9) != 1 
if ( ( *( &mode_class_0 + v31) & 0xFFFFFFF9) != 1 
v7 = ( tree_node *)*( &global_trees + 14); 
if ( ( tree_node *)*( &global_trees + 14) != v7 ) 
v3 = lang_hooks_0.expand_constant( v3->vector.elements); 
v7 = ( tree_node *)*( &global_trees + 14); 
rtx v85; // [rsp+8h] [rbp-40h] 
v43 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v40) - 5) < 2) + 1; 
&& ( v63 = *( ( unsigned __int8 *)x + 2), *( ( _DWORD *)&mode_class_0 + v63) == 1) 
&& ( v64 = *( unsigned __int8 *)( v29.rtwint + 2), *( ( _DWORD *)&mode_class_0 + v64) == 1) 
v73 = this_insn_0; 
v45[v46->reg_qty].const_insn = this_insn_0; 
v85 = adjust_address_1( x, v33, 0LL, 0, 1); 
v74 = memory_address_p( v33, v85->fld[0].rtx); 
v35 = v85; 
v36 = this_insn_0; 
v15 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v11) - 5) <= 1; 
v38 = *( ( _DWORD *)uid_cuid_0 + v37[1]); 
if ( v38 > cse_basic_block_end || *( ( _DWORD *)uid_cuid_0 + *v37) < cse_basic_block_start ) 
if ( v38 > *( ( _DWORD *)uid_cuid_0 + *( int *)( reg_n_info->data.l[first_reg] + 4)) ) 
*( _OWORD *)&v30->const_rtx = 0LL; 
reg_eqv_table[v8] = ( reg_eqv_elem)-1LL; 
fatal_insn_not_found( insn, "insn-attrtab.c", 356, "insn_default_length"); 
if ( !register_operand( recog_data_0.operand[0], DImode) ) 
if ( !register_operand( recog_data_0.operand[0], SImode) ) 
if ( !register_operand( recog_data_0.operand[0], VOIDmode) ) 
rtx v13; // r8 
rtx v14; // r9 
rtx v15; // rax 
rtx s1a; // [rsp+0h] [rbp-48h] 
rtx s1; // [rsp+0h] [rbp-48h] 
rtx s2; // [rsp+8h] [rbp-40h] 
v13 = 0LL; 
v10 = *( tree_node **)( i + 112); 
v10 = ( tree_node *)i; 
v11 = *( tree_node **)( i + 80); 
rtx v12; // rax 
v7 = canon_hash( rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2)) & 0x1F; 
v27 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v23) - 5) < 2) + 1; 
v44 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v41) - 5) < 2) + 1; 
v12 = canon_rtx( rtx); 
v13 = v12; 
full_mode = *( ( unsigned __int8 *)v12 + 2); 
v15 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v12) - 5) < 2) + 1; 
invalidate( *( rtx *)( v8.rtwint + 8), ( machine_mode)*( unsigned __int8 *)( v8.rtwint + 2)); 
invalidate( v6->fld[0].rtx, ( machine_mode)*( ( unsigned __int8 *)v6 + 2)); 
v12 = *( ( _DWORD *)&mode_class_0 + v11); 
reverse_condition_0( ( rtx_code)v13); 
v14 = CSWTCH_103[v13]; 
v8 = gen_rtx_fmt_ee( v14, ( machine_mode)*( ( unsigned __int8 *)v4 + 2), ( rtx)v9, ( rtx)v10); 
rtx v12; // rcx 
rtx v24; // rcx 
v12 = x; 
rtx = v12; 
v24 = x; 
LODWORD( v1) = recog_data_0.n_operands - 1; 
v2 = recog_data_0.operand[v1]; 
LODWORD( v3) = recog_data_0.n_operands - 1; 
v6 = recog_data_0.operand[v3]; 
fatal_insn( "unknown insn mode", insn, "i386.c", 9956, "ix86_attr_length_immediate_default"); 
v4 = CSWTCH_1591[v5]; 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)op0 + 2)) == 2 ) 
rtx v12; // rdx 
rtx v21; // rdx 
v12 = addr; 
rtx = v12; 
v21 = addr; 
rtx = v21; 
v9 = gen_rtx_MEM( ( machine_mode)v8, pointer); 
v10 = adjust_address_1( v9, ( machine_mode)v8, v12, 1, 1); 
v11 = gen_rtx_REG( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v4); 
v8 = gen_rtx_REG( ( machine_mode)v7, v4); 
v10 = gen_rtx_MEM( ( machine_mode)v9, pointer); 
v11 = adjust_address_1( v10, ( machine_mode)v9, offset, 1, 1); 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)v4 + 2)) == 2 ) 
v7 = gen_rtx_REG( ( machine_mode)v6, 17); 
v8 = gen_rtx_fmt_ee( COMPARE, ( machine_mode)v6, v4, v3); 
v24 = gen_rtx_REG( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 2); 
v26 = gen_rtx_fmt_ee( PLUS, ( machine_mode)v25, global_rtl[4], v24); 
v29 = gen_rtx_MEM( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), global_rtl[4]); 
v32 = gen_rtx_fmt_ee( PLUS, ( machine_mode)v25, global_rtl[2], v24); 
*operands = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), *operands); 
v15 = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v15); 
*operands = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), *operands); 
v15 = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v15); 
v30 = reverse_condition_maybe_unordered( ( rtx_code)*( _WORD *)v26); 
v19 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
tmp = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
v8 = gen_rtx_fmt_eee( IF_THEN_ELSE, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v5, v7, operands[3]); 
( machine_mode)*( ( unsigned __int8 *)*operands + 2), 
( machine_mode)*( ( unsigned __int8 *)*operands + 2), 
rtx v17; // r8 
rtx v25; // rax 
if ( ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) == 2 || *( ( _DWORD *)&mode_class_0 + ( int)mode) == 8) 
if ( ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) == 2 || *( ( _DWORD *)&mode_class_0 + ( int)mode) == 8) 
v17 = operands[1]; 
v18 = *( _WORD *)v17; 
if ( *( _WORD *)v17 != 58 ) 
v22.rtwint = ( __int64)v17->fld[0]; 
v30 = force_reg( ( machine_mode)v6, v17); 
v30 = force_reg( ( machine_mode)v6, v17); 
v25 = gen_reg_rtx( ( machine_mode)v6); 
v25 = gen_reg_rtx( ( machine_mode)v6); 
v17 = operands[1]; 
v19 = v25; 
v8 = reverse_condition_maybe_unordered( ( rtx_code)( unsigned __int16)**( ( _WORD **)&test + 1)); 
v10 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), global_rtl[2], v9); 
v18 = gen_rtx_fmt_e( PRE_DEC, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), global_rtl[2]); 
v13 = gen_rtx_fmt_e( PRE_DEC, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), global_rtl[2]); 
v3 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), global_rtl[2], v2); 
v2 = *( ( _DWORD *)&mode_class_0 + ( int)mode); 
v6 = CSWTCH_1605[v4]; 
|| ( v4 = *( ( unsigned __int8 *)x + 2), ( *( &mode_class_0 + v4) & 0xFFFFFFFB) != 2) 
&& *( ( _DWORD *)&mode_class_0 + v4) != 8 
if ( ( *( &mode_class_0 + v15) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + v15) != 8 
if ( ( *( &mode_class_0 + v15) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + v15) != 8 
if ( ( ( *( &mode_class_0 + v22) & 0xFFFFFFFB) == 2 || *( ( _DWORD *)&mode_class_0 + v22) == 8) 
if ( ( ( *( &mode_class_0 + v22) & 0xFFFFFFFB) == 2 || *( ( _DWORD *)&mode_class_0 + v22) == 8) 
&& ( ( v20 = *( ( unsigned __int8 *)v7 + 2), ( *( &mode_class_0 + v20) & 0xFFFFFFFB) == 2) 
|| *( ( _DWORD *)&mode_class_0 + v20) == 8) 
v9 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) < 2) + 1; 
v13 = CSWTCH_1605[v12]; 
v13 = CSWTCH_1605[v26]; 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) <= 1 ) 
rtx v38; // r15 
rtx v40; // r13 
rtx i; // [rsp+8h] [rbp-50h] 
rtx second; // [rsp+10h] [rbp-48h] BYREF 
rtx bypass; // [rsp+18h] [rbp-40h] BYREF 
v11 = ix86_expand_fp_compare( code, op1, op2, tmp, &second, &bypass); 
v11 = ix86_expand_fp_compare( code, op1, op2, tmp, &second, &bypass); 
v13 = bypass; 
if ( !bypass ) 
v40 = global_rtl[0]; 
v42 = v40; 
v43 = gen_rtx_fmt_eee( IF_THEN_ELSE, VOIDmode, bypass, v41, v42); 
if ( !bypass ) 
rtx tmp; // [rsp+0h] [rbp-58h] BYREF 
rtx x[2]; // [rsp+8h] [rbp-50h] BYREF 
rtx parts; // [rsp+18h] [rbp-40h] BYREF 
rtx y; // [rsp+20h] [rbp-38h] 
rtx v40; // [rsp+28h] [rbp-30h] 
v6 = ix86_split_to_parts( operands[1], &parts, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
v6 = ix86_split_to_parts( operands[1], &parts, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
ix86_split_to_parts( *operands, &tmp, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
ix86_split_to_parts( *operands, &tmp, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
v25 = y; 
y = change_address( y, ( machine_mode)*( ( unsigned __int8 *)y + 2), v40->fld[0].rtx); 
y = change_address( y, ( machine_mode)*( ( unsigned __int8 *)y + 2), v40->fld[0].rtx); 
y = change_address( y, ( machine_mode)*( ( unsigned __int8 *)y + 2), v40->fld[0].rtx); 
y = change_address( y, ( machine_mode)*( ( unsigned __int8 *)y + 2), v40->fld[0].rtx); 
rtx v16; // rax 
rtx pool_constant; // rax 
rtx op; // [rsp+8h] [rbp-70h] BYREF 
if ( !push_operand( op, VOIDmode) ) 
v16 = copy_rtx( op); 
v16 = copy_rtx( op); 
*( ( _BYTE *)v16 + 2) = ( ( BYTE3( target_flags) & 2) != 0) + 4; 
parts[2] = v16; 
*( __m128i *)parts = _mm_unpacklo_epi64( ( __m128i)( unsigned __int64)v16, ( __m128i)( unsigned __int64)v16); 
&& ( *( &mode_class_0 + *( ( unsigned __int8 *)x + 2)) & 0xFFFFFFFB) != 2 
&& *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)x + 2)) != 8 
&& ( *( &mode_class_0 + *( ( unsigned __int8 *)val + 2)) & 0xFFFFFFFB) != 2 
&& *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)val + 2)) != 8 ) 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
if ( comparison_dominates_p( v7, ( rtx_code)v8) ) 
|| ( v31.rtwint = ( __int64)x->fld[0], *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( v31.rtwint + 2)) == 4) 
( rtx_code)*( _WORD *)x, 
rtx v33; // r12 
rtx v39; // rax 
rtx v47; // rax 
v17 = gen_rtx_fmt_Ei( UNSPEC, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v16, 15); 
v18 = gen_rtx_fmt_e( CONST, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v17); 
v19 = gen_rtx_MEM( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v18); 
v20 = set_31; 
set_31 = new_alias_set( ); 
v20 = set_31; 
v10 = gen_reg_rtx( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4)); 
v36 = gen_rtx_fmt_Ei( UNSPEC, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v35, 6); 
v37 = gen_rtx_fmt_e( CONST, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v36); 
v38 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), pic_offset_table_rtx, v37); 
v39 = gen_rtx_MEM( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v38); 
v39 = gen_rtx_MEM( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v38); 
v40 = set_31; 
v3 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), pic_label_name); 
fancy_abort( ( const char *)&a, 7976, "loc_descriptor"); 
return mem_loc_descriptor( rtl->fld[0].rtx, ( machine_mode)*( ( unsigned __int8 *)rtl + 2)); 
fancy_abort( ( const char *)&a, 8292, "loc_descriptor_from_tree"); 
if ( strcmp( file_name, ( const char *)&a.dw_attr_val.val_class + 3) ) 
last_lookup_index = file_table_0.last_lookup_index; 
table = file_table_0.table; 
if ( !file_table_0.last_lookup_index || strcmp( file_name, file_table_0.table[file_table_0.last_lookup_index]) ) 
if ( !file_table_0.last_lookup_index || strcmp( file_name, file_table_0.table[file_table_0.last_lookup_index]) ) 
if ( !file_table_0.last_lookup_index || strcmp( file_name, file_table_0.table[file_table_0.last_lookup_index]) ) 
in_use = file_table_0.in_use; 
if ( file_table_0.in_use <= 1 ) 
file_table_0.last_lookup_index = last_lookup_index; 
if ( last_lookup_index == file_table_0.allocated ) 
file_table_0.allocated = last_lookup_index + 64; 
file_table_0.table = ( char **)xrealloc( table, 8LL * ( last_lookup_index + 64)); 
table = file_table_0.table; 
*( _QWORD *)&file_table_0.in_use = _mm_unpacklo_epi32( 
fprintf( asm_out_file, ( const char *)&a.dw_attr_val.v.val_unsigned + 6, last_lookup_index, file_name); 
induction_1 *giv; // rbx 
induction_1 *giv; // rbx 
induction_1 *same; // rax 
induction_1 *same; // rax 
giv = bl_0->giv; 
if ( giv ) 
same = giv->same; 
same = giv->same; 
if ( same ) 
if ( ( *( ( _BYTE *)giv + 100) & 4) == 0 ) 
new_reg = giv->new_reg; 
if ( giv->giv_type ) 
if ( ( *( ( _BYTE *)giv + 100) & 1) != 0 ) 
reg_map[giv->dest_reg->fld[0].rtuint] = new_reg; 
v15 = gen_move_insn( giv->dest_reg, new_reg); 
emit_insn_after( v15, giv->insn); 
validate_change( giv->insn, giv->location, new_reg, 0); 
validate_change( giv->insn, giv->location, new_reg, 0); 
rtx v5; // rax 
v5 = insn; 
rtint = v5->fld[0].rtint; 
v8 = *( _WORD *)v5; 
v5 = v5[1].fld[0].rtx; 
v5 = v5[1].fld[0].rtx; 
if ( !v5 ) 
if ( reference == v5 || !rtx ) 
induction_1 *biv; // rdx 
induction_1 *biv; // rdx 
induction_1 *v38; // rdx 
induction_1 *v38; // rdx 
rtx v55; // rax 
rtx v56; // r12 
rtx v58; // rax 
rtx v72; // rax 
rtx increment; // [rsp+0h] [rbp-60h] BYREF 
rtx const2; // [rsp+8h] [rbp-58h] 
induction_1 *biv_inc; // [rsp+10h] [rbp-50h] 
induction_1 *biv_inc; // [rsp+10h] [rbp-50h] 
v12 = expand_mult_add( v11, reg, mult, add, ( machine_mode)*( ( unsigned __int8 *)reg + 2), 1); 
v10 = expand_mult_add( v9, reg, v8, v7, ( machine_mode)*( ( unsigned __int8 *)reg + 2), 1); 
v9 = expand_mult_add( v8, reg, mult, add, ( machine_mode)*( ( unsigned __int8 *)reg + 2), 1); 
return gen_rtx_fmt_e( ( rtx_code)*( _WORD *)temg, v8, v66.rtx); 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)( v3->fld[0].rtwint + 8) + 2LL), 
v72 = gen_rtx_fmt_ee( ( rtx_code)*( unsigned __int16 *)v3->fld[0].rtwint, v8, v71, v70); 
if ( ix86_hard_regno_mode_ok( v11, ( machine_mode)( BYTE5( type->block.abstract_origin) >> 1)) ) 
v22 = gen_rtx_fmt_i0( REG, ( machine_mode)LOBYTE( decl->block.supercontext), v11); 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v23) - 5) > 1 ) 
v15 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), low); 
v16 = gen_rtx_MEM( ( machine_mode)LOBYTE( decl->block.supercontext), v15); 
*( _OWORD *)&result->insns = 0LL; 
*( _OWORD *)&result->pred_next = 0LL; 
*( _OWORD *)&result->src = 0LL; 
*( _OWORD *)&result->flags = 0LL; 
rtx v15; // r15 
rtx v20; // rax 
rtx v24; // r12 
rtx v29; // rax 
rtx v33; // rax 
rtx v36; // rcx 
rtx v42; // rax 
rtx result; // rax 
result = adjust_address_1( v3, ( machine_mode)*( ( unsigned __int8 *)ad + 2), 0LL, 0, 1); 
result = adjust_address_1( v3, ( machine_mode)*( ( unsigned __int8 *)ad + 2), 0LL, 0, 1); 
if ( reg_equiv_memory_loc[regno] == result ) 
return copy_rtx( result); 
return result; 
v3 = ( tree_node *)ggc_alloc( v2); 
*( ( _OWORD *)result + 2) = 0LL; 
*( _OWORD *)result = 0LL; 
*( ( _OWORD *)result + 1) = 0LL; 
*( ( _OWORD *)result + 3) = 0LL; 
tree v14; // r12 
tree v15; // rax 
tree v18; // r12 
tree v19; // rax 
tree v22; // rax 
tree v26; // r12 
v4 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v2) - 5) < 2) + 1; 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v11) - 5) > 1 ) 
v8 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) < 2) + 1; 
( machine_mode)( unsigned __int8)v33, 
( machine_mode)( unsigned __int8)v32); 
v43 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v39) - 5) < 2) + 1; 
v14 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v11) - 5) <= 1; 
rtx v18; // r15 
rtx v19; // r11 
rtx v24; // r15 
rtx mem_set_list; // r15 
rtx v28; // r11 
rtx temp; // [rsp+0h] [rbp-68h] 
rtx *listp; // [rsp+10h] [rbp-58h] 
rtx preva; // [rsp+18h] [rbp-50h] 
listp = &pbi->mem_set_list; 
free_INSN_LIST_list( listp); 
v9 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v8) - 5) < 2) + 1; 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)x + 2)) == 2 
|| *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( x->fld[0].rtwint + 2)) == 2 
|| *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( *( _QWORD *)&x[1] + 2LL)) == 2 ) 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)x + 2)) == 2 || v9 == 54 && !*( ( _QWORD *)v7 + 1) ) 
if ( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)x + 2)) != 2 ) 
v19 = ( const char *)&off_7447DC; 
if ( ( sch_istable[*( unsigned __int8 *)*format] & 4) != 0 ) 
if ( ( sch_istable[v13] & 4) == 0 ) 
rtx v8; // rax 
rtx pool_constant_mark; // rax 
pool_constant_mark = get_pool_constant_mark( v3, marked); 
if ( *( _WORD *)pool_constant_mark != 68 ) 
rtx = pool_constant_mark; 
if ( ( *( ( _BYTE *)pool_constant_mark + 3) & 4) != 0 ) 
get_pool_constant_mark( pool_constant_mark, marked); 
v10 = mem_loc_descriptor( v3->fld[0].rtx, ( machine_mode)*( ( unsigned __int8 *)v3 + 2)); 
v8 = gen_rtx_fmt_ee( PLUS, *(short *)0xmode, rtx->fld[0].rtx, v7); 
v4 = *( _WORD *)v8; 
rtx = v8; 
rtx v10; // r14 
rtx constant_term; // [rsp+8h] [rbp-40h] BYREF 
v4 = force_reg_0( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), x); 
constant_term = const_int_rtx[64]; 
v10 = eliminate_constant_term_0( v4, &constant_term); 
v10 = eliminate_constant_term_0( v4, &constant_term); 
if ( const_int_rtx[64] != constant_term && memory_address_p( mode, v10) ) 
if ( const_int_rtx[64] != constant_term && memory_address_p( mode, v10) ) 
v11 = constant_term; 
v12 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)v10 + 2)); 
v12 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)v10 + 2)); 
if ( !general_operand( v10, VOIDmode) ) 
v10 = force_operand( v10, v12); 
v10 = force_operand( v10, v12); 
if ( v12 != v10 ) 
emit_move_insn( v12, v10); 
rtx v9; // rbp 
rtx v15; // rbx 
rtx addr_0; // rax 
rtx v22; // rax 
rtx v30; // rax 
rtx x1a; // [rsp+8h] [rbp-50h] 
rtx y1a; // [rsp+10h] [rbp-48h] 
rtx x0; // [rsp+18h] [rbp-40h] 
rtx x0a; // [rsp+18h] [rbp-40h] 
v9 = y; 
v22 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v24) - 5) < 2) + 1; 
result = *( unsigned __int16 *)( *( ( _QWORD *)*( &CSWTCH_517 + ( unsigned int)pattern) + 3) + 24LL * opno + 16); 
fancy_abort( ( const char *)&a, 7381, "base_type_die"); 
fancy_abort( ( const char *)&a, 7573, "modified_type_die"); 
fancy_abort( ( const char *)&a, 7461, "is_base_type"); 
rtx *v45; // r8 
rtx *v46; // r14 
rtx v53; // rax 
rtx *v62; // rax 
rtx *v64; // r14 
rtx v68; // rax 
rtx v70; // rax 
rtx *v89; // [rsp+10h] [rbp-68h] 
rtx *v90; // [rsp+10h] [rbp-68h] 
rtx note; // [rsp+18h] [rbp-60h] 
rtx v93; // [rsp+20h] [rbp-58h] 
rtx *pnotesa; // [rsp+28h] [rbp-50h] 
if ( ( *( &mode_class_0 + v16) & 0xFFFFFFFB) == 2 ) 
if ( ( *( &mode_class_0 + v24) & 0xFFFFFFFB) != 2 ) 
if ( ( *( &mode_class_0 + v27) & 0xFFFFFFFB) == 2 ) 
if ( ( *( &mode_class_0 + v16) & 0xFFFFFFFB) == 2 ) 
v2 = ( const char *)&unk_746FC4; 
return ++last_alias_set_5; 
if ( !( _BYTE)v5 && ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFD) != 1 ) 
if ( trunc_int_for_mode( op->fld[0].rtwint, ( machine_mode)v2) != op->fld[0].rtwint ) 
if ( *( ( _DWORD *)&mode_class_0 + v5) == 2 && mode_size[v5] > mode_size[*( ( unsigned __int8 *)op + 2)] ) 
if ( !legitimate_address_p( ( machine_mode)*( ( unsigned __int8 *)op + 2), rtx, 0) ) 
if ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFD) != 1 ) 
rtx v19; // rdx 
rtx v20; // r8 
rtx v26; // rax 
rtx v28; // rax 
rtx v39; // rax 
rtx v43; // rax 
rtx v49; // rax 
rtx v53; // rax 
v5 = *( ( _DWORD *)&mode_class_0 + ( unsigned __int8)v3); 
|| ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) == 2 
|| *( ( _DWORD *)&mode_class_0 + ( int)mode) == 8 ) 
v5 = *( ( _DWORD *)&mode_class_0 + ( int)mode); 
if ( ( _DWORD)v33 == v7 || *( ( _DWORD *)&mode_class_0 + v33) == 1 && v5 == 1 ) 
rtx last_value; // rax 
if ( ( *( &mode_class_0 + ( int)v4) & 0xFFFFFFFB) == 2 
|| *( ( _DWORD *)&mode_class_0 + ( int)v4) == 8 
|| ( v8 = *( ( _BYTE *)a1 + 2), ( *( &mode_class_0 + v8) & 0xFFFFFFFB) == 2) 
|| *( ( _DWORD *)&mode_class_0 + v8) == 8 ) 
last_value = get_last_value( (  struct bitmap_head_def *)a1); 
if ( !last_value ) 
a1 = ( unsigned __int16 *)last_value; 
v20 = nonzero_bits( *( ( rtx *)a1 + 1), ( machine_mode)v4), 
v48 = nonzero_bits( *( ( rtx *)a1 + 1), ( machine_mode)v4); 
v7 = gen_rtx_fmt_ee( LO_SUM, ( machine_mode)*( ( unsigned __int8 *)y + 2), y->fld[0].rtx, v13); 
&& v6 <= get_mode_alignment( ( machine_mode)( unsigned __int8)v3) >> 3 ) 
v7 = gen_rtx_fmt_ee( LO_SUM, ( machine_mode)*( ( unsigned __int8 *)rtwint + 2), *( ( rtx *)rtwint + 1), v11); 
( machine_mode)*( unsigned __int8 *)( v23.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)rtx + 2)) 
( machine_mode)*( unsigned __int8 *)( v8.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)y + 2)) 
rtx v13; // rbx 
v10 = &reg_avail_info_0[rtx->fld[0].rtuint]; 
v18 = *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
v11 = uid_cuid_1; 
v12 = *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
v13 = modify_mem_list[*( int *)( basic_block_info->data.l[current_bb] + 88)]; 
if ( !v13 ) 
v16.rtwint = ( __int64)v13->fld[0]; 
gcse_mem_operand = rtx; 
gcse_mems_conflict_p = 0; 
if ( gcse_mems_conflict_p ) 
v13 = ( rtx)v13[1]; 
if ( !v13 ) 
v11 = uid_cuid_1; 
v14.rtwint = ( __int64)v13->fld[0]; 
gcse_mem_operand = rtx; 
gcse_mems_conflict_p = 0; 
if ( gcse_mems_conflict_p ) 
if ( in_section_0 == in_const ) 
if ( in_section_0 == in_data ) 
in_section_0 = in_data; 
if ( in_section_0 == in_data ) 
mergeable_constant_section_0( ( machine_mode)LOBYTE( v4->block.supercontext), v5, flag_pic); 
if ( in_section_0 == in_const ) 
in_section_0 = in_const; 
fprintf( asm_out_file, ".%s%u:\n", ( const char *)&a.dw_attr_val, ( unsigned int)labelno); 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( ( unsigned __int8 *)mem + 2)) ) 
sprintf( digit_buffer, off_6B644F, *v42); 
sprintf( digit_buffer, ( const char *)&off_6B6453, *v51); 
sprintf( digit_buffer, off_6B86B9, *v48); 
sprintf( digit_buffer, ( const char *)&off_6B8AA4, *v45); 
sprintf( digit_buffer, &off_698152[1], *v11); 
sprintf( digit_buffer, &off_6B8A7E[1], *v30); 
attributes = *( tree_node **)&cfun; 
if ( ( sch_istable[( unsigned __int8)v14] & 0x88) == 0 ) 
v32 = ( _cpp_buff_0 *)xmalloc( 0x1F60uLL); 
v14 = ( _cpp_buff_0 *)&v23[v22]; 
if ( ( sch_istable[( unsigned __int8)v5] & 0x204) == 0 ) 
if ( ( sch_istable[( unsigned __int8)v15] & 0x88) == 0 ) 
v18 = ( _cpp_buff_0 *)p_free_buffs; 
free_buffs = ( _cpp_buff_0 *)&v26[v25]; 
if ( ( sch_istable[( unsigned __int8)v13] & 0x400) == 0 ) 
v10 = &peep2_insn_data_0[v7]; 
v13 = &peep2_insn_data_0[v7]; 
v18 = v17 + search_ofs_1; 
if ( v17 + search_ofs_1 > 52 ) 
v18 = v17 + search_ofs_1 - 53; 
v25 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) <= 1; 
search_ofs_1 = 0; 
v28 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) <= 1; 
search_ofs_1 = v38; 
rtx v15; // r8 
rtx *constant_term_loc; // r12 
rtx p; // [rsp+8h] [rbp-40h] BYREF 
p = rtx; 
p = rtx; 
p = rtx; 
v2 = force_const_mem( ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v14); 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v2 + 2), v2->fld[0].rtx) ) 
v2 = force_const_mem( ( machine_mode)*( ( unsigned __int8 *)rtx + 2), pool_constant); 
if ( memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v2 + 2), v2->fld[0].rtx) ) 
p = rtx; 
v15 = ( rtx)rtx[1]; 
if ( *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( *( ( _QWORD *)aux + 6) + 2LL)) == 2 
|| ( v14 = *( ( _QWORD *)aux + 4), *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( v14 + 2)) == 2) ) 
hitrate = predictor_info_0[predictor].hitrate; 
if ( taken != TAKEN_0 ) 
n_alternatives = recog_data_0.n_alternatives; 
if ( recog_data_0.n_operands > 0 ) 
v3 = ( char *)recog_data_0.constraints[i]; 
n_alternatives = recog_data_0.n_alternatives; 
if ( recog_data_0.n_operands <= ( int)i + 1 ) 
fprintf( outfile, off_6C49D0, name->int_cst.int_cst.low); 
fwrite( &stru_74575F, 1uLL, 0xCuLL, outfile); 
fprintf( file, off_6C61E2, v9); 
( const char *)&stru_74575F._IO_read_end + 7, 
result = fprintf( outfile, ( const char *)&stru_74575F._IO_read_base + 6); 
fwrite( ( char *)&stru_74575F._IO_write_end + 1, 1uLL, 2uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_write_end + 4, 1uLL, 2uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_write_end + 7, 1uLL, 2uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_buf_base + 2, 1uLL, 2uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_buf_base + 5, 1uLL, 2uLL, outfile); 
fwrite( &stru_74575F._IO_buf_end, 1uLL, 2uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_buf_end + 3, 1uLL, 2uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_read_end + 3, 1uLL, 3uLL, outfile); 
fwrite( ( char *)&stru_74575F._IO_save_end + 6, 1uLL, 2uLL, outfile); 
fprintf( outfile, off_698152, *( ( unsigned int *)a1 + 8)); 
fprintf( outfile, ( const char *)&stru_74575F._markers + 1, *( unsigned int *)( v48 + 88), v9); 
v55 = ( char *)&stru_74575F._IO_read_ptr + 5; 
v55 = ( char *)&stru_74575F._IO_read_end + 3; 
v26 = ( char *)&stru_74575F._IO_buf_end + 6; 
fprintf( outfile, ( const char *)&stru_74575F._IO_backup_base, v25); 
fwrite( ( char *)&stru_74575F._chain + 2, 1uLL, 2uLL, file); 
fatal_insn( "Attempt to delete prologue/epilogue insn:", insn, "flow.c", 1615, "propagate_one_insn"); 
rtx result; // rax 
rtx v6; // r13 
rtx v7; // rax 
rtx v12; // r12 
rtx v13; // rax 
rtx v14; // rax 
v8 = gen_reg_rtx( ( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
v6 = protect_from_queue( x->fld[0].rtx, 0); 
if ( x->fld[0].rtx != v6 ) 
v7 = copy_rtx( x); 
rtx rtwint; // r13 
rtx *v31; // r10 
rtx v36; // rsi 
rtx v47; // rax 
rtx *v52; // r15 
rtx v59; // rdi 
rtx *v62; // r10 
if ( ( sch_istable[( unsigned __int8)ch_0] & 0xC00) == 0 ) 
if ( v6 == -1 || ( sch_istable[( unsigned __int8)v6] & 0xC00) != 0 ) 
rtx v170; // rax 
rtx v172; // rax 
rtx v189; // rdx 
rtx v277; // rax 
rtx v279; // rax 
rtx v412; // rdx 
rtx v481; // rdx 
rtx v755; // rdx 
recog_data_0.insn = 0LL; 
frees = undobuf_0.frees; 
if ( undobuf_0.frees ) 
undobuf_0.frees = undobuf_0.frees->next; 
undobuf_0.frees = undobuf_0.frees->next; 
undos = undobuf_0.undos; 
undobuf_0.undos = frees; 
reg_dead_regno = v20; 
reg_dead_flag = 0; 
reg_dead_endregno = v20 + 1; 
v25 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v21) - 5) < 2) + 1; 
reg_dead_flag = 0; 
reg_dead_endregno = v20 + v25; 
if ( ( unsigned int)v20 >= reg_dead_endregno ) 
v29 = reg_dead_flag; 
if ( reg_dead_flag ) 
if ( reg_dead_flag != 1 ) 
if ( find_regno_note( nonnote_insn, REG_DEAD, reg_dead_regno) ) 
v27 = reg_dead_endregno; 
LODWORD( v20) = reg_dead_regno; 
v7 = ( __m128i *)&costs_0[v4->fld[0].rtuint]; 
v8 = ix86_memory_move_cost( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), ( reg_class)v3, 1); 
v8 = ix86_memory_move_cost( ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), ( reg_class)v3, 1); 
record_address_regs( v4->fld[v34].rtx, ( reg_class)v3, scale); 
rtx v12; // rbp 
rtx v15; // rax 
v12 = ( rtx)rtx[1]; 
if ( ( *( _DWORD *)v12 & 0x8000FFFF) == -2147483587 ) 
v4 = *( _WORD *)v12; 
if ( *( _WORD *)v12 == 61 ) 
v15 = find_base_value( v12); 
v15 = find_base_value( v12); 
if ( v15 ) 
v12 = v15; 
v12 = v15; 
v13 = *( _WORD *)v12; 
if ( ( unsigned __int16)( *( _WORD *)v12 - 67) <= 1u ) 
v4 = *( _WORD *)v12; 
val = v12; 
if ( !*( ( _BYTE *)v12 + 2) ) 
v10 = lang_hooks_0.expand_constant( exp); 
v34 = ( tree_node *)v33[4]; 
v34 = ( tree_node *)v33[4]; 
rtx v18; // rcx 
rtx v20; // rbx 
rtx rtwint; // rax 
rtx v45; // rsi 
rtx v59; // rax 
rtx dest_rega; // [rsp+8h] [rbp-80h] BYREF 
rtx mult_vala; // [rsp+18h] [rbp-70h] BYREF 
rtx v70; // [rsp+20h] [rbp-68h] 
v33 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v24); 
rtx v21; // rax 
v31 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v28) - 5) < 2) + 1; 
v15 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v16) - 5) < 2) + 1; 
if ( v10 != 69 || ( v21 = *v9, LODWORD( v22) = *( _DWORD *)*v9 - 1, ( int)v22 < 0) ) 
if ( loc != ( rtx *)&v21->fld[v22] ) 
v23 = refers_to_regno_for_reload_p( regno, endregno, v21->fld[( int)v22].rtx, loc); 
v21 = *v9; 
v25 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v27) - 5) < 2) + 1; 
v16 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v17) - 5) < 2) + 1; 
if ( reg_pref_0 ) 
return reg_pref_0[regno].altclass; 
v27 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v26) - 5) < 2) + 1; 
v31 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v30) - 5) < 2) + 1; 
v41 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v40) - 5) < 2) + 1; 
v45 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v44) - 5) < 2) + 1; 
v15 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v14) - 5) < 2) + 1; 
v19 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v18) - 5) < 2) + 1; 
v11 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v4) - 5) <= 1; 
( machine_mode)*( unsigned __int8 *)( v11.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)rtx + 2)) 
v12 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v15) - 5) < 2) + 1; 
v12 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v18) - 5) < 2) + 1; 
v40 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v41) - 5) < 2) + 1; 
v53 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v54) - 5) < 2) + 1; 
rtx insn; // [rsp+20h] [rbp-78h] 
rtx y; // [rsp+58h] [rbp-40h] 
y = reg; 
y = reg; 
insn = ( rtx)rtwint; 
&& ( rtx_equal_p( *( rtx *)( v30 + 8), y) || *( _WORD *)v31 != 66 && reg_overlap_mentioned_p( y, v31)) ) 
&& ( rtx_equal_p( *( rtx *)( v30 + 8), y) || *( _WORD *)v31 != 66 && reg_overlap_mentioned_p( y, v31)) ) 
if ( rtx_equal_p( rtx, y) || *( _WORD *)rtx != 66 && reg_overlap_mentioned_p( y, rtx) ) 
if ( rtx_equal_p( rtx, y) || *( _WORD *)rtx != 66 && reg_overlap_mentioned_p( y, rtx) ) 
rtwint = ( unsigned __int16 *)insn; 
&& ( rtx_equal_p( *( rtx *)( v43 + 8), y) || *( _WORD *)v44 != 66 
&& reg_overlap_mentioned_p( y, v44)) ) 
if ( rtx_equal_p( v25, y) || *( _WORD *)v25 != 66 && reg_overlap_mentioned_p( y, v25) ) 
rtx i; // [rsp+0h] [rbp-78h] 
rtx rega; // [rsp+38h] [rbp-40h] 
rega = reg; 
for ( i = v2; ; rtwint = ( _DWORD *)i->fld[0].rtwint ) 
for ( i = v2; ; rtwint = ( _DWORD *)i->fld[0].rtwint ) 
&& ( rtx_equal_p( *( rtx *)( v31 + 8), rega) || *( _WORD *)v32 != 66 && reg_overlap_mentioned_p( rega, v32)) ) 
&& ( rtx_equal_p( *( rtx *)( v31 + 8), rega) || *( _WORD *)v32 != 66 && reg_overlap_mentioned_p( rega, v32)) ) 
if ( rtx_equal_p( rtx, rega) || *( _WORD *)rtx != 66 && reg_overlap_mentioned_p( rega, rtx) ) 
if ( rtx_equal_p( rtx, rega) || *( _WORD *)rtx != 66 && reg_overlap_mentioned_p( rega, rtx) ) 
&& ( rtx_equal_p( *( rtx *)( v42 + 8), rega) || *( _WORD *)v43 != 66 
&& reg_overlap_mentioned_p( rega, v43)) ) 
if ( rtx_equal_p( v26, rega) || *( _WORD *)v26 != 66 && reg_overlap_mentioned_p( rega, v26) ) 
v25 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v26) - 5) < 2) + 1; 
if ( *( ( _DWORD *)&mode_class_0 + v2) == 2 ) 
v7 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)mode) - 5) < 2) + 1; 
v23 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v20) - 5) <= 1; 
*( _OWORD *)&e->pred_next = 0LL; 
*( _OWORD *)&e->src = 0LL; 
*( _OWORD *)&e->insns = 0LL; 
*( _OWORD *)&e->flags = 0LL; 
rtx v4; // r8 
rtx v5; // rax 
if ( !rtx_equal_p( rtx, memref->fld[0].rtx) || ( v4 = memref, v3 != *( ( unsigned __int8 *)memref + 2)) ) 
v5 = gen_rtx_fmt_e0( MEM, v3, rtx); 
*( _QWORD *)&v5[1] = 0LL; 
v4 = v5; 
v4 = v5; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 8 | *( ( _BYTE *)v5 + 3) & 0xF7; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 8 | *( ( _BYTE *)v5 + 3) & 0xF7; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 0x10 | ( unsigned __int8)v5 & 0xEF; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 0x10 | ( unsigned __int8)v5 & 0xEF; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 0x80 | ( unsigned __int8)v5 & 0x7F; 
rtx v4; // r8 
rtx v5; // rax 
if ( !rtx_equal_p( rtx, memref->fld[0].rtx) || ( v4 = memref, v3 != *( ( unsigned __int8 *)memref + 2)) ) 
v5 = gen_rtx_fmt_e0( MEM, v3, rtx); 
*( _QWORD *)&v5[1] = 0LL; 
v4 = v5; 
v4 = v5; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 8 | *( ( _BYTE *)v5 + 3) & 0xF7; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 8 | *( ( _BYTE *)v5 + 3) & 0xF7; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 0x10 | ( unsigned __int8)v5 & 0xEF; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 0x10 | ( unsigned __int8)v5 & 0xEF; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
*( ( _BYTE *)v4 + 3) = ( _BYTE)v5; 
LOBYTE( v5) = *( ( _BYTE *)memref + 3) & 0x80 | ( unsigned __int8)v5 & 0x7F; 
if ( ( *( &mode_class_0 + v4) & 0xFFFFFFFB) != 2 ) 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( v9.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
if ( !warning_message_3 ) 
warning_message_3 = 1; 
v35 = ( tree_node *)i5[5]; 
sprintf( p, &off_698152[1], v6); 
fancy_abort( ( const char *)&label, 2184, "resolve_operand_name_1"); 
timevar_push( TV_VARCONST_0); 
timevar_pop( TV_VARCONST_0); 
timevar_push( TV_SYMOUT_0); 
timevar_pop( TV_SYMOUT_0); 
reverse_condition_0( ( rtx_code)v1); 
return CSWTCH_103[v1]; 
if ( *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( v9.rtwint + 2)) != 4 ) 
v12 = reversed_comparison_code_parts( ( rtx_code)*( _WORD *)exp, last_value->fld[0].rtx, *( rtx *)&last_value[1], 0LL); 
v6 = *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( v4.rtwint + 2)); 
v6 = *( ( _DWORD *)&mode_class_0 + *( unsigned __int8 *)( *( _QWORD *)&comparison[1] + 2LL)); 
return CSWTCH_103[v7]; 
v5 = *( ( _DWORD *)&mode_class_0 + v4); 
reverse_condition_0( ( rtx_code)v7); 
return CSWTCH_103[v7]; 
result = CSWTCH_103[v6]; 
fancy_abort( ( const char *)&a, 8957, "rtl_for_decl_location"); 
if ( *( ( _DWORD *)&mode_class_0 + v18) == 1 ) 
|| ( v28 = *( ( unsigned __int8 *)v2 + 2), ( *( &mode_class_0 + v28) & 0xFFFFFFF9) != 1) ) 
v18 = cselib_lookup( x, ( machine_mode)*( ( unsigned __int8 *)x + 2), 0); 
v17 = cselib_lookup( y, ( machine_mode)*( ( unsigned __int8 *)y + 2), 0); 
v6 = gen_lowpart_for_combine( ( machine_mode)*( unsigned __int8 *)( v5.rtwint + 2), x); 
v4 = gen_lowpart_for_combine( ( machine_mode)*( unsigned __int8 *)( v3.rtwint + 2), y); 
if ( *( _OWORD *)&x == 0LL ) 
rtx real_insn; // rbx 
rtx *v24; // r12 
rtx *i; // rbp 
rtx v27; // rdx 
rtx v28; // rcx 
real_insn = next_real_insn( rtx); 
result = real_insn == next_real_insn( v4->fld[0].rtx); 
v37 = subreg_regno_offset( v36, ( machine_mode)*( unsigned __int8 *)( v34.rtwint + 2), v35, ( machine_mode)v13); 
v37 = subreg_regno_offset( v36, ( machine_mode)*( unsigned __int8 *)( v34.rtwint + 2), v35, ( machine_mode)v13); 
( machine_mode)*( unsigned __int8 *)( v32.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)v4 + 2)); 
rtx *v12; // r14 
v12 = ( rtx *)( rtwint + 2); 
while ( !rtx_varies_p( *v12, for_alias) ) 
if ( ++v12 == ( rtx *)v13 ) 
object_base = ( cpp_buffer_0 *)pfile->buffer_ob.object_base; 
v10 = ( cpp_buffer_0 *)( next_free + 144); 
cselib_lookup( rtwint->fld[0].rtx, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), 1); 
add_dependence_2( insn, pending_read_insns->fld[0].rtx, ( reg_note)pending_mem); 
add_dependence_1( insn, pending_write_insns->fld[0].rtx, ( reg_note)pending_mema); 
add_dependence_2( insn, k->fld[0].rtx, ( reg_note)j); 
rtx v22; // r14 
rtx pending_read_insns; // rbx 
rtx pending_read_mems; // r15 
rtx v25; // rbx 
rtx i; // r15 
rtx v27; // rbx 
rtx v31; // rsi 
rtx v34; // rax 
fancy_abort( ( const char *)&a, 9575, "scope_die_for"); 
if ( ( unsigned int)debug_info_level_0 > DINFO_LEVEL_TERSE && ( *( ( _BYTE *)&rtl[1] + 1) & 0x40) == 0 ) 
fancy_abort( ( const char *)&a, 9604, "scope_die_for"); 
v5 = *( tree_node **)( low + 104); 
if ( initial != ( tree_node *)global_trees ) 
if ( initial != ( tree_node *)global_trees ) 
v7 = *( tree_node **)( v6 + 8); 
v7 = ( tree_node *)ref[1]; 
honor_readonly = lang_hooks_0.honor_readonly; 
if ( !lang_hooks_0.honor_readonly || ( v15 & 0x10) == 0 && ( *( ( _BYTE *)&elements->block.common + 17) & 0x10) == 0 ) 
mem_attrs = get_mem_attrs( alias, v35, const_int_rtx[64], v8, align, ( machine_mode)*( ( unsigned __int8 *)ref + 2)); 
v24 = ( tree_node *)*( &global_trees + 15); 
if ( !ix86_hard_regno_mode_ok( v7, *( ( machine_mode *)&rld + 26 * r + 7)) ) 
if ( *( _BYTE *)( v8 + 2) && !ix86_hard_regno_mode_ok( v7, ( machine_mode)*( unsigned __int8 *)( v8 + 2)) ) 
if ( !ix86_hard_regno_mode_ok( v7, ( machine_mode)*( unsigned __int8 *)( v9 + 2)) ) 
mark_reload_reg_in_use( v11, *( ( _DWORD *)v12 + 18), *( ( reload_type *)v12 + 23), *( ( machine_mode *)v12 + 7)); 
mark_reload_reg_in_use( v11, *( ( _DWORD *)v12 + 18), *( ( reload_type *)v12 + 23), *( ( machine_mode *)v12 + 7)); 
result = gen_rtx_fmt_ee( EXPR_LIST, ( machine_mode)kind, datum, insn[3].fld[0].rtx); 
rtx v9; // rax 
rtx v10; // rax 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)x + 2), x, c); 
v10 = gen_rtx_CONST_INT( VOIDmode, v2->fld[0].rtwint + c->fld[0].rtwint); 
v6 = v10; 
v9 = sge_plus_constant( *( rtx *)&x[1], c); 
rtx = v9; 
if ( ( tree_node *)*( &global_trees + 10) == section_name ) 
if ( ( tree_node *)*( &global_trees + 9) == section_name ) 
if ( ( tree_node *)*( &global_trees + 8) == section_name ) 
if ( ( tree_node *)*( &global_trees + 7) == section_name ) 
if ( ( tree_node *)*( &global_trees + 6) == section_name ) 
v13 = *( _OWORD *)&imag->block.vars != *( _OWORD *)&elements->block.vars 
v13 = *( _OWORD *)&imag->block.vars != *( _OWORD *)&elements->block.vars 
v2 = ( tree_node *)*( &global_trees + 25); 
if ( section_name == ( tree_node *)*( &global_trees + 24) ) 
v17 = simplify_and_const_int( 0LL, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), *( rtx *)&rtx[1], v8); 
v18 = simplify_and_const_int( 0LL, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), rtx->fld[0].rtx, v8); 
v19 = gen_binary( ( rtx_code)*( _WORD *)rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v18, v17); 
v19 = gen_binary( ( rtx_code)*( _WORD *)rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v18, v17); 
rtx pool_constant; // r14 
rtx result; // rax 
rtx v25; // rax 
rtx v28; // rax 
rtx v47; // rax 
rtx compound_operation; // r14 
rtx v59; // rbp 
rtx v67; // rax 
rtx v81; // rax 
rtx v85; // rax 
rtx v15; // r8 
rtx result; // rax 
rtx v21; // rax 
rtx v28; // rax 
rtx v29; // r9 
rtx v57; // rax 
rtx v63; // rax 
rtx v12; // rax 
rtx compound_operation; // rax 
rtx v18; // rdx 
rtx v31; // rdx 
rtx last_value; // rax 
rtx v60; // r10 
rtx *v65; // rdi 
rtx v10; // r15 
rtx v14; // rax 
rtx result; // rax 
v10 = op1; 
result = simplify_gen_subreg( ( machine_mode)*( ( unsigned __int8 *)x + 2), v30, v29, v27); 
return simplify_relational_operation( ( rtx_code)*( _WORD *)x, v9, v7.rtx, ( rtx)v8); 
( rtx_code)*( _WORD *)x, 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
( rtx_code)*( _WORD *)x, 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
return simplify_binary_operation( ( rtx_code)v2, v1, v6, rtx); 
&& *( ( _DWORD *)&mode_class_0 + v15) == 1 ) 
rtx = simplify_subreg( outermode, op->fld[0].rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v10); 
rtx = simplify_subreg( outermode, v29.rtx, ( machine_mode)*( unsigned __int8 *)( v29.rtwint + 2), v30); 
if ( *( ( _DWORD *)&mode_class_0 + v15) != 1 ) 
if ( *( ( _DWORD *)&mode_class_0 + v40) != 1 ) 
&& ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + ( int)mode) != 8 
&& ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + ( int)mode) != 8 
&& ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + ( int)mode) != 8 
&& ( ( *( &mode_class_0 + ( int)mode) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + ( int)mode) != 8 
v19 = simplify_relational_operation( ( rtx_code)*( _WORD *)op0, op0_mode, v16.rtx, ( rtx)v17); 
rtx result; // rax 
rtx v26; // rax 
|| ( v26 = simplify_subreg( v20, pool_constant, pool_mode, 0), v11 = v38, 
*( _OWORD *)v37.r = *( _OWORD *)&args.operand; 
v2 = size_htab_11; 
if ( !size_htab_11 ) 
size_htab_11 = htab_create( 0x400uLL, size_htab_hash, size_htab_eq, 0LL); 
ggc_add_deletable_htab( size_htab_11, 0LL, 0LL); 
new_const_10 = make_node( INTEGER_CST); 
ggc_add_tree_root( &new_const_10, 1); 
v2 = size_htab_11; 
v4 = new_const_10; 
new_const_10->int_cst.int_cst.low = number; 
v4 = new_const_10; 
v10 = new_const_10; 
*slot = new_const_10; 
new_const_10 = make_node( INTEGER_CST); 
v9 = sch_istable[*( v7 - 1)]; 
v13 = cpp_trigraph_map[*( ( unsigned __int8 *)v5->cur + 1)]; 
v4 = cpp_trigraph_map[v17]; 
*( _OWORD *)&result->left = 0LL; 
fancy_abort( ( const char *)&a, 4588, "AT_ref"); 
fancy_abort( ( const char *)&a, 5055, "splice_child_die"); 
recog_data_0.operand[0] = v3.rtx; 
recog_data_0.operand[1] = ( rtx)v146; 
return gen_split_1133( recog_data_0.operand); 
recog_data_0.operand[1] = ( rtx)v48; 
return gen_split_1135( recog_data_0.operand); 
recog_data_0.operand[0] = v3.rtx; 
recog_data_0.operand[1] = v116; 
return gen_split_943( recog_data_0.operand); 
recog_data_0.operand[0] = v3.rtx; 
recog_data_0.operand[1] = ( rtx)v45; 
recog_data_0.insn = 0LL; 
recog_data_0.operand[0] = ( rtx)v5; 
return gen_split_1178( recog_data_0.operand); 
recog_data_0.operand[0] = ( rtx)v5; 
return ( rtx)gen_split_1179( &recog_data_0); 
recog_data_0.operand[0] = ( rtx)v40; 
recog_data_0.operand[1] = v184; 
recog_data_0.operand[2] = v186; 
recog_data_0.operand[3] = v188; 
recog_data_0.operand[4] = v190; 
return gen_split_938( recog_data_0.operand); 
recog_data_0.operand[1] = v170; 
recog_data_0.operand[0] = *( rtx *)( v44 + 16); 
if ( !rtx_equal_p( *( rtx *)( v446 + 8), recog_data_0.operand[1]) ) 
recog_data_0.operand[2] = v453; 
&& recog_data_0.operand[1]->fld[0].rtint != 2 ) 
rtx v7; // rdi 
rtx v8; // rdx 
v7 = rtx; 
v8 = rtx; 
if ( *( _WORD *)v8 == 37 && ( unsigned int)( v8[2].fld[0].rtint + 98) <= 5 ) 
if ( *( _WORD *)v8 == 37 && ( unsigned int)( v8[2].fld[0].rtint + 98) <= 5 ) 
v5 = v8; 
if ( v7 == v8 ) 
if ( v7 == v8 ) 
v7 = rtx; 
v9 = ( __int64)v7[1]; 
v10 = ( __int64)v8[1]; 
v8[1].fld[0].rtwint = ( __int64)v7; 
v8[1].fld[0].rtwint = ( __int64)v7; 
*( _QWORD *)&v8[1] = v9; 
*( _QWORD *)( v9 + 24) = v8; 
if ( ( *( &mode_class_0 + v1) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + v1) != 8 ) 
if ( ( *( &mode_class_0 + v1) & 0xFFFFFFFB) != 2 && *( ( _DWORD *)&mode_class_0 + v1) != 8 ) 
*( _OWORD *)( v1 + 8) = 0LL; 
rtx v12; // r14 
rtx v13; // rax 
rtx v14; // r14 
rtx v15; // rax 
v16 = gen_reg_rtx( ( machine_mode)*( unsigned __int8 *)( to->fld[0].rtwint + 2)); 
v12 = protect_from_queue( to->fld[0].rtx, 0); 
if ( v12 != to->fld[0].rtx ) 
v13 = copy_rtx( to); 
v13->fld[0].rtwint = ( __int64)v12; 
v13->fld[0].rtwint = ( __int64)v12; 
rtx = v13; 
v14 = protect_from_queue( to->fld[0].rtx, 0); 
v7 = expand_shift_1( ( tree_code)v10, ( machine_mode)v7, v32, 0LL, ( rtx)1, v33); 
v7 = gen_lowpart( ( machine_mode)*( ( unsigned __int8 *)v8 + 2), v7); 
v7 = convert_to_mode( ( machine_mode)*( ( unsigned __int8 *)v8 + 2), v7, 1); 
v19 = operand_sub*(short *)0xforce( op0, offset, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v19 = operand_sub*(short *)0xforce( op0, offset, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
( machine_mode)*( unsigned __int8 *)( op0->fld[0].rtwint + 2)); 
v18 = operand_sub*(short *)0xforce( 
v9 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v5) - 5) < 2) + 1; 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v5) - 5) <= 1 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v10) - 5) > 1 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v5) - 5) <= 1 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v2) - 5) > 1 ) 
v8 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v4) - 5) < 2) + 1; 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v4) - 5) <= 1 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v13) - 5) > 1 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)v4) - 5) <= 1 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + ( int)ymode) - 5) > 1 ) 
rtx v18; // rax 
rtx v21; // r11 
rtx v25; // rsi 
rtx *v26; // rdi 
rtx v35; // rax 
rtx v36; // rax 
rtx v37; // rax 
rtx v38; // rax 
rtx v44; // rax 
rtx ***v17; // r12 
rtx **v18; // rdx 
rtx **v19; // r11 
rtx v28; // rax 
rtx v29; // rax 
rtx v35; // rax 
rtx *v41; // rdi 
rtx ***v48; // r14 
rtx *v56; // rdi 
rtx *v59; // rdx 
rtx **v60; // rcx 
rtx v61; // rax 
rtx **v70; // r14 
rtx v71; // rsi 
rtx v35; // r14 
rtx v45; // rax 
rtx *dest; // [rsp+0h] [rbp-78h] 
rtx desta; // [rsp+0h] [rbp-78h] 
rtx op0b; // [rsp+8h] [rbp-70h] 
rtx src_copy; // [rsp+10h] [rbp-68h] BYREF 
rtx x; // [rsp+18h] [rbp-60h] 
rtx inner; // [rsp+38h] [rbp-40h] BYREF 
dest = loc; 
if ( ( *( &mode_class_0 + v24) & 0xFFFFFFFB) == 2 ) 
if ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + *( ( unsigned __int8 *)*v19 + 2)) - 5) <= 1 ) 
if ( ( *( &mode_class_0 + ( unsigned __int8)v91) & 0xFFFFFFFB) == 2 ) 
if ( ( *( &mode_class_0 + v90) & 0xFFFFFFFB) != 2 ) 
if ( ( *( &mode_class_0 + v35) & 0xFFFFFFFB) == 2 ) 
v92 = ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v91) - 5) <= 1; 
if ( ( *( &mode_class_0 + v86) & 0xFFFFFFFB) == 2 ) 
*( _WORD *)pat = swap_condition( ( rtx_code)*( _WORD *)pat); 
*( _QWORD *)b = *( _QWORD *)XFlittlenan; 
b[4] = XFlittlenan[4]; 
*( _OWORD *)&result->decl.common.type = 0LL; 
if ( *( _OWORD *)&t1->block.vars == 0LL ) 
rtx base_term; // rax 
rtx addr_0; // rax 
addr_0 = get_addr_0( x->fld[0].rtx); 
rtx = addr_0; 
base_term = find_base_term( rtx); 
if ( !base_term ) 
base_term = find_base_term( rtx); 
if ( !base_term ) 
if ( *( _WORD *)base_term == 67 || ( *( _DWORD *)base_term & 0x400FFFF) == 67108932 ) 
if ( *( _WORD *)base_term == 67 || ( *( _DWORD *)base_term & 0x400FFFF) == 67108932 ) 
if ( !base_alias_check( rtx, v13, ( machine_mode)*( ( unsigned __int8 *)x + 2), mem_mode) ) 
( machine_mode)*( unsigned __int8 *)( v1.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2)); 
( machine_mode)*( unsigned __int8 *)( v1.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2)); 
( machine_mode)*( unsigned __int8 *)( v2 + 2), 
( machine_mode)*( unsigned __int8 *)( v1.rtwint + 2)) 
( machine_mode)*( unsigned __int8 *)( v3 + 2), 
( machine_mode)*( unsigned __int8 *)( v2 + 2)) 
( machine_mode)*( unsigned __int8 *)( v4 + 2), 
( machine_mode)*( unsigned __int8 *)( v3 + 2)) 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)( v4 + 8) + 2LL), 
( machine_mode)*( unsigned __int8 *)( v4 + 2)) 
( machine_mode)( BYTE5( index_type->block.abstract_origin) >> 1), 
( machine_mode)( BYTE5( range->common.type->block.abstract_origin) >> 1), 
emit_cmp_and_jump_insns( v19, v24, GTU, 0LL, ( machine_mode)( unsigned __int8)v25, 1, default_label); 
v26 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)v25, table_label); 
v29 = gen_rtx_fmt_ee( MULT, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v19, v28); 
v31 = gen_rtx_fmt_ee( PLUS, ( machine_mode)( ( ( BYTE3( target_flags) & 2) != 0) + 4), v29, v26); 
v32 = memory_address_noforce( ( machine_mode)v30, v31); 
v35 = gen_reg_rtx( ( machine_mode)v33); 
v38 = gen_rtx_MEM( ( machine_mode)v36, v34); 
rtx v14; // rdx 
rtx v19; // rax 
rtx v20; // rdx 
rtx v22; // rax 
rtx v36; // rax 
rtx i; // r14 
rtx *v45; // rax 
rtx *v59; // rax 
rtx *v61; // rdx 
rtx *v66; // rdi 
rtx *v80; // rbp 
if ( ( tree_node *)*( &global_trees + 5) == section_name ) 
if ( ( tree_node *)*( &global_trees + 4) == section_name ) 
if ( ( tree_node *)*( &global_trees + 3) == section_name ) 
if ( ( tree_node *)*( &global_trees + 2) == section_name ) 
if ( ( tree_node *)*( &global_trees + 1) == section_name ) 
v4 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v6) - 5) < 2) + 1; 
v22 = ( ( unsigned int)( *( ( _DWORD *)&mode_class_0 + v19) - 5) < 2) + 1; 
v11 = ( tree_node *)global_trees; 
v14 = ( change_t_0 *)xrealloc( v11, v13); 
rtx v34; // rcx 
rtx *v36; // r12 
rtx v38; // rax 
rtx x; // [rsp+18h] [rbp-60h] 
rtx froma; // [rsp+20h] [rbp-58h] 
x = v4; 
froma = v8; 
v34 = *v13; 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)ref + 2), rtx) ) 
if ( statement_code_p( ( tree_code)*( ( unsigned __int8 *)*v6 + 16)) && ( *( ( _BYTE *)*v6 + 19) & 4) == 0 ) 
result = lang_hooks_0.tree_inlining.walk_subtrees( ( tree_node **)v6, walk_subtrees, func, data, htab_); 
result = lang_hooks_0.tree_inlining.walk_subtrees( ( tree_node **)v6, walk_subtrees, func, data, htab_); 
&& !lang_hooks_0.tree_inlining.tree_chain_matters_p( ( tree_node *)*v6) ) 
&& !lang_hooks_0.tree_inlining.tree_chain_matters_p( ( tree_node *)*v6) ) 
v4 = general_operand( x, ( machine_mode)*( ( unsigned __int8 *)x + 2)); 
v6 = test_insn_13; 
if ( !test_insn_13 ) 
test_insn_13 = insn_raw; 
ggc_add_rtx_root( &test_insn_13, 1); 
v6 = test_insn_13; 
if ( ( _DWORD)v5 == 16 || ( _DWORD)v5 == 22 || ( *( &mode_class_0 + v5) & 0xFFFFFFFB) == 1 ) 
if ( ( _DWORD)v3 != 16 && ( _DWORD)v3 != 22 && ( *( &mode_class_0 + v3) & 0xFFFFFFFB) != 1 ) 
