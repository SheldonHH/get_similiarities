free_buffs = ( _cpp_buff_0 *)&v11[v10]; 
free_buffs = ( _cpp_buff_0 *)&v12[v11]; 
result = ( _cpp_buff_0 *)&v9[v8]; 
p_free_buffs = ( _cpp_buff_0 *)&pfile->free_buffs; 
v4 = ( tokenrun_0 *)xmalloc( 0x20uLL); 
v5 = ( cpp_token_0 *)xmalloc( 0x1770uLL); 
v5 = ( tokenrun_0 *)xmalloc( 0x20uLL); 
v6 = ( cpp_token_0 *)xmalloc( 0x1770uLL); 
u_buff = ( _cpp_buff_0 *)&result[v10]; 
v8 = mem_loc_descriptor( rtl->fld[0].rtx, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)rtl)); 
v36 = gen_rtx_MEM( ( machine_mode)*( ( unsigned __int8 *)v36 + 2), fixed); 
&& ( _DWORD)v10 == reverse_condition( ( rtx_code)v11) 
*hv = ( __PAIR128__( h2, l2) + *( _OWORD *)&l1) >> 64; 
return ( ~( v6 ^ h2) & ( v6 ^ ( ( __PAIR128__( h2, l2) + *( _OWORD *)&l1) >> 64))) >> 63; 
v6 = dwarf2out_cfi_label_label_num++; 
v3 = dwarf2out_cfi_label_label; 
sprintf( dwarf2out_cfi_label_label, "*.%s%u", "LCFI", v6); 
assemble_name( ( FILE *)asm_out_file, dwarf2out_cfi_label_label); 
if ( mode_class_0[( unsigned __int8)v16] != MODE_FLOAT ) 
*( _OWORD *)&v7->left = 0LL; 
rtx v13; // rax 
rtx v14; // rbp 
rtx v15; // rbx 
rtx v20; // rax 
rtx v21; // r14 
rtx v23; // r12 
rtx v29; // rax 
_OWORD *v41; // rax 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v11 = ( ( unsigned int)( mode_class_0[v9] - 5) < 2) + 1; 
if ( alloc_aux_for_blocks_initialized ) 
alloc_aux_for_blocks_initialized = 1; 
if ( alloc_aux_for_edges_initialized ) 
alloc_aux_for_edges_initialized = 1; 
*( _OWORD *)&result->count = 0LL; 
*( _OWORD *)&result->aux = 0LL; 
*( _OWORD *)&result->global_live_at_start = 0LL; 
*( _OWORD *)&result->local_set = 0LL; 
*( _OWORD *)&result->pred = 0LL; 
*( _OWORD *)&result->head_tree = 0LL; 
*( _OWORD *)&result->head = 0LL; 
return gen_rtx_fmt_ee( EXPR_LIST, ( machine_mode)kind, val, next); 
rtx *v8; // r14 
rtx *v14; // rbx 
uid_cuid_1 = v3; 
v8 = ( rtx *)xmalloc( ( unsigned int)v7); 
cuid_insn = v8; 
memset( v8, 0, v7); 
v8[v10] = v1; 
v14 = ( rtx *)xmalloc( v13); 
canon_modify_mem_list = v14; 
memset( v14, 0, 8LL * ( int)n_basic_blocks); 
v5 = convert_to_mode( ( machine_mode)v8, v5, 1); 
v4 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v26 = *( ( unsigned __int16 *)&insn_data_0[1234].operand[1] + 8); 
predicate = insn_data_0[1234].operand[1].predicate; 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
free( reg_pref_0); 
if ( reg_pref_0 ) 
reg_pref_0 = reg_pref_buffer; 
v13 = assign_stack_local( ( machine_mode)v9, v12, -( v10 < v11)); 
v13 = assign_stack_local( ( machine_mode)v9, v12, -( v12 != v10)); 
v20 = adjust_address_1( v13, ( machine_mode)*( ( unsigned __int8 *)cfun->emit->x_regno_reg_rtx[v3] + 2), 0LL, 0, 1); 
result = simplify_subreg( v3, v2.rtx, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v2.rtwint), *( _DWORD *)&v1[1]); 
verbatim( off_6763D0, v1); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
result = apply_args_size_size; 
if ( apply_args_size_size < 0 ) 
apply_args_size_size = v1; 
apply_args_size_size = 2 * v1; 
v6 = ( ( unsigned int)( mode_class_0[v5] - 5) < 2) + 1; 
v13 = apply_args_size_size; 
if ( apply_args_size_size % v12 ) 
v13 = v12 + apply_args_size_size - 1 - ( v12 + apply_args_size_size - 1) % v12; 
v13 = v12 + apply_args_size_size - 1 - ( v12 + apply_args_size_size - 1) % v12; 
apply_args_size_size = v13; 
apply_args_size_size = v13 + mode_size[v3]; 
return apply_args_size_size; 
rtx *v9; // rsi 
rtx v10; // rdx 
rtx *p_old; // rdx 
if ( !legitimate_address_p( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)object), rtx, 0) ) 
v9 = ( rtx *)&object[2]; 
v10 = ( rtx)*( ( _QWORD *)v7 + 1); 
v10 = gen_rtx_fmt_E( PARALLEL, VOIDmode, v11); 
rtwint = ( int *)v10->fld[0].rtwint; 
rtwint = ( int *)v10->fld[0].rtwint; 
v9 = ( rtx *)&object[2]; 
validate_change( object, v9, v10, 1); 
validate_change( object, v9, v10, 1); 
v7 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)old_reg + 2)); 
rtx v9; // r13 
rtx v10; // r14 
rtx *v18; // r14 
rtx *v19; // rbp 
v3 = *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)x >> 14) & 0x3FC)); 
v9 = expand_compound_operation( rtx); 
v10 = expand_compound_operation( v8); 
v11 = *( _WORD *)v9; 
if ( ( _DWORD)v11 != *( _WORD *)v10 ) 
if ( ( ( *( _DWORD *)v9->fld[0].rtwint ^ *( _DWORD *)v10->fld[0].rtwint) & 0xFF0000) != 0 ) 
if ( ( ( *( _DWORD *)v9->fld[0].rtwint ^ *( _DWORD *)v10->fld[0].rtwint) & 0xFF0000) != 0 ) 
if ( v9[1] != v10[1] ) 
if ( v9[1] != v10[1] ) 
result = apply_result_size_size; 
if ( apply_result_size_size < 0 ) 
apply_result_size_size = 0; 
v8 = apply_result_size_size; 
if ( apply_result_size_size % v7 ) 
v8 = v7 + apply_result_size_size - 1 - ( v7 + apply_result_size_size - 1) % v7; 
v8 = v7 + apply_result_size_size - 1 - ( v7 + apply_result_size_size - 1) % v7; 
apply_result_size_size = v8; 
apply_result_size_size = v8 + mode_size[v2]; 
apply_result_size_size = 116; 
if ( ( _DWORD)v8 == 46 || ( sch_istable[( unsigned __int8)v8] & 4) != 0 ) 
while ( ( sch_istable[v8] & 4) != 0 ); 
while ( ( sch_istable[v9] & 4) != 0 ); 
v7 = offsettable_address_p( 0, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)op), op->fld[0].rtx); 
if ( mode_class_0[v17] == MODE_FLOAT ) 
sprintf( &text, "*.%s%u", ( const char *)&off_71EE08, ( unsigned int)++assemble_end_function_labelno); 
sprintf( &text, "*.%s%u", ( const char *)&off_71EE08, ( unsigned int)++assemble_end_function_labelno); 
fprintf( ( FILE *)asm_out_file, ".%s%u:\n", ( const char *)&off_71EE08, ( unsigned int)assemble_end_function_labelno); 
fprintf( ( FILE *)asm_out_file, ".%s%u:\n", ( const char *)&off_71EE08, ( unsigned int)assemble_end_function_labelno); 
else if ( in_section_0 != in_text ) 
in_section_0 = in_text; 
fprintf( ( FILE *)asm_out_file, off_71EE04, 6650609LL); 
splay_tree_value v19; // rax 
splay_tree_value v42; // r14 
if ( v17 != ( tree_node *)global_trees ) 
v19 = global_trees; 
if ( v18 && v18 != ( tree_node *)global_trees || dont_output_data || decl->decl.section_name ) 
v19 = global_trees; 
if ( v18 == ( tree_node *)v19 ) 
if ( in_section_0 == in_text ) 
fprintf( ( FILE *)asm_out_file, off_71EE04, "object"); 
v42 = global_trees; 
if ( !v41 || v41 == ( tree_node *)v42 ) 
tree result; // r14 
tree *regno_decl; // rbp 
tree *v106; // rbp 
v1 = off_67BF48; 
*( _OWORD *)&head->first = 0LL; 
if ( *( _OWORD *)current->bits == 0LL ) 
*( _OWORD *)&to->first = 0LL; 
*( _OWORD *)( object_base + 24) = v5; 
*( _OWORD *)&v7.first = 0LL; 
*( _OWORD *)&head->first = 0LL; 
*( _OWORD *)&to->first = 0LL; 
*( _OWORD *)( object_base + 24) = v10; 
_mm_xor_si128( _mm_loadu_si128( ( const __m128i *)v16->bits), ( __m128i)xmm*(short *)0x63FCE0), 
*( _OWORD *)object_base = v10; 
*( _OWORD *)( object_base + 24) = 0LL; 
*( _OWORD *)object_base = 0LL; 
induction_1 *biv; // rbx 
induction_1 *biv; // rbx 
biv = bl_0->biv; 
if ( !biv ) 
v3 = *( ( _WORD *)biv + 50); 
mult_val = biv->mult_val; 
v1 = fold_rtx_mult_add( v1, mult_val, biv->add_val, biv->mode); 
v1 = fold_rtx_mult_add( v1, mult_val, biv->add_val, biv->mode); 
biv = biv->next_iv; 
biv = biv->next_iv; 
if ( biv ) 
rtx v202; // rax 
branch_prob_ignore_next_note = 1; 
if ( branch_prob_ignore_next_note == 1 ) 
branch_prob_ignore_next_note = 0; 
return force_reg( ( machine_mode)BYTE2( v1), x); 
( rtx_code)*( _WORD *)x, 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
return force_reg( ( machine_mode)BYTE2( v1), x); 
*( _OWORD *)&result->common.chain = 0LL; 
if ( v23 != ( tree_node *)global_trees ) 
if ( !v7 || ( v8 = ( tree_node *)*( ( _QWORD *)v7 + 1)) == 0LL ) 
v8 = ( tree_node *)v5; 
v14 = ( tree_node *)low; 
if ( !v4 || ( v5 = ( tree_node *)*( ( _QWORD *)v4 + 1)) == 0LL ) 
v7 = ( const char *)&unk_71B26A; 
v7 = ( const char *)&unk_71B27E; 
v7 = ( const char *)&unk_71B294; 
v7 = ( const char *)&unk_71B2A6; 
v7 = ( const char *)&unk_71B317; 
sprintf( ( char *)&v11[-4], "%s.%d", "__compound_literal", ( unsigned int)var_labelno); 
++var_labelno; 
v32 = ( tree_node *)*( &global_trees + 27); 
if ( v20 == ( tree_node *)global_trees ) 
if ( !v7 || ( v8 = ( tree_node *)*( ( _QWORD *)v7 + 1)) == 0LL ) 
node->int_cst.int_cst = *( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&v4->block.vars; 
reg_dies( v7[2], ( machine_mode)( unsigned __int8)BYTE2( *v7), v5); 
reg_dies( v18[2], ( machine_mode)( unsigned __int8)BYTE2( *v18), v5); 
*( _OWORD *)( p_chain + 9) = 0LL; 
*( _OWORD *)&result->block.vars = *( _OWORD *)d.r; 
*( _OWORD *)&result->block.vars = *( _OWORD *)d.r; 
*( _OWORD *)v6.r = v8; 
*( _OWORD *)&node->block.vars = *( _OWORD *)v6.r; 
*( _OWORD *)&node->block.vars = *( _OWORD *)v6.r; 
*( _OWORD *)( v7 + 9) = 0LL; 
v16 = ( tree_node *)v7; 
if ( !debug_no_type_hash ) 
if ( !v17 || ( v16 = ( tree_node *)*( ( _QWORD *)v17 + 1)) == 0LL ) 
v16 = ( tree_node *)v7; 
*( _OWORD *)( v4 + 9) = 0LL; 
v8 = ( tree_node *)global_trees; 
v8 = ( tree_node *)global_trees; 
if ( v14 != ( tree_node *)global_trees && v16 != ( tree_node *)global_trees ) 
v3 = ( tree_node *)*( &global_trees + 16); 
v3 = ( tree_node *)*( &global_trees + 15); 
v11 = ( tree_node *)*( &global_trees + 16); 
v4 = *( tree_node **)( *( _QWORD *)( low + 8) + 8LL); 
v4 = *( tree_node **)( *( _QWORD *)( low + 8) + 8LL); 
if ( ( tree_node *)v18 == elements ) 
timevar_push( TV_INTEGRATION_0); 
timevar_pop( TV_INTEGRATION_0); 
timevar_pop( TV_INTEGRATION_0); 
timevar_push( TV_EXPAND_0); 
timevar_pop( TV_EXPAND_0); 
if ( if_stack_0[v1].needs_warning ) 
if_stack_0[v1].file, 
if_stack_0[v1].line, 
if ( v3 == ( tree_node *)global_trees ) 
p_int_cst = &elements->int_cst.int_cst; 
elements = ( tree)p_int_cst->low; 
p_int_cst = &elements->int_cst.int_cst; 
v6 = ( if_elt *)xrealloc( if_stack_0, 32 * v5); 
if_stack_0 = v6; 
v7 = if_stack_0; 
if_stack_0[v9].compstmt_count = compstmt_count; 
v1 = if_stack_0; 
if ( if_stack_0[( unsigned int)v0].compstmt_count == if_stack_0[v2].compstmt_count ) 
if ( if_stack_0[( unsigned int)v0].compstmt_count == if_stack_0[v2].compstmt_count ) 
if_stack_0[v2].needs_warning = 1; 
if_stmt = if_stack_0[if_stack_pointer - 1].if_stmt; 
if_stmt = if_stack_0[if_stack_pointer - 1].if_stmt; 
timevar_push( TV_CPP_0); 
timevar_pop( TV_CPP_0); 
if ( ( sch_istable[v3->val.c] & 0xAC) == 0 ) 
v2 = ( tree_node *)*( &global_trees + 16); 
v2 = ( tree_node *)*( &global_trees + 15); 
*( _OWORD *)&result->flags = 0LL; 
*( _OWORD *)&result->insns = 0LL; 
*( _OWORD *)&result->src = 0LL; 
*( _OWORD *)&result->pred_next = 0LL; 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666470); 
v10 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666480); 
v11 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666490); 
v12 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664A0); 
v13 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664B0); 
v14 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664C0); 
v15 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664D0); 
v16 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664E0); 
v17 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664F0); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666470); 
v20 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666480); 
v21 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666490); 
v24 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666470); 
v27 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666480); 
v28 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x666490); 
v29 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664A0); 
v30 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6664B0); 
return general_operand( op, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
rtl_op = first_rtl_op( ( tree_code)*( ( unsigned __int8 *)&exp->block.common + 16)); 
v11 = ( tree_node *)i[13]; 
v19 = ( tree_node *)*( ( _QWORD *)&chain->vector.elements + v18); 
rtx v36; // rax 
rtx v37; // rcx 
rtx v40; // rcx 
rtx v52; // rcx 
rtx *datum; // [rsp+8h] [rbp-60h] 
rtx datuma; // [rsp+8h] [rbp-60h] 
rtx v58; // [rsp+10h] [rbp-58h] 
v58 = insn; 
v13 = *( ( unsigned int *)uid_cuid_1 + v12); 
v14 = *( unsigned __int64 *)( ( char *)reaching_defs[v11]->elms + ( ( *( ( _DWORD *)uid_cuid_1 + v12) >> 3) & 0x1FFFFFF8)); 
|| ( int)v13 >= *( ( _DWORD *)uid_cuid_1 + rtint) ) 
if ( reg_note ) 
v10 = *( _QWORD *)( reg_note->fld[0].rtwint + 8); 
if ( reg_note ) 
v10 = *( _QWORD *)( reg_note->fld[0].rtwint + 8); 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)&v30[2 * v31 + 2] + 2LL)); 
v4 += canon_hash( v53[1], ( machine_mode)*( ( unsigned __int8 *)v53[1] + 2)); 
v4 += canon_hash( ( rtx)v57, ( machine_mode)*( unsigned __int8 *)( v57 + 2)); 
|| !fixed_regs[rtuint] && *( const mode_class *)( ( char *)mode_class_0 + ( ( v5 >> 14) & 0x3FC)) != MODE_CC) ) 
|| insn_data_0[rtint].n_dups > 0) ) 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)x + 2), v4, v5); 
if ( base_alias_check( rtx, mem_addr, ( machine_mode)*( ( unsigned __int8 *)v5 + 2), mem_mode) ) 
if ( mode_class_0[v16] != MODE_INT || ( v19 = mode_bitsize[v16], v19 > 0x40) || ( _DWORD)v19 != 1 ) 
&& ( mode_class_0[*( unsigned __int8 *)( v14->fld[0].rtwint + 2)] == MODE_CC) == ( mode_class_0[( _QWORD)v31] != MODE_CC) ) 
&& ( mode_class_0[*( unsigned __int8 *)( v14->fld[0].rtwint + 2)] == MODE_CC) == ( mode_class_0[( _QWORD)v31] != MODE_CC) ) 
|| mode_class_0[*( unsigned __int8 *)( v14->fld[0].rtwint + 2)] != MODE_INT 
LOBYTE( v21) = mode_class_0[( _QWORD)v31] != MODE_CC; 
LOBYTE( v22) = mode_class_0[v22] == MODE_CC; 
if ( mode_class_0[BYTE2( v23)] == MODE_CC ) 
rtx v14; // rcx 
v14 = function_tail_eff_head; 
if ( v14 ) 
*( _QWORD *)&v14[1] = rtx; 
if ( reg_note && *( __int64 *)( reg_note->fld[0].rtwint + 8) <= 4999 ) 
if ( reg_note && *( __int64 *)( reg_note->fld[0].rtwint + 8) <= 4999 ) 
__m256i arg0; // [rsp+0h] [rbp-58h] BYREF 
arg0.m256i_i64[0] = v9; 
slot_with_hash = htab_find_slot_with_hash( const_int_htab, &arg0, v9, INSERT); 
v7 = gen_rtx_fmt_w( CONST_INT, VOIDmode, arg0.m256i_i64[0]); 
mode_alignment = get_mode_alignment( ( machine_mode)v4); 
arg0.m256i_i64[0] = v6; 
*( _OWORD *)&arg0.m256i_u64[1] = 0LL; 
*( _OWORD *)&arg0.m256i_u64[1] = 0LL; 
arg0.m256i_i64[3] = ( __int64)v7; 
slot = htab_find_slot( mem_attrs_htab, &arg0, INSERT); 
v15 = *( _OWORD *)arg0.m256i_i8; 
*( ( _OWORD *)v14 + 1) = *( _OWORD *)&arg0.m256i_u64[2]; 
*( ( _OWORD *)v14 + 1) = *( _OWORD *)&arg0.m256i_u64[2]; 
rtx v7; // rax 
v7 = gen_rtx_fmt_e0( MEM, v6, rtx); 
*( _QWORD *)&v7[1] = 0LL; 
v8 = *( _DWORD *)memref & 0x8000000 | *( _DWORD *)v7 & 0xF7FFFFFF; 
*( _DWORD *)v7 = v8; 
*( _DWORD *)v7 = v9; 
*( _DWORD *)v7 = v10; 
*( _DWORD *)v7 = v11; 
*( _DWORD *)v7 = *( _DWORD *)memref & 0x1000000 | v11 & 0xFEFFFFFF; 
*( _QWORD *)&v7[1] = memref[1]; 
return v7; 
emit_pop_insn( rtx, old, *( rtx *)&may_move_out_cost[57][15][118 * v11 + 4], EMIT_BEFORE); 
emit_swap_insn( rtx, old, *( rtx *)&may_move_out_cost[57][15][118 * old->reg[( int)v12] + 4]); 
emit_swap_insn( rtx, old, *( rtx *)&may_move_out_cost[57][15][118 * v15 + 4]); 
if ( ( sch_istable[v11] & 4) != 0 && !*( ( _BYTE *)v9 + 1) ) 
rtx v23; // rax 
rtx v28; // rsi 
rtx *clobber_reg; // [rsp+A0h] [rbp-40h] 
n_operands = recog_data_0.n_operands; 
v36 = recog_data_0.n_operands; 
if ( recog_data_0.n_operands > 0 ) 
v8 = recog_data_0.operand[v7]; 
recog_data_0.operand[v7] = rtx; 
clobber_reg = 0LL; 
clobber_reg = ( rtx *)( ( char *)&v35 - ( ( 8 * v13 + 15) & 0xFFFFFFFFFFFFFFF0LL)); 
clobber_reg[( int)v15] = v17; 
v23 = recog_data_0.operand[v22]; 
v23 = recog_data_0.operand[v22]; 
if ( *( _WORD *)v23 == 61 ) 
rtuint = v23->fld[0].rtuint; 
v25 = ( tree_node *)low; 
if ( ( sch_istable[( unsigned __int8)v54] & 4) == 0 ) 
while ( ( sch_istable[v54] & 4) != 0 ); 
if ( ( sch_istable[( unsigned __int8)v73[1]] & 4) != 0 
( sch_istable[( unsigned __int8)*format] & 4) != 0) ) 
while ( ( sch_istable[v79] & 4) != 0 ); 
if ( v67 && ( sch_istable[( unsigned __int8)*v80] & 4) == 0 ) 
if ( ( sch_istable[( unsigned __int8)*v80] & 4) != 0 ) 
while ( ( sch_istable[v87] & 4) != 0 ); 
rtx v11; // rbx 
induction_1 *v28; // rbx 
induction_1 *v28; // rbx 
induction_1 *v34; // rsi 
induction_1 *v34; // rsi 
rtx add_val; // [rsp+10h] [rbp-68h] BYREF 
rtx v38; // [rsp+18h] [rbp-60h] BYREF 
rtx v39; // [rsp+20h] [rbp-58h] BYREF 
rtx ext_val; // [rsp+28h] [rbp-50h] BYREF 
rtx v42; // [rsp+38h] [rbp-40h] 
v7 = ( ( unsigned int)( mode_class_0[v3] - 5) < 2) + 1; 
v11 = ( ( unsigned int)( mode_class_0[v8] - 5) < 2) + 1; 
v15 = ( ( unsigned int)( mode_class_0[v12] - 5) < 2) + 1; 
v19 = ( ( unsigned int)( mode_class_0[v16] - 5) < 2) + 1; 
v21 = ( ( unsigned int)( mode_class_0[v4] - 5) < 2) + 1; 
if ( v21 == v2 && ix86_hard_regno_mode_ok( regno, ( machine_mode)v4) ) 
( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( type->common.type->block.abstract_origin)) >> 1), 
( machine_mode)( ( unsigned __int8)HIBYTE( *( _WORD *)( v41 + 60)) >> 1), 
v57 = classify_argument( ( machine_mode)v54, v53.rttree, s, v55 - ( v56 & 0xFFFFFF00)); 
( machine_mode)( ( unsigned __int8)HIBYTE( *( _WORD *)( v21 + 60)) >> 1), 
( machine_mode)( ( unsigned __int8)HIBYTE( *( _WORD *)( v30->fld[0].rtwint + 60)) >> 1), 
v87 = xmm*(short *)0x67A6D0; 
rtx v2; // rbx 
v2 = rtx; 
if ( *( _WORD *)v2 == 35 ) 
nonnote_insn = prev_nonnote_insn( v2); 
if ( *( _WORD *)v2 != 35 ) 
delete_insn( v2); 
else if ( nonnote_insn != (  struct rtx_def *)v2[1] ) 
reorder_insns( v2, v2, nonnote_insn); 
reorder_insns( v2, v2, nonnote_insn); 
timevar_push( TV_CLEANUP_CFG_0); 
timevar_pop( TV_CLEANUP_CFG_0); 
if ( recog_data_0.n_operands > 0 ) 
v4 = recog_data_0.operand_loc[v1]; 
if ( ( unsigned __int16)( *( _WORD *)recog_data_0.operand[v1] - 66) > 0xCu 
|| !_bittest( &v2, *( _DWORD *)recog_data_0.operand[v1] - 66) ) 
recog_data_0.operand[v1] = v3; 
while ( v1 < recog_data_0.n_operands ); 
if ( recog_data_0.n_dups > 0 ) 
v7 = recog_data_0.dup_loc[v5]; 
*recog_data_0.dup_loc[v5] = v6; 
while ( v5 < recog_data_0.n_dups ); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641440); 
v10 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v11 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
v17 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v16] - 5) < 2) + 1; 
v25 = _mm_shuffle_pd( ( __m128d)v18, ( __m128d)xmm*(short *)0x675E90, 2); 
*( _OWORD *)v7 = 0LL; 
*( _OWORD *)( v7 + 16) = 0LL; 
*( _OWORD *)( v7 + 32) = 0LL; 
*( _OWORD *)( v7 + 48) = 0LL; 
*( _OWORD *)( v3 + 32) = 0LL; 
*( _OWORD *)&v11->pred_next = 0LL; 
*( _OWORD *)&v11->src = 0LL; 
*( _OWORD *)&v11->insns = 0LL; 
*( _OWORD *)&v11->flags = 0LL; 
*( _OWORD *)&sequence_result[2] = 0LL; 
*( _OWORD *)sequence_result = 0LL; 
v7 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v18 = ( ( unsigned int)( mode_class_0[v16] - 5) < 2) + 1; 
fancy_abort( &off_7088B8[4], 4328, "clear_reload_reg_in_use"); 
v25 = _mm_add_epi64( _mm_shuffle_epi32( ( __m128i)v5, 68), ( __m128i)xmm*(short *)0x641400); 
v27 = _mm_shuffle_pd( ( __m128d)v21, ( __m128d)xmm*(short *)0x675E90, 2); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641410); 
v32 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v33 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641430); 
v34 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E870); 
v35 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E880); 
tree v36; // rbx 
tree v40; // rax 
tree v41; // rax 
mode_alignment = get_mode_alignment( ( machine_mode)BYTE2( v2)); 
if ( rtuint <= 0x34 && !ix86_hard_regno_mode_ok( rtuint, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)i)) ) 
induction_1 *giv; // rax 
induction_1 *giv; // rax 
induction_1 *v4; // rcx 
induction_1 *v4; // rcx 
giv = bl_0->giv; 
if ( giv ) 
v4 = bl_0->giv; 
v3 = ( ( *( ( _WORD *)v4 + 50) & 4) == 0LL) + ( unsigned __int64)( unsigned int)v3; 
v4 = v4->next_iv; 
v4 = v4->next_iv; 
while ( v4 ); 
if ( ( *( ( _BYTE *)giv + 100) & 4) == 0 ) 
*( __int64 *)( ( char *)&v52 + 8 * v7 - ( ( 8 * v3 + 15) & 0xFFFFFFFFFFFFFFF0LL)) = ( __int64)giv; 
giv = giv->next_iv; 
giv = giv->next_iv; 
while ( giv ); 
v20 = express_from( ( induction_1 *)v16, *( induction_1 **)( v13 + 8 * v18)); 
rtx v8; // rbx 
rtx v13; // rbx 
rtx v17; // rax 
rtx v21; // rax 
rtx v24; // rax 
reg_last_set_mode = ( machine_mode *)xmalloc( 4LL * nregs); 
v8 = f; 
*( ( _DWORD *)uid_cuid + v8->fld[0].rtint) = v7; 
subst_insn = v8; 
v9 = *( _WORD *)v8; 
v9 += subreg_regno_offset( rtuint, ( machine_mode)BYTE2( v12), v14, ( machine_mode)BYTE2( v11)); 
v9 += subreg_regno_offset( rtuint, ( machine_mode)BYTE2( v12), v14, ( machine_mode)BYTE2( v11)); 
v17 = ( unsigned int)( ( unsigned int)( mode_class_0[( unsigned __int8)v16] - 5) < 2) + 1; 
v27 = subreg_regno_offset( v22, ( machine_mode)BYTE2( v21), v23, v24); 
v30 = ( ( unsigned int)( mode_class_0[BYTE2( v20)] - 5) < 2) + 1; 
&& ( ( int)v15 < 53 || v30 <= ( int)v17 || ( int)v17 >= qty_0[*( ( int *)reg_qty + v15)].size) ) 
min_class = qty_0[v47[v15]].min_class; 
v54 = qty_0; 
*( ( _DWORD *)reg_next_in_qty + v70) = qty_0[v53].first_reg; 
if ( reg_class_subset_p( v56, qty_0[v55].min_class) ) 
qty_0[v55].min_class = v56; 
if ( reg_class_subset_p( v57, qty_0[v55].alternate_class) ) 
if ( memory_address_p( ( machine_mode)*( unsigned __int8 *)( v9.rtwint + 2), global_rtl[2]) ) 
rtx end; // rsi 
end = dest->end; 
end = dest->end; 
if ( *( _WORD *)end != 33 ) 
head = end; 
end = ( rtx)end[1]; 
while ( *( _WORD *)end == 37 && end[2].fld[0].rtint == -96 ); 
while ( *( _WORD *)end == 37 && end[2].fld[0].rtint == -96 ); 
end = ( rtx)rtx[1]; 
end = 0LL; 
nonnote_insn = emit_insns_after( insns, end); 
p_int_cst = &exp->int_cst.int_cst; 
p_int_cst = &exp->int_cst.int_cst; 
high = ( unsigned __int8 *)p_int_cst; 
if ( v21 != *( tree_node **)( p + 5) ) 
v25 = ( tree_node *)v16[4]; 
v13 = lang_hooks_0.expand_constant( exp); 
rtx v10; // r15 
rtx v16; // r14 
rtx regno_note; // r15 
rtx v31; // rbx 
rtx *v37; // rcx 
rtx v64; // r11 
( machine_mode)( unsigned __int8)BYTE2( *v9), 
( machine_mode)BYTE2( v8))] 
v10 = pat_src + 1; 
for ( i = ( int **)v10; ; i = ( int **)( v12 + 2) ) 
( machine_mode)( unsigned __int8)BYTE2( *v14), 
( machine_mode)BYTE2( v13))] 
if ( *( _OWORD *)&t->block.vars == 0LL 
change_stack( src->end, &old, v5, ( emit_where)( *( _WORD *)src->end == 33)); 
v9 = ( tree_node *)high[3]; 
rtx nonnote_insn; // rax 
rtx v4; // r14 
rtx v8; // rax 
nonnote_insn = head; 
if ( rtx_class[*( _WORD *)nonnote_insn] == 105 ) 
v4 = nonnote_insn; 
v4 = nonnote_insn; 
nonnote_insn = next_nonnote_insn( nonnote_insn); 
nonnote_insn = next_nonnote_insn( nonnote_insn); 
&& ( *( _DWORD *)nonnote_insn & 0x10000000) != 0 
while ( nonnote_insn 
&& ( unsigned __int16)*( _DWORD *)nonnote_insn != 36 ); 
for ( i = ( __int64)v4[3]; i; i = *( _QWORD *)( i + 16) ) 
v8 = alloc_INSN_LIST( v4, h_i_d[v7->fld[0].rtint].depend); 
v8 = alloc_INSN_LIST( v4, h_i_d[v7->fld[0].rtint].depend); 
*( ( _BYTE *)v8 + 2) = *( _BYTE *)( i + 2); 
reg_avail_info_0 = v3; 
v13 = reg_avail_info_0; 
v14 = uid_cuid_1; 
v3 = reg_avail_info_0; 
reg_avail_info_0 = 0LL; 
v3 = ( machine_mode)rtl[3]; 
reg_set_0 *v31; // rax 
reg_set_0 *v31; // rax 
reg_set_0 *v34; // rax 
reg_set_0 *v34; // rax 
v31 = reg_set_table[rtuint]; 
if ( v31 ) 
v33 = bmap[*( int *)( v32->data.l[v31->insn->fld[0].rtint] + 88)]; 
v31 = v31->next; 
v31 = v31->next; 
while ( v31 ); 
v34 = reg_set_table[rtuint]; 
if ( v34 ) 
v36 = bmap[*( int *)( v35->data.l[v34->insn->fld[0].rtint] + 88)]; 
v34 = v34->next; 
v34 = v34->next; 
while ( v34 ); 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)rtx), 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641440); 
v5 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v6 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
fancy_abort( &off_7088B8[4], 573, "compute_use_by_pseudos"); 
v18 = ( ( unsigned int)( mode_class_0[v16] - 5) < 2) + 1; 
if ( ( i - 1 < 0) ^ __OFADD__( -1LL, i) | ( i == 1) ) 
rtx v9; // rcx 
v5 = mem_loc_descriptor( x0->fld[0].rtx, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)x0)); 
v9 = x1; 
v9 = x1->fld[0].rtx; 
v10 = v9->fld[0].rtuint; 
v12 = mem_loc_descriptor( x1->fld[0].rtx, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)x1)); 
rtx v24; // rax 
rtx v26; // rax 
induction_1 *v30; // rcx 
induction_1 *v30; // rcx 
rtx src_rega; // [rsp+0h] [rbp-40h] BYREF 
induction_1 *v; // [rsp+8h] [rbp-38h] 
induction_1 *v; // [rsp+8h] [rbp-38h] 
src_rega = src_reg; 
*( &src_rega - 20) = src_reg; 
*( &src_rega - 14) = *mult_val; 
*( &src_rega - 13) = *add_val; 
*( ( _DWORD *)&src_rega - 24) = first_benefit; 
*( ( _WORD *)&src_rega - 38) = 0; 
*( ( _OWORD *)&src_rega - 4) = 0LL; 
*( ( _OWORD *)&src_rega - 4) = 0LL; 
v19 = ( tree_node *)j[4]; 
v6 = mode_class_0[mode]; 
rtx v17; // rdi 
rtx v18; // rsi 
rtx v43; // rdi 
n_operands = ( unsigned __int8)recog_data_0.n_operands; 
if ( !recog_data_0.n_operands || !recog_data_0.n_alternatives ) 
if ( !recog_data_0.n_operands || !recog_data_0.n_alternatives ) 
if ( recog_data_0.n_operands > 0 ) 
memset( s, 255, 4LL * ( unsigned __int8)recog_data_0.n_operands); 
memcpy( dest, recog_data_0.constraints, 8 * n_operands); 
rtx = recog_data_0.operand[v3]; 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v11), 
( machine_mode)BYTE2( v9)); 
v17 = recog_data_0.operand[v14]; 
v17 = recog_data_0.operand[v14]; 
v18 = recog_data_0.operand[v3]; 
fprintf( v25, off_6763D0, x86_64_reg_class_name[v52[i - 1]]); 
rtx v30; // rbx 
rtx resume; // rdi 
rtx label; // rdi 
rtx continue_label; // rsi 
rtx insns; // [rsp+0h] [rbp-28h] BYREF 
insns = v0; 
insns = get_insns( ); 
convert_from_eh_region_ranges_1( &insns, v13, 0); 
if ( reg_note ) 
rtx v39; // r13 
rtx v41; // rax 
rtx v42; // rbp 
rtx v71; // rax 
rtx x; // [rsp+18h] [rbp-3140h] BYREF 
x = *( rtx *)( v27 + 16); 
v28 = *( _WORD *)x; 
for_each_rtx( &x, mark_reg_in_phi, element); 
bitmap_set_bit( element, x->fld[0].rtint); 
v39 = v31[5]; 
|| mode_class_0[mode] != MODE_INT 
if ( mode_class_0[mode] == MODE_INT && mode_class_0[oldmode] == MODE_INT ) 
if ( mode_class_0[mode] == MODE_INT && mode_class_0[oldmode] == MODE_INT ) 
v9 = mode_class_0[mode]; 
v11 = mode_class_0[v8] != MODE_FLOAT; 
v6 = gen_lowpart( ( machine_mode)mode, v6); 
v33 = simplify_gen_subreg( ( machine_mode)v14, v6, ( machine_mode)*( ( unsigned __int8 *)v6 + 2), 0); 
v33 = simplify_gen_subreg( ( machine_mode)v14, v6, ( machine_mode)*( ( unsigned __int8 *)v6 + 2), 0); 
if ( ( unsigned int)( mode_class_0[( unsigned int)v8] - 7) < 2 ) 
to = simplify_gen_subreg( ( machine_mode)v8, target, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)target), 0); 
to = simplify_gen_subreg( ( machine_mode)v8, target, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)target), 0); 
fprintf( file, off_6474F4, 9LL); 
fprintf( file, off_6474F4, 11LL); 
fprintf( file, off_6474F4, 13LL); 
fprintf( file, off_6474F4, 15LL); 
fprintf( file, off_6474F4, 8LL); 
fprintf( file, off_6474F4, 10LL); 
fprintf( file, off_6474F4, 12LL); 
fprintf( file, off_6474F4, 14LL); 
v24 = type_for_mode( ( machine_mode)v19, ( v18 >> 13) & 1); 
v14 = build( ( tree_code)v9, v40, v41, v42, 0LL); 
return build1( ( tree_code)v5, type, expr); 
return build1( ( tree_code)v5, type, expr); 
str_rtx = operand_sub*(short *)0xforce( v20, v10 >> ( 6 - ( v13 == 0)), ( machine_mode)*( ( unsigned __int8 *)v20 + 2)); 
str_rtx = operand_sub*(short *)0xforce( v20, v10 >> ( 6 - ( v13 == 0)), ( machine_mode)*( ( unsigned __int8 *)v20 + 2)); 
rtvec v16; // rax 
v5 = rtx_alloc( ( rtx_code)( unsigned __int16)v2); 
v16 = rtvec_alloc( rtvec->num_elem); 
v5->fld[v13].rtwint = ( __int64)v16; 
if ( v16->num_elem > 0 ) 
rtx v7; // r13 
rtx v9; // r13 
rtx inline_target; // rcx 
rtx v28; // rbp 
rtx v29; // rax 
rtx v50; // rax 
v9 = ( rtx)rtx[2]; 
v9 = single_set_2( rtx, *( rtx *)&rtx[2]); 
v9 = 0LL; 
inline_target = map->inline_target; 
if ( v9 && !inline_target && ( *( _DWORD *)v9->fld[0].rtwint & 0x4000FFFF) == 1073741885 ) 
rtx v9; // rbx 
rtx v11; // r15 
rtx v13; // rsi 
rtx v16; // rax 
rtx v17; // [rsp+0h] [rbp-38h] BYREF 
v17 = v3; 
v9 = map->insn_map[rtx->fld[0].rtint]; 
if ( v9 ) 
*( _DWORD *)( *( _QWORD *)&v9[2] + 8LL) += eh_region_offset; 
v17 = copy_rtx_and_substitute( v10, map, 0); 
v3 = ( tree_node *)ggc_alloc( v2); 
v12 = ( tree_node *)ggc_alloc( v11); 
rtx v17; // rbp 
rtx v19; // rax 
induction_1 *v22; // rsi 
induction_1 *v22; // rsi 
rtx v24; // rax 
rtx v27; // rcx 
rtx v31; // rax 
rtx label_from_map; // rax 
rtx v42; // rax 
rtx v43; // rcx 
rtx nonnote_insn; // rax 
rtx v5; // rdi 
rtx v6; // rax 
rtx *v16; // rbx 
rtx v17; // rax 
rtx *v18; // r14 
rtx v26; // r15 
rtx v28; // rax 
rtx v35; // rax 
rtx v48; // rax 
v5 = rtx_alloc( ( rtx_code)( unsigned __int16)*( _DWORD *)orig); 
v3 = ( tree_node *)ggc_alloc( v2); 
rtx v53; // rax 
rtx v57; // rax 
rtx v61; // rcx 
rtx v63; // rcx 
rtx v64; // rcx 
rtx v65; // rcx 
rtx v67; // rbp 
v10 = rtx_alloc( ( rtx_code)( unsigned __int16)v1); 
v28 = *( _OWORD *)&rtwint[2 * v26 + 6]; 
*( _OWORD *)rtvec[v26 / 2 + 1].elem = v28; 
v29 = *( _OWORD *)&rtwint[2 * v26 + 14]; 
*( _OWORD *)rtvec[v26 / 2 + 3].elem = v29; 
v30 = *( _OWORD *)&rtwint[2 * v26 + 22]; 
*( _OWORD *)rtvec[v26 / 2 + 5].elem = v30; 
v2 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)x + 2)); 
n_operands = recog_data_0.n_operands; 
recog_data_0.operand_type[j] = OP_INOUT; 
if ( *v19 >= 0 || v17 == 38 && recog_data_0.operand_type[j] == OP_OUT ) 
kill_value( recog_data_0.operand[v24], v9); 
kill_value( recog_data_0.operand[v26], v9); 
v33 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v29] - 5) < 2) + 1; 
v34 = ( ( unsigned int)( mode_class_0[v31] - 5) < 2) + 1; 
|| ( oldest_value_reg = find_oldest_value_reg( ( reg_class)*( ( _DWORD *)&regclass_map + rtuint), reg, v9)) == 0LL 
v50 = gen_rtx_fmt_i0( REG, ( machine_mode)v30, v52); 
if ( !*recog_data_0.constraints[v35] ) 
v37 = recog_data_0.operand[v35]; 
if ( recog_data_0.operand_type[v35] ) 
fld = recog_data_0.operand_loc[v35]; 
v38 = recog_data_0.operand[v35]; 
rtx *v24; // [rsp+10h] [rbp-38h] 
v24 = &last_set[rtint]; 
v13 = ( ( unsigned int)( mode_class_0[v14] - 5) < 2) + 1; 
v24[i] = insn; 
v16 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v17] - 5) < 2) + 1; 
fancy_abort( &off_7088B8[4], 1599, "count_pseudo"); 
v5 = ( ( unsigned int)( mode_class_0[v3] - 5) < 2) + 1; 
v16 = ( _DWORD *)( ( char *)&unk_9FF34C + 4 * v9 + 4 * v2); 
v8 = ( ( unsigned int)( mode_class_0[v6] - 5) < 2) + 1; 
v13 = ( _DWORD *)( ( char *)&unk_9FF34C + 4 * ( unsigned int)v8 + 4 * v5); 
return ( cpp_hashnode_0 *)ht_lookup( pfile->hash_table, str, len, HT_ALLOC); 
if ( ( sch_istable[v25] & 0x100) != 0 ) 
v26 = hex_value[v25]; 
if ( ( sch_istable[*v24] & 0x100) != 0 ) 
v27 = hex_value[v25]; 
if ( ( sch_istable[v28] & 0x100) == 0 ) 
v37 = hex_value[v28]; 
if ( ( sch_istable[v28] & 0x100) == 0 ) 
if ( ( sch_istable[**pstr] & 0xAC) != 0 ) 
result = ( cpp_buffer_0 *)pfile->buffer_ob.object_base; 
*( _OWORD *)&result->buf = 0LL; 
*( _OWORD *)&result->line_base = 0LL; 
*( _OWORD *)&result->backup_to = 0LL; 
*( _OWORD *)&result->last_Wtrigraphs = 0LL; 
*( _OWORD *)&result->dir.dev = 0LL; 
*( _OWORD *)&result->dir.len = 0LL; 
*( _OWORD *)&result->dir.next = 0LL; 
rtx v32; // rax 
rtx v53; // rax 
rtx nonnote_insn; // rax 
if ( status != NOT_TAKEN_0 ) 
v32 = cse_process_notes( v31, 0LL); 
rtx[3].fld[0].rtwint = ( __int64)v32; 
if ( v32 ) 
rtx v30; // rax 
v9 = *( ( _DWORD *)uid_cuid_0 + insn->fld[0].rtint); 
if ( data->path[v12].status != NOT_TAKEN_0 ) 
data->path[v12].status = NOT_TAKEN_0; 
v21 = *( ( _DWORD *)uid_cuid_0 + rtint); 
if ( data->path[v43].status != NOT_TAKEN_0 ) 
v30 = rtx; 
v30 = v30[1].fld[0].rtx; 
v30 = v30[1].fld[0].rtx; 
while ( v30 && v30 != ( rtx)i && *( _WORD *)v30 != 36 ); 
while ( v30 && v30 != ( rtx)i && *( _WORD *)v30 != 36 ); 
rtx v18; // rax 
rtx v44; // r15 
rtx v64; // rax 
rtx v93; // rax 
rtx v99; // r14 
rtx v101; // rax 
rtx inner_dest; // r12 
rtx *p_inner_dest; // r13 
rtx v126; // r12 
rtx v14; // rax 
uid_cuid_0 = v7; 
v14 = cse_basic_block( v6, data.last, data.path, after_loop == 0); 
v6 = v14; 
free( uid_cuid_0); 
rtx *loc; // [rsp+0h] [rbp-58h] 
rtx pattern; // [rsp+18h] [rbp-40h] 
rtx after; // [rsp+20h] [rbp-38h] 
v6 = table[canon_hash( *( rtx *)&x[1], ( machine_mode)BYTE2( v3)) & 0x1F]; 
loc = ( rtx *)&x[1]; 
v10 = *loc; 
v11 = *( _DWORD *)*loc; 
|| ( v13 = BYTE2( v11), mode_class_0[v13] != MODE_INT) 
|| ( v14 = BYTE2( v12), mode_class_0[v14] != MODE_INT) 
|| !subreg_lowpart_p( *loc) ) 
htab_empty( hash_table_0); 
htab_delete( hash_table_0); 
hash_table_0 = v3; 
v2 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v17 = ( ( unsigned int)( mode_class_0[v16] - 5) < 2) + 1; 
htab_traverse( hash_table_0, cselib_invalidate_mem_1, dest); 
cselib_invalidate_regno( dest->fld[0].rtuint, ( machine_mode)BYTE2( v5)); 
v6 = push_operand( dest, ( machine_mode)*( ( unsigned __int8 *)dest + 2)); 
v22 = hash_table_0; 
slot_with_hash = htab_find_slot_with_hash( v22, v24, v21, ( insert_option)( create != 0)); 
v11 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
elt = new_cselib_val( ++next_unknown_value, ( machine_mode)v7); 
v49 = hash_table_0; 
v12 = mode_class_0[v7]; 
v14 = cselib_lookup( x->fld[0].rtx, ( machine_mode)v7, create); 
v17 = new_cselib_val( ++next_unknown_value, ( machine_mode)v7); 
v60 = hash_table_0; 
v5 = gen_rtx_fmt_e( CONST, ( machine_mode)v7, x); 
rtx v20; // rax 
rtx v24; // r15 
rtx memref[2]; // [rsp+10h] [rbp-D78h] 
htab_empty( hash_table_0); 
htab_traverse( hash_table_0, cselib_invalidate_mem_1, callmem); 
*( __m128i *)memref = _mm_shuffle_epi32( _mm_loadu_si128( ( const __m128i *)( v7 + 4)), 78); 
v20 = ( rtx)*( v16 - 3); 
v20 = gen_rtx_fmt_eee( IF_THEN_ELSE, ( machine_mode)*( ( unsigned __int8 *)v20 + 2), v9, ( rtx)*( v16 - 3), rtx); 
v20 = gen_rtx_fmt_eee( IF_THEN_ELSE, ( machine_mode)*( ( unsigned __int8 *)v20 + 2), v9, ( rtx)*( v16 - 3), rtx); 
v20 = gen_rtx_fmt_eee( IF_THEN_ELSE, ( machine_mode)*( ( unsigned __int8 *)v20 + 2), v9, ( rtx)*( v16 - 3), rtx); 
*( v16 - 1) = cselib_lookup( v20, ( machine_mode)BYTE2( v19), 1); 
*( v16 - 1) = cselib_lookup( v20, ( machine_mode)BYTE2( v19), 1); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v10 = mode_class_0[( unsigned __int8)v8]; 
v26 = ( tree_node *)*( &global_trees + 27); 
fprintf( asmfile, &off_6474F4[1], ( unsigned int)current_sym_value); 
fprintf( asmfile, &off_6474F4[1], ( unsigned int)current_sym_value); 
fprintf( asmfile, &off_6474F4[1], ( unsigned int)current_sym_value); 
if ( !initial || !strcmp( lang_hooks_0.name, "GNU C++") && initial == ( tree)global_trees ) 
fprintf( asmfile, &off_6474F4[1], ( unsigned int)current_sym_value); 
v102 = ( tree_node *)rtl[4]; 
fwrite( &unk_65E21B, 2uLL, 1uLL, asmfile); 
v69 = ( unsigned int)dbxout_type_anonymous_type_number++; 
if ( ( *( _BYTE *)( v54 + 18) & 4) != 0 && !strcmp( lang_hooks_0.name, "GNU C++") ) 
v5 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v6] - 5) < 2) + rtuint; 
v30 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v29] - 5) < 2) + 1; 
v32 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v31] - 5) < 2) + 1; 
*( reload_type *)( v2 + 10510828), 
*( machine_mode *)( v2 + 10510764)); 
fprintf( file, off_66C47B, ( unsigned int)( j + ( i->indx << 7))); 
fprintf( file, off_66C47B, ( unsigned int)( k + ( i->indx << 7) + 64)); 
fprintf( v1, "%s, ", reg_class_names_0[*( ( int *)v2 - 20)]); 
fprintf( v1, "%ssecondary_in_icode = %s", "\n\t", insn_data_0[v9].name); 
fprintf( v1, "%ssecondary_out_icode = %s", v10, insn_data_0[v11].name); 
v3 = assign_stack_local( ( machine_mode)v2, mode_size[v2], 0); 
error_with_decl( decl, &off_71EFD0[4]); 
p_int_cst = &exp->int_cst.int_cst; 
low = p_int_cst->low; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)( low + 32); 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)( low + 32); 
v3 += int_byte_position( ( tree)p_int_cst->high); 
v9 = *( tree_node **)( low + 40); 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)( low + 32); 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)( low + 32); 
if ( ( sch_istable[v3] & 4) == 0 ) 
*( _OWORD *)value = 0LL; 
*( _OWORD *)&value->un.vechi[13] = 0LL; 
*( _OWORD *)&value->un.vechi[11] = 0LL; 
*( _OWORD *)&value->un.vechi[9] = 0LL; 
*( _OWORD *)&value->un.vechi[7] = 0LL; 
*( _OWORD *)&value->un.vechi[5] = 0LL; 
*( _OWORD *)&value->un.vechi[3] = 0LL; 
*( _OWORD *)&value->un.vechi[1] = 0LL; 
v14 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v12] - 5) < 2) + 1; 
rtwint = gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)rtwint + 2), rtwint, *( rtx *)&rtx[1]); 
rtwint = gen_rtx_fmt_e( CONST, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)rtwint), rtwint); 
v2 = *( _OWORD *)&loc_p->reg; 
*( _OWORD *)&cfa.base_offset = *( _OWORD *)&loc_p->base_offset; 
*( _OWORD *)&cfa.base_offset = *( _OWORD *)&loc_p->base_offset; 
*( _OWORD *)&cfa.reg = v2; 
*( ( _OWORD *)v12 + 1) = 0LL; 
v16 = ( tree_node *)*( &global_trees + 25); 
if ( *v23 != v16 || !byte_9FE5BF[v24] ) 
if ( *v20 != v16 || !byte_9FE5BF[v19] ) 
if ( reg_note ) 
v5.rtwint = ( __int64)reg_note->fld[0]; 
rtx v7; // r14 
rtx real_insn; // r15 
rtx v9; // r12 
if ( reg_note ) 
v7 = reg_note; 
v7 = reg_note; 
if ( reg_note->fld[0].rtx != v5 ) 
real_insn = next_real_insn( v5); 
v9 = find_reg_note( v7->fld[0].rtx, REG_RETVAL, 0LL); 
v9 = find_reg_note( v7->fld[0].rtx, REG_RETVAL, 0LL); 
v7->fld[0].rtx, 
real_insn[3].fld[0].rtx); 
rtx in; // rdi 
rtx out_reg; // rdi 
rtx v15; // rbp 
rtx v20; // rbx 
rtx v28; // rax 
rtx v30; // rax 
rtx dead_insn; // [rsp+10h] [rbp-58h] 
dead_insn = spill_reg_store[last_reload_reg]; 
in = rld[v4].in; 
if ( in ) 
v8 = *( _WORD *)in; 
in = rld[v4].in_reg; 
rtx nonnote_insn; // rax 
nonnote_insn = next_nonnote_insn( ( rtx)v5); 
if ( nonnote_insn && *( _WORD *)nonnote_insn == 33 && ( **( _DWORD **)&nonnote_insn[2] & 0xFFFE) == 44 ) 
if ( nonnote_insn && *( _WORD *)nonnote_insn == 33 && ( **( _DWORD **)&nonnote_insn[2] & 0xFFFE) == 44 ) 
if ( nonnote_insn && *( _WORD *)nonnote_insn == 33 && ( **( _DWORD **)&nonnote_insn[2] & 0xFFFE) == 44 ) 
delete_related_insns( nonnote_insn); 
rtx v264; // rax 
rtx v266; // rax 
rtx v291; // rax 
rtx v295; // rax 
*( _OWORD *)&insns[v26].defs = 0LL; 
v13 = gen_rtx_fmt_e( USE, ( machine_mode)*( ( unsigned __int8 *)v12 + 2), v12); 
v22 = gen_rtx_fmt_e( USE, ( machine_mode)*( ( unsigned __int8 *)v21 + 2), v21); 
v27 = gen_rtx_fmt_e( CLOBBER, ( machine_mode)*( ( unsigned __int8 *)v26 + 2), v26); 
rtx *v15; // r15 
v14 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v13] - 5) < 2) + 1; 
v15 = loc; 
v15 = loc; 
v15 = loc; 
v15 = loc; 
df_ref_record_1( df_0, v17, v15, insn, ref_type, ref_flags); 
rtx v14; // rbp 
rtx *v27; // rbp 
v14 = rtx; 
v29 = ( int *)v14[2]; 
*( _OWORD *)&buffer->state.cursor = 0LL; 
if ( !count_error_warning_message ) 
count_error_warning_message = 1; 
v29 = *( _OWORD *)&diagnostic_buffer->state.prefix; 
v30 = *( _OWORD *)&diagnostic_buffer->state.indent_skip; 
v31 = *( _OWORD *)&diagnostic_buffer->state.cursor; 
v32 = *( _OWORD *)diagnostic_buffer->state.diagnostic_count; 
v33 = *( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4]; 
if ( ( sch_istable[v27] & 0x88) != 0 ) 
p_int_cst = &arg0->int_cst.int_cst; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&arg0->block.subblocks; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&arg0->block.subblocks; 
v12 = build( code, type, p_int_cst->low, *p_elements); 
low = p_int_cst->low; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&arg0->block.subblocks; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&arg0->block.subblocks; 
if ( operand_equal_p( ( tree)p_int_cst->high, p_elements[1], 0) ) 
rtx v10; // rcx 
v10 = added_links_insn; 
while ( *( _WORD *)v10 == 32 && **( _WORD **)&v10[2] == 48 ) 
while ( *( _WORD *)v10 == 32 && **( _WORD **)&v10[2] == 48 ) 
v10 = v10[1].fld[0].rtx; 
v10 = v10[1].fld[0].rtx; 
rtint = v10->fld[0].rtint; 
rtx v13; // rcx 
rtx v14; // r14 
rtx v15; // rbx 
rtx real_insn; // rax 
rtx v26; // rbx 
rtx v27; // rax 
rtx v31; // rdx 
rtx v34; // rax 
v81 = __CFADD__( lnum_orig, *v78); 
v81 = __CFADD__( ( *v31)++, 1LL); 
v81 = __CFADD__( lnum_orig, *v78); 
if ( v2 != ( tree_node *)global_trees ) 
if ( mode_class_0[mode] == MODE_INT && !can_compare_p( op, mode, ccp_jump) ) 
do_compare_rtx_and_jump( v25, v24, v19, v17, ( machine_mode)v14, v22, if_false_label, v26); 
v13 = mode_class_0[mode]; 
v11 = operand_sub*(short *)0xforce( op, v9, ( machine_mode)v6); 
v11 = operand_sub*(short *)0xforce( op, v9, ( machine_mode)v6); 
v12 = operand_sub*(short *)0xforce( v8, v9, ( machine_mode)v6); 
v12 = operand_sub*(short *)0xforce( v8, v9, ( machine_mode)v6); 
v6 = operand_sub*(short *)0xforce( op0, 0, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v6 = operand_sub*(short *)0xforce( op0, 0, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v11 = operand_sub*(short *)0xforce( op0, i, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v11 = operand_sub*(short *)0xforce( op0, i, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v15 = operand_sub*(short *)0xforce( op0, j, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v15 = operand_sub*(short *)0xforce( op0, j, ( machine_mode)*( ( unsigned __int8 *)op0 + 2)); 
v17 = operand_sub*(short *)0xforce( op, v15, v13); 
v18 = operand_sub*(short *)0xforce( v27, v15, v13); 
v19 = ( tree_node *)v13; 
v14 = swap_condition( ( rtx_code)v14); 
v19 = ( tree_node *)v12; 
if ( can_compare_p( ( rtx_code)v14, ( machine_mode)v11, ccp_store_flag) ) 
if ( v26 != 1317 && ( !v53 || *( ( unsigned __int16 *)insn_data_0[v26].operand + 8) == mode) 
v33 = emit_store_flag( target, v51, v30, v32, ( machine_mode)v11, v52, 1); 
target = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)target + 2)); 
v37 = compare_from_rtx( v28, v29, v51, v52, ( machine_mode)v11, 0LL); 
target = expand_shift( RSHIFT_EXPR, ( machine_mode)v11, target, v46, v45, 1); 
if ( mode_class_0[v4] == MODE_INT && *( _WORD *)newval == 54 ) 
if ( rtwint != trunc_int_for_mode( rtwint, ( machine_mode)v4) ) 
frees = undobuf_0.frees; 
if ( undobuf_0.frees ) 
undobuf_0.frees = undobuf_0.frees->next; 
undobuf_0.frees = undobuf_0.frees->next; 
frees->next = undobuf_0.undos; 
undobuf_0.undos = frees; 
if ( ( doing_eh_warned & 1) == 0 ) 
doing_eh_warned = 1; 
( machine_mode)*( ( unsigned __int8 *)v73 + 2), 
( machine_mode)*( ( unsigned __int8 *)v76 + 2), 
fwrite( &unk_6476BF, 0x11uLL, 1uLL, outf); 
fprintf( file, off_6474F4, ( unsigned int)v5->index); 
fprintf( file, &off_6474F4[1], ( unsigned int)v10); 
fputs( dump_edge_info_bitnames[v10], file); 
fprintf( file, "; pref %s", dump_flow_info_reg_class_names[v13]); 
v15 = dump_flow_info_reg_class_names[v13]; 
fprintf( file, "; pref %s, else %s", v15, dump_flow_info_reg_class_names[v14]); 
fprintf( file, off_6474F4, ( unsigned int)i); 
fprintf( v198.stream, "%-13s ", &off_71BF47); 
v25 = *( tree_node **)( key + 64); 
fprintf( v198.stream, off_71BF36, v24, 6552963LL); 
fprintf( v198.stream, asc_71BF35, 25LL, 6552963LL); 
v13 = *( tree_node **)( key + 32); 
v13 = *( tree_node **)( key + 40); 
if ( lang_hooks_0.tree_dump.dump_tree( &v198, ( tree)key) ) 
v13 = *( tree_node **)( key + 8); 
predictor_info_0[predictor].name, 
fprintf( outf, off_6474F4, ( unsigned int)v5); 
fprintf( outf, off_6474F4, ( unsigned int)v8); 
fprintf( file, &off_6474F4[1], _bittest64( ( const __int64 *)&v8, v7)); 
v4 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), label); 
v17 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), lab2); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
if ( ( sch_istable[( unsigned __int8)v8] & 0x10) != 0 ) 
v16 = byte_666590[v15]; 
else if ( byte_666590[v15] ) 
v19 = byte_666590[v13]; 
if ( byte_666590[v13] ) 
v4 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), label); 
if ( ( unsigned int)debug_info_level_0 >= DINFO_LEVEL_NORMAL ) 
*( _OWORD *)&v3[v6].dw_fde_current_label = 0LL; 
args_size_0 = 0LL; 
if ( ( unsigned int)debug_info_level_0 < DINFO_LEVEL_NORMAL || ( BYTE1( decl->block.supercontext) & v5) != 0 ) 
if ( ( unsigned int)debug_info_level_0 < DINFO_LEVEL_NORMAL ) 
*( _OWORD *)&cfa.offset = 0LL; 
cfa_temp_0 = -1LL; 
v13 = dwarf2out_cfi_label_label_num++; 
sprintf( dwarf2out_cfi_label_label, "*.%s%u", "LCFI", v13); 
assemble_name( ( FILE *)asm_out_file, dwarf2out_cfi_label_label); 
dwarf2out_frame_debug_expr( *fld, dwarf2out_cfi_label_label); 
v4 = -args_size_0; 
v5 = args_size_0 + v4; 
args_size_0 = v5; 
v6 = dwarf2out_cfi_label_label_num++; 
sprintf( dwarf2out_cfi_label_label, "*.%s%u", "LCFI", v6); 
assemble_name( ( FILE *)asm_out_file, dwarf2out_cfi_label_label); 
def_cfa_1( dwarf2out_cfi_label_label, &cfa); 
v7 = args_size_0; 
if ( old_args_size != args_size_0 ) 
rtx v11; // r15 
v11 = ( rtx)expr[1]; 
if ( *( _WORD *)v11 != 61 ) 
if ( cfa_temp_0 != v35 ) 
if ( cfa_temp_0 != v20 ) 
if ( ( v11->fld[0].rtint & 0xFFFFFFFE) == 6 || cfa.reg != v11->fld[0].rtint ) 
if ( ( v11->fld[0].rtint & 0xFFFFFFFE) == 6 || cfa.reg != v11->fld[0].rtint ) 
v11 = global_rtl[2]; 
v47->reg = v11; 
if ( cfa_temp_0 != *( _DWORD *)( *( _QWORD *)( v15.rtwint + 8) + 8LL) ) 
v13 = ( unsigned __int16)*( _DWORD *)v11; 
switch ( ( unsigned __int16)*( _DWORD *)v11 ) 
cfa_temp_0 = rtx->fld[0].rtuint; 
if ( ( unsigned int)debug_info_level_0 >= DINFO_LEVEL_NORMAL ) 
return off_66A360[v1]; 
if ( ( unsigned int)format > 0xFF || ( result = eh_data_format_name_format_names[format]) == 0LL ) 
*( _OWORD *)equot = 0LL; 
*( _OWORD *)v9 = 0LL; 
v5 = simplify_binary_operation( PLUS, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)x), *constptr, v4); 
v9 = simplify_binary_operation( PLUS, ( machine_mode)*( ( unsigned __int8 *)x + 2), *constptr, op1[0]); 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)x + 2), v7, v8); 
y = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)v47 + 2)); 
rtx v9; // rbp 
rtx v13; // rdi 
rtx v19; // rbx 
rtx v20; // rax 
rtx v23; // rax 
rtx v25; // rbx 
rtx result; // rax 
rtx v19; // rax 
rtx v21; // r15 
rtx v28; // r8 
rtx v64; // r12 
rtx *p_to_rtx; // rbp 
rtx v66; // rax 
rtx *fld; // r12 
rtx nonnote_insn; // rax 
rtx v71; // rbp 
rtx *v75; // r15 
rtx *v76; // rbx 
rtx *v88; // [rsp+10h] [rbp-248h] 
rtx v27; // rcx 
rtx v30; // rdx 
rtx v34; // rdx 
v27 = global_rtl[4]; 
v30 = v22[2].to_rtx; 
if ( v30 == v31 && v31 != v27 ) 
if ( v30 == v31 && v31 != v27 ) 
if ( *( _WORD *)v32 == 75 && *( rtx *)( v32 + 8) == v30 && ( v33 = *( _QWORD *)( v32 + 16), *( _WORD *)v33 == 54) ) 
v34 = v22[3].to_rtx; 
if ( v34 == v35 && v35 != v27 ) 
if ( v34 == v35 && v35 != v27 ) 
*( _OWORD *)rbit = 0LL; 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x7023D0); 
*( _OWORD *)&v32[v34 - 24] = 0LL; 
*( _OWORD *)&v32[v34 - 16] = 0LL; 
*( _OWORD *)&v32[v34 - 8] = 0LL; 
*( _OWORD *)&v32[v34] = 0LL; 
*( _OWORD *)&s[v43 + 1] = 0LL; 
tree v42; // rbx 
tree v47; // rax 
tree v48; // rax 
v15 = convert_modes( v9, ( machine_mode)v11, v12, v10); 
v10 = force_reg( ( machine_mode)v8, v10); 
v11 = force_reg( ( machine_mode)v8, v11); 
v22 = mode_class_0[( int)v8]; 
v10 = convert_to_mode( ( machine_mode)v68, v65, 0); 
v11 = convert_to_mode( ( machine_mode)v68, v66, 0); 
operand = insn_data_0[v36].operand; 
if ( !operand->predicate( v26, ( machine_mode)*( ( unsigned __int16 *)operand + 8)) ) 
x = gen_reg_rtx( ( machine_mode)*( ( unsigned __int16 *)operand + 8)); 
if ( !operand[2].predicate( v27, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
copy_to_mode_reg( ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8), v27); 
if ( !operand[3].predicate( v28, ( machine_mode)*( ( unsigned __int16 *)&operand[3] + 8)) ) 
copy_to_mode_reg( ( machine_mode)*( ( unsigned __int16 *)&operand[3] + 8), v28); 
v31 = insn_data_0[( int)v36].genfun( x, v30); 
rtx x_pending_chain; // rbx 
rtx dsta; // [rsp+0h] [rbp-80h] BYREF 
rtx *tmps; // [rsp+30h] [rbp-50h] 
rtx src; // [rsp+38h] [rbp-48h] 
v8 = ( rtx *)( ( char *)&dsta - ( ( 8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL)); 
src = orig_src; 
dsta = dst; 
tmps = ( rtx *)( ( char *)&dsta - ( ( 8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL)); 
tmps = ( rtx *)( ( char *)&dsta - ( ( 8 * v7 + 15) & 0xFFFFFFFFFFFFFFF0LL)); 
mode_alignment = get_mode_alignment( ( machine_mode)v11); 
rtx j; // rbx 
rtx *tmps; // [rsp+28h] [rbp-58h] 
rtx dst; // [rsp+40h] [rbp-40h] 
rtx y; // [rsp+48h] [rbp-38h] 
dst = orig_dst; 
y = *( rtx *)( *( ( _QWORD *)rtwint + 1) + 8LL); 
v5 = y == 0LL; 
v8 = y == 0LL; 
v10 = gen_reg_rtx( ( machine_mode)*( unsigned __int8 *)( v9 + 2)); 
tmps = v7; 
v13 = *( _DWORD *)dst; 
if ( !insn_data_0[1159].operand->predicate( loc, ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32) ) 
v1 = copy_to_mode_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), loc); 
rtx v6; // rbx 
rtx *fld; // r13 
v6 = first; 
fld = ( rtx *)first[1].fld; 
v6 = *fld; 
v6 = *fld; 
fld = ( rtx *)v6[1].fld; 
v8.rtwint = ( __int64)v6[1].fld[0]; 
set_block_for_insn( v6, v4); 
v4->end = v6; 
v6 = rtx; 
v6[1].fld[0] = v9; 
*( _QWORD *)( v9.rtwint + 16) = v6; 
rtx v12; // rbx 
rtx v13; // rbp 
rtx v14; // r12 
rtx last_insn; // rbp 
rtx v22; // rbx 
rtx v25; // rbp 
v6 = gen_reg_rtx( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)target)); 
if ( reg_note ) 
rtx v19; // rax 
rtx v25; // rax 
rtx *overflow_arg_area; // rax 
rtx v29; // rax 
if ( !memory_address_p( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v3), v3->fld[0].rtx) 
&& !push_operand( v3, ( machine_mode)*( ( unsigned __int8 *)v3 + 2)) 
if ( !memory_address_p( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v5), v5->fld[0].rtx) 
v41 = insn_data_0[insn_code].genfun( v4, v5); 
v9 = mode_class_0[v6]; 
if ( push_operand( v4, ( machine_mode)*( ( unsigned __int8 *)v4 + 2)) ) 
v47 = insn_data_0[v44].genfun( v45, v46); 
v26 = insn_data_0[v48].genfun( v49, v50); 
v73 = expand_binop( ( machine_mode)( v69 ^ 5), v70, v71, v72, global_rtl[2], 0, OPTAB_LIB_WIDEN); 
rtx v10; // r15 
rtx rtwint; // r14 
rtx last_insn; // r15 
rtx v24; // rbx 
rtx v27; // rbp 
v10 = insns; 
rtwint = v10; 
rtwint = v10; 
rtwint = ( rtx)v12.rtwint; 
rtwint = v10; 
if ( ( unsigned int)( *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)reg >> 14) & 0x3FC)) - 5) > 1 ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
if ( *( insn_code *)( ( char *)&optab_table[30]->handlers[0].insn_code + v48) != CODE_FOR_nothing 
v73 = operand_sub*(short *)0xforce( v16, v71, mode); 
operand = insn_data_0[v57].operand; 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
predicate = insn_data_0[( _QWORD)insn_code].operand->predicate; 
v14 = insn_data_0[v12].genfun( x, insn_code); 
if ( memory_address_p( ( machine_mode)BYTE2( v5), rtwint) ) 
if ( !memory_address_p( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v5), v8) ) 
v10 = assign_stack_local( ( machine_mode)v6, mode_size[v6], 0); 
v10 = gen_reg_rtx( ( machine_mode)v6); 
if ( !memory_address_p( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v5), v12) ) 
|| mode_class_0[v28] != MODE_INT 
v11 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)target + 2)); 
rtx v25; // rbp 
v24 = subreg_regno_offset( v21[2], ( machine_mode)( unsigned __int8)BYTE2( *v21), v22, ( machine_mode)BYTE2( v20)); 
v24 = subreg_regno_offset( v21[2], ( machine_mode)( unsigned __int8)BYTE2( *v21), v22, ( machine_mode)BYTE2( v20)); 
v25 = rtx; 
( machine_mode)( unsigned __int8)BYTE2( *rtwint), 
( machine_mode)BYTE2( v26))] 
v25->fld[0].rtwint = ( __int64)rtx; 
operand = insn_data_0[icode].operand; 
if ( !operand->predicate( v8, ( machine_mode)*( ( unsigned __int8 *)v8 + 2)) ) 
v11 = gen_reg_rtx( ( machine_mode)BYTE2( v13)); 
v14 = insn_data_0[icode].genfun( v11, v10); 
v21 = canon_hash( v13, ( machine_mode)BYTE2( v14)); 
if ( mode_class_0[mode] != MODE_FLOAT ) 
*( _OWORD *)x = 0LL; 
*( _OWORD *)x = 0LL; 
*( _OWORD *)x = 0LL; 
*( _OWORD *)x = 0LL; 
if ( mode_class_0[mode] != MODE_FLOAT ) 
*( _OWORD *)x = 0LL; 
*( _OWORD *)x = 0LL; 
*( _OWORD *)x = 0LL; 
*( _OWORD *)x = 0LL; 
*( _OWORD *)a1 = *( _OWORD *)p_e; 
v6 = rndprc; 
wstring[0] = 0; 
strcpy( wstring, " NaN "); 
rndprc = 80; 
strcpy( wstring, " -Infinity "); 
strcpy( wstring, " Infinity "); 
strcpy( wstring, "NaN"); 
v28 = etens[8]; 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
set_diagnostic_context( &dc, msgid, ( va_list_0 *)va, 0LL, 0, 0); 
v3 = spelling_0; 
if ( spelling_base < spelling_0 ) 
v3 = spelling_0; 
diagnostic_for_decl( decl, msgid, ( va_list_0 *)va, 0); 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
rtx v67; // rax 
rtx v88; // rax 
rtx *v91; // rbp 
rtx v92; // rbp 
rtx *fld; // [rsp+0h] [rbp-68h] 
rtx earliest; // [rsp+10h] [rbp-58h] BYREF 
rtx jump; // [rsp+18h] [rbp-50h] 
jump = *( rtx *)( v28 + 8); 
v29 = jump; 
jump = v29; 
*( _OWORD *)v11 = 0LL; 
*( _OWORD *)v11 = 0LL; 
v16 = build( ( tree_code)v10, tta, v29, v34, v18); 
v16 = build( ( tree_code)v10, tt, v21, new0, v9); 
v16 = build( ( tree_code)v10, ttb, v25, v26, v27); 
v16 = build1( ( tree_code)v10, type, v15); 
*( _OWORD *)v12 = v13; 
v27 = ( ( unsigned int)( mode_class_0[BYTE2( v8)] - 5) < 2) + 1; 
v23 = mode_class_0[v10]; 
*( _OWORD *)&next->buff = 0LL; 
v55 = store_field( v53, v46, v47, modea, exp, ( machine_mode)v48, v50, v51, alias_set); 
( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( v64->common.type->block.abstract_origin)) >> 1), 
( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( v54->common.type->block.abstract_origin)) >> 1), 
v21 = expand_expr( from, 0LL, ( machine_mode)*( ( unsigned __int8 *)v17 + 2), EXPAND_NORMAL); 
( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( sizetype_tab[0]->block.abstract_origin)) >> 1), 
v4 = *( tree_node **)( *high + 32LL); 
v6 = ( tree_node *)high[4]; 
predict_insn_def( v13, PRED_BUILTIN_EXPECT, ( prediction)v12); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v9 = adjust_address_1( v4, ( machine_mode)v1, ( unsigned int)v7, 1, 1); 
v5 = force_reg( ( machine_mode)( ( ( v3 | 0x500000000uLL) - 1) >> 32), buf_addr); 
v7 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v5); 
v12 = gen_rtx_MEM( ( machine_mode)( v2 ^ 5), v11); 
rtx v18; // r14 
v18 = get_memory_rtx( elements); 
set_mem_align( v18, v11); 
v6 = clear_storage( v18, v17); 
rtx = v18->fld[0].rtx; 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v5 = memory_address( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), tem); 
v6 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v5); 
return gen_rtx_MEM( ( machine_mode)v9, v11); 
v9 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v8); 
v12 = gen_rtx_MEM( ( machine_mode)v10, v11); 
if ( !expand_builtin_va_arg_gave_help ) 
expand_builtin_va_arg_gave_help = 1; 
v16 = gen_rtx_MEM( ( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( type->block.abstract_origin)) >> 1), v14); 
rtx v34; // rax 
rtx result; // rax 
tree v45; // rax 
tree v46; // rcx 
tree chain; // rbx 
tree rttree; // r13 
v7 = mode_class_0[mode]; 
operand = insn_data_0[insn_code].operand; 
v20 = insn_data_0[v46].genfun( v18, v19); 
v25 = convert_modes( ( machine_mode)v23, v6, x, unsignedp); 
v26 = expand_complex_abs( ( machine_mode)v24, v25, 0LL, unsignedp); 
v34 = convert_modes( ( machine_mode)v22, v6, x, unsignedp); 
v26 = expand_complex_abs( ( machine_mode)v22, v34, 0LL, unsignedp); 
rtx v26; // rax 
v12 = gen_rtx_fmt_e( USE, ( machine_mode)BYTE2( v2), rtx); 
v13 = nonzero_bits( x->fld[0].rtx, ( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
x = gen_rtx_fmt_e( ZERO_EXTEND, ( machine_mode)BYTE2( v2), v14); 
v18 = nonzero_bits( v15->fld[0].rtx, ( machine_mode)v17); 
v30 = nonzero_bits( *( rtx *)( x->fld[0].rtwint + 8), ( machine_mode)v29); 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
v26 = simplify_shift_const( 0LL, ( rtx_code)( v3 + 89), ( machine_mode)v23, v27, ( int)v24 - ( int)v8); 
v26 = simplify_shift_const( 0LL, ( rtx_code)( v3 + 89), ( machine_mode)v23, v27, ( int)v24 - ( int)v8); 
rtx v19; // r15 
rtx v22; // rax 
rtx *p_rtl; // r15 
rtx *v26; // rax 
rtx v28; // rax 
v13 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v19 = assign_temp( decl, 1, 1, 1); 
set_mem_attributes( v19, decl, 1); 
decl->decl.rtl = v19; 
if ( !v19 ) 
v19 = decl->decl.rtl; 
v20 = force_operand( v19->fld[0].rtx, rtx); 
( save_level)( x_block_stack->next == 0LL), 
v12 = gen_rtx_MEM( ( machine_mode)LOBYTE( decl->block.supercontext), dynamic_stack_space); 
v21 = promote_mode( type, ( machine_mode)( unsigned __int8)v5, punsignedp, 0); 
v11 = ( tree_node *)*( &global_trees + 27); 
v8 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
exc_ptr = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v0 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 2); 
rtx v17; // rbp 
rtx *v21; // rbx 
rtx v23; // rbx 
rtx v26; // r13 
v8 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v17 = gen_label_rtx( ); 
*( ( _BYTE *)v17 + 3) |= 0x10u; 
v18 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v17); 
v18 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v17); 
emit_label( v17); 
v13 = gen_rtx_fmt_ee( EXPR_LIST, VOIDmode, v17, arg1a); 
v21 = &cfun->x_nonlocal_goto_handler_slots; 
v23 = *v21; 
rtx v9; // r15 
rtx v22; // r12 
v9 = *( rtx *)( v2.rtwint + 24); 
if ( *( _WORD *)v9 == 54 
&& v9->fld[0].rtwint + ( int)v7 > *( const unsigned __int16 *)( ( char *)mode_bitsize 
rtx = gen_rtx_fmt_e( USE, ( machine_mode)BYTE2( v3), rtx); 
v17 = mode_class_0[BYTE2( v11)]; 
v22 = copy_rtx( rtx); 
v23 = gen_binary( ASHIFT, ( machine_mode)v16, v21, v9); 
v23 = gen_binary( ASHIFT, ( machine_mode)v16, v21, v9); 
v24 = simplify_gen_unary( NOT, ( machine_mode)v16, v23, ( machine_mode)v16); 
v24 = simplify_gen_unary( NOT, ( machine_mode)v16, v23, ( machine_mode)v16); 
v25 = gen_binary( AND, ( machine_mode)v16, v24, rtx); 
v12 = &optab_table[17]->handlers[v10]; 
v17 = *( insn_code *)( ( char *)&fixtrunctab[0][0][v9] + v16); 
if ( v12->insn_code != CODE_FOR_nothing ) 
v17 = *( insn_code *)( ( char *)&fixtab[0][0][v9] + v16); 
v45 = convert_to_mode( ( machine_mode)v11, v45, 0); 
v46 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)v45 + 2)); 
v45 = expand_unop( ( machine_mode)*( ( unsigned __int8 *)v45 + 2), optab_table[17], v45, v46, 0); 
v55 = gen_reg_rtx( ( machine_mode)v14); 
emit_unop_insn( v17, v55, v45, ( rtx_code)( 2 * ( operand0 != 0) + 126)); 
if ( v12->insn_code != CODE_FOR_nothing ) 
( machine_mode)*( ( unsigned __int8 *)v41 + 2), 
v52 = gen_rtx_fmt_e( ( rtx_code)( 2 * ( unsignedp != 0) + 126), ( machine_mode)*( ( unsigned __int8 *)v41 + 2), v49); 
emit_cmp_and_jump_insns( v20, const_int_rtx[64], GE, 0LL, ( machine_mode)*( ( unsigned __int8 *)v20 + 2), 0, v17); 
if ( significand_size( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v18)) + 1 >= *( const unsigned __int16 *)( ( char *)mode_bitsize + ( ( *( _DWORD *)v20 >> 15) & 0x1FE)) ) 
v37 = &off_6FFDC0 + ( char)v36; 
v37 = &off_6FFDD8 + ( char)v47; 
v37 = &off_6FFDF0 + ( char)v45; 
v37 = &off_6FFE08 + ( char)v46; 
( machine_mode)*( ( unsigned __int8 *)v18 + 2), 
v51 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)v18 + 2), v35); 
v14 = convert_to_mode( ( machine_mode)v8, v14, unsignedp); 
emit_unop_insn( v12, v15, v14, ( rtx_code)( 2 * ( v54 != 0) + 125)); 
rtx v25; // rax 
rtx v26; // r14 
rtx v31; // rax 
rtx x_nonlocal_goto_handler_slots; // rbx 
rtx v37; // rax 
rtx v41; // rax 
rtx return_rtx; // rbx 
rtx v50; // rcx 
v12 = assign_stack_local_1( ( machine_mode)v11, mode_size[v11], 0, v6); 
v14 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v15 = gen_rtx_MEM( ( machine_mode)LOBYTE( subr->decl.result->block.supercontext), v14); 
subr->decl.result->decl.rtl = gen_reg_rtx( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v17)); 
rtx last_insn; // rax 
rtx v21; // rbx 
v5 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)v3, v4); 
last_insn = get_last_insn( ); 
if ( last_insn ) 
v21 = last_insn; 
v21 = last_insn; 
while ( *( _WORD *)v21 != 34 ) 
if ( *( _WORD *)v21 == 33 ) 
v21[3].fld[0].rtwint = ( __int64)alloc_EXPR_LIST( 27, const_int_rtx[64], v21[3].fld[0].rtx); 
v21[3].fld[0].rtwint = ( __int64)alloc_EXPR_LIST( 27, const_int_rtx[64], v21[3].fld[0].rtx); 
v21 = ( rtx)v21[1]; 
if ( !v21 ) 
if ( *( _OWORD *)&v13->data.case_stmt.nominal_type == 0LL ) 
( machine_mode)*( ( unsigned __int8 *)target + 2), 
( tree_code)( ( ( *( ( _DWORD *)&exp->common + 4) & 0xFD) == 129) + 59), 
operand = insn_data_0[insn_code].operand; 
if ( operand->predicate( v8, ( machine_mode)v19) && operand[1].predicate( v8, ( machine_mode)v19) ) 
if ( operand->predicate( v8, ( machine_mode)v19) && operand[1].predicate( v8, ( machine_mode)v19) ) 
v30 = operand[2].predicate( v43, ( machine_mode)v19); 
v28 = insn_data_0[v27].operand; 
if ( v28->predicate( v8, ( machine_mode)v19) && v28[1].predicate( v8, ( machine_mode)v19) ) 
if ( v28->predicate( v8, ( machine_mode)v19) && v28[1].predicate( v8, ( machine_mode)v19) ) 
if ( !v28[2].predicate( v14, ( machine_mode)v19) ) 
force_reg( ( machine_mode)v19, v14); 
v38 = insn_data_0[v42].genfun( v34, v35); 
( machine_mode)*( ( unsigned __int8 *)v8 + 2), 
if ( general_operand( v8->fld[0].rtx, ( machine_mode)v19) ) 
v32 = force_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v8->fld[0].rtx); 
v8 = force_reg( ( machine_mode)*( ( unsigned __int8 *)v36 + 2), v36); 
rtx v35; // rbp 
rtx last_insn; // rax 
rtx v58; // rax 
rtx v61; // rbx 
rtx v63; // rbp 
rtx v65; // r13 
rtx v72; // rax 
rtx v85; // rax 
v5 = force_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), const_int_rtx[64]); 
if ( *( const mode_class *)( ( char *)mode_class_0 + ( ( v8 >> 14) & 0x3FC)) == MODE_INT 
v47 = expand_binop( mode, optab_table[21], arg1, v43, 0LL, 0, ( optab_methods)methods); 
v70 = expand_binop( mode, optab_table[21], arg0, v69, 0LL, 0, ( optab_methods)v72); 
v57 = expand_binop( mode, optab_table[21], arg0, v56, 0LL, 0, ( optab_methods)v59); 
v64 = expand_binop( mode, optab_table[21], arg1, v63, 0LL, 0, ( optab_methods)v66); 
v51 = expand_binop( mode, optab_table[21], arg1, v50, 0LL, 0, ( optab_methods)v53); 
tree v12; // rax 
tree v14; // rbp 
tree v15; // rax 
v10 = type_for_mode( ( machine_mode)*( ( unsigned __int8 *)add + 2), unsignedp); 
v12 = make_tree( v9, mult); 
v13 = build( MULT_EXPR, v9, tree, v12); 
v14 = fold( v13); 
v15 = make_tree( v10, add); 
v16 = build( PLUS_EXPR, v9, v14, v15); 
v16 = build( PLUS_EXPR, v9, v14, v15); 
v13 = immed_double_const( cnst1, v12, ( machine_mode)v10); 
v16 = convert_to_mode( ( machine_mode)v10, op0, unsignedp); 
v17 = expand_mult( ( machine_mode)v10, v16, v13, 0LL, 0); 
v19 = expand_shift( RSHIFT_EXPR, ( machine_mode)v10, v17, v18, 0LL, 1); 
return convert_modes( mode, ( machine_mode)v10, v19, unsignedp); 
v44 = expand_shift( RSHIFT_EXPR, ( machine_mode)oldmode, v42, v43, 0LL, 1); 
v45 = convert_modes( mode, ( machine_mode)oldmode, v44, v40); 
v34 = expand_binop( ( machine_mode)v28, v30, op0, v55, 0LL, v33, OPTAB_WIDEN); 
v47 = expand_shift( RSHIFT_EXPR, ( machine_mode)oldmode, v35, v46, 0LL, 1); 
v14 = expand_binop( mode, optab_table[23], v27, v12, 0LL, 0, ( optab_methods)methods); 
v23 = expand_binop( mode, optab_table[23], op1, v21, 0LL, 0, ( optab_methods)v22); 
v22 = operand_sub*(short *)0xforce( result_val, v23 >> ( 6 - ( v28 == 0)), BLKmode); 
v20 = expand_expr( ( tree)high, v19, ( machine_mode)*( ( unsigned __int8 *)v19 + 2), EXPAND_NORMAL); 
tree v29; // rax 
tree v41; // [rsp+40h] [rbp-38h] 
v41 = amount; 
type = v41->common.type; 
v29 = convert( type, v28); 
v30 = build( MINUS_EXPR, type, v29, v41); 
if ( x_block_stack && *( _OWORD *)&x_block_stack->data.case_stmt.nominal_type != 0LL ) 
*( _OWORD *)( object_base + 40) = 0LL; 
if ( ( tree_node *)global_trees != elements ) 
if ( !genrtl_case_label_explained ) 
genrtl_case_label_explained = 1; 
v11 = force_reg( ( machine_mode)v10, v11); 
v12 = force_reg( ( machine_mode)v10, v12); 
v17 = gen_reg_rtx( ( machine_mode)v10); 
v18 = gen_reg_rtx( ( machine_mode)v10); 
operand = insn_data_0[insn_code].operand; 
if ( !operand->predicate( operand0, ( machine_mode)v10) || !operand[3].predicate( to, ( machine_mode)v10) ) 
if ( !operand->predicate( operand0, ( machine_mode)v10) || !operand[3].predicate( to, ( machine_mode)v10) ) 
v29 = insn_data_0[v47].genfun( operand0, operand1); 
v31 = mode_class_0[v10]; 
v35 = gen_reg_rtx( ( machine_mode)i); 
v39 = gen_reg_rtx( ( machine_mode)v34); 
v20 = mode_class_0[mode]; 
operand = insn_data_0[insn_code].operand; 
v18 = insn_data_0[v12].genfun( v15, v16); 
v20 = mode_class_0[mode]; 
( machine_mode)i, 
v25 = expand_unop( ( machine_mode)v23, v7, v24, 0LL, unsignedp); 
( machine_mode)v51, 
rtx v5; // rax 
rtx v6; // rax 
v5 = pc_set( rtx); 
v6 = canonicalize_condition( rtx, *( rtx *)( *( _QWORD *)&v5[1] + 8LL), 0, 0LL, v2); 
v6 = canonicalize_condition( rtx, *( rtx *)( *( _QWORD *)&v5[1] + 8LL), 0, 0LL, v2); 
if ( v6 ) 
if ( v6->fld[0].rtx == v2 ) 
v7 = (  struct rtx_def *)v6[1]; 
v8 = gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)v6, VOIDmode, *( rtx *)( v3 + 16), v7); 
v8 = gen_rtx_fmt_ee( ( rtx_code)*( _WORD *)v6, VOIDmode, *( rtx *)( v3 + 16), v7); 
rtx v6; // rbx 
v6 = modify_mem_list[bb->index]; 
if ( v6 ) 
v8.rtwint = ( __int64)v6->fld[0]; 
if ( *( ( _DWORD *)uid_cuid_1 + *( int *)( v8.rtwint + 8)) <= v7 ) 
v6 = ( rtx)v6[1]; 
while ( v6 ); 
( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( sizetype_tab[0]->block.abstract_origin)) >> 1), 
rtx v23; // rax 
v23 = gen_rtx_CONST_INT( VOIDmode, *( _QWORD *)( v20 + 8) * v17); 
v25 = v23; 
rtx v10; // rbp 
v10 = b; 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)v10 + 2), v12, v14); 
return gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)v10 + 2), v12, v14); 
*( _OWORD *)&b->aux = 0LL; 
*( _OWORD *)&b->count = 0LL; 
*( _OWORD *)&b->global_live_at_start = 0LL; 
*( _OWORD *)&b->local_set = 0LL; 
*( _OWORD *)&b->pred = 0LL; 
*( _OWORD *)&b->head_tree = 0LL; 
*( _OWORD *)&b->head = 0LL; 
*( _OWORD *)&b->aux = 0LL; 
*( _OWORD *)&b->count = 0LL; 
*( _OWORD *)&b->global_live_at_start = 0LL; 
*( _OWORD *)&b->local_set = 0LL; 
*( _OWORD *)&b->pred = 0LL; 
*( _OWORD *)&b->head_tree = 0LL; 
*( _OWORD *)&b->head = 0LL; 
( rtx_code)( unsigned __int16)*( _DWORD *)ext_dependent, 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)ext_dependent), 
( rtx_code)( unsigned __int16)*( _DWORD *)ext_dependent, 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)ext_dependent), 
rtx v21; // rax 
rtx v29; // rax 
rtx v34; // r15 
rtx v37; // rbp 
if ( recog_data_0.insn != insn || insn[2].fld[0].rtint < 0 ) 
recog_data_0.insn = insn; 
fatal_insn_not_found( insn, "recog.c", 2063, "extract_constrain_insn_cached"); 
v38 = force_reg( ( machine_mode)( unsigned __int8)v28, v10); 
v58 = expand_binop( v40, optab_table[23], v10, v56, target, 0, ( optab_methods)methods); 
v37 = immed_double_const( v34, v36, ( machine_mode)*( ( unsigned __int8 *)v10 + 2)); 
recog_data_0.insn = 0LL; 
*( _WORD *)&recog_data_0.n_operands = 0; 
recog_data_0.n_alternatives = 0; 
recog_data_0.n_operands = n_operands; 
fatal_insn_not_found( insn, "recog.c", 2139, "extract_insn"); 
recog_data_0.operand, 
recog_data_0.operand_loc, 
recog_data_0.constraints, 
recog_data_0.operand_mode); 
recog_data_0.n_alternatives = 1; 
v10 = *recog_data_0.constraints[0]; 
if ( !*recog_data_0.constraints[0] ) 
v11 = recog_data_0.constraints[0] + 1; 
recog_data_0.n_alternatives = v12; 
v14 = *recog_data_0.constraints[v13]; 
if ( recog_data_0.insn != insn || insn[2].fld[0].rtint < 0 ) 
recog_data_0.insn = insn; 
v16 = operand_sub*(short *)0xforce( op0, v11, v15); 
v33 = expand_binop( v27, optab_table[21], v24, v31, 0LL, 0, ( optab_methods)methods); 
v40 = expand_binop( v34, optab_table[23], v24, v38, 0LL, 0, ( optab_methods)v39); 
v3 = *( _OWORD *)&diagnostic_buffer->state.indent_skip; 
v4 = *( _OWORD *)&diagnostic_buffer->state.cursor; 
v5 = *( _OWORD *)diagnostic_buffer->state.diagnostic_count; 
v15 = *( _OWORD *)&diagnostic_buffer->state.prefix; 
v19 = *( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4]; 
diagnostic_buffer->state.format_args = ( va_list_0 *)va; 
*( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4] = v19; 
*( _OWORD *)v10->state.diagnostic_count = v18; 
imag = ( tree_node *)*( &global_trees + 17); 
rtx v9; // rcx 
rtx i; // rax 
v9 = first; 
if ( *( _WORD *)v9 == 37 ) 
rtuint = v9[2].fld[0].rtuint; 
v9 = v9[1].fld[0].rtx; 
v9 = v9[1].fld[0].rtx; 
while ( v9 ); 
for ( i = first[1].fld[0].rtx; i; i = final_scan_insn( i, file, ( __int64)insn_addresses_, ( unsigned int)prescan, v11) ) 
for ( i = first[1].fld[0].rtx; i; i = final_scan_insn( i, file, ( __int64)insn_addresses_, ( unsigned int)prescan, v11) ) 
for ( i = first[1].fld[0].rtx; i; i = final_scan_insn( i, file, ( __int64)insn_addresses_, ( unsigned int)prescan, v11) ) 
for ( i = first[1].fld[0].rtx; i; i = final_scan_insn( i, file, ( __int64)insn_addresses_, ( unsigned int)prescan, v11) ) 
v13 = i->fld[0].rtuint; 
induction_1 *i; // rbx 
induction_1 *i; // rbx 
if ( mode_class_0[bl_0->biv->mode] != MODE_INT ) 
for ( i = bl_0->biv; i; i = i->next_iv ) 
for ( i = bl_0->biv; i; i = i->next_iv ) 
for ( i = bl_0->biv; i; i = i->next_iv ) 
for ( i = bl_0->biv; i; i = i->next_iv ) 
v8 = *( ( _WORD *)i + 50); 
mult_val = i->mult_val; 
v6 = fold_rtx_mult_add( v6, mult_val, i->add_val, i->mode); 
v6 = fold_rtx_mult_add( v6, mult_val, i->add_val, i->mode); 
v8 = fold_rtx_mult_add( v8, v11, *( rtx *)( i + 72), ( machine_mode)*( _DWORD *)( i + 48)); 
( machine_mode)*( ( unsigned __int8 *)v6 + 2), 
rtx insna; // [rsp+8h] [rbp-38h] 
if ( ( debug_info_level_0 & 0xFFFFFFFE) == 2 
if ( ( debug_info_level_0 & 0xFFFFFFFE) == 2 
if ( v3 > 0x16 || ( v4 = &unk_740000, !_bittest64( ( const __int64 *)&v4, v3)) ) 
type->type.align = get_mode_alignment( ( machine_mode)v2); 
rtx v10; // rax 
v10 = find_base_term( v5); 
if ( !v10 ) 
v12 = *( _DWORD *)v10; 
if ( ( unsigned __int16)( *( _DWORD *)v10 - 67) < 2u ) 
return v10; 
base_term = v10; 
v10 = find_base_term( ( rtx)v8[1]); 
if ( v10 ) 
return v10; 
rtx v24; // r15 
rtx nonnote_insn; // rax 
timevar_push( TV_CFG_0); 
v24 = f; 
if ( control_flow_insn_p( v24) ) 
v38 = ( __int64)v24[2]; 
v40 = find_label_refs( *( rtx *)( *( _QWORD *)&v24[2] + 16LL), label_refs); 
v23 = find_label_refs( *( rtx *)( *( _QWORD *)&v24[2] + 24LL), v40); 
v41 = *(  struct rtx_def **)( *( _QWORD *)&v24[2] + 32LL); 
if ( v24[2].fld[0].rtint == -80 ) 
rtx *v33; // [rsp+18h] [rbp-50h] 
v15 = canon_hash( v7, ( machine_mode)v10); 
v33 = v6; 
v6 = v33; 
if ( mode_class_0[v21] != MODE_INT ) 
if ( mode_class_0[( unsigned int)v21] != MODE_INT ) 
v6 = v33; 
v6 = v33; 
rtx v19; // rbx 
rtx v47; // rdi 
rtx v48; // rax 
rtx v54; // [rsp+10h] [rbp-78h] 
v19 = real_out; 
rtx = v19->fld[0].rtx; 
v17 += subreg_regno_offset( rtuint, ( machine_mode)BYTE2( v20), *( _DWORD *)&v19[1], ( machine_mode)BYTE2( v15)); 
v17 += subreg_regno_offset( rtuint, ( machine_mode)BYTE2( v20), *( _DWORD *)&v19[1], ( machine_mode)BYTE2( v15)); 
rtx v37; // r14 
rtx v42; // rax 
rtx x; // [rsp+18h] [rbp-90h] 
|| flag_float_store && *( const mode_class *)( ( char *)mode_class_0 + ( ( v8 >> 14) & 0x3FC)) == MODE_FLOAT ) 
x = v15->fld[0].rtx; 
v16 = true_regnum( x); 
x = ( rtx)v15[1]; 
v16 = true_regnum( x); 
if ( ( *( ( _BYTE *)cfun + 425) & 1) != 0 && qty_0[v8].n_calls_crossed > 0 ) 
if ( !qty_0[v8].n_calls_crossed ) 
n_calls_crossed = qty_0[v81].n_calls_crossed; 
if ( 4 * n_calls_crossed < qty_0[v81].n_refs ) 
if ( ( unsigned int)( mode_class_0[mode] - 5) < 2 ) 
v34 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v42 = _mm_add_epi32( _mm_shuffle_epi32( v41, 80), ( __m128i)xmm*(short *)0x641440); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v47 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
rtx v35; // rax 
rtx nonnote_insn; // rax 
rtx v78; // rax 
rtx pool_constant; // r15 
rtx v85; // rax 
rtx v86; // r12 
rtx v93; // rax 
rtx v100; // rax 
n_operands = ( unsigned __int8)recog_data_0.n_operands; 
if ( recog_data_0.n_operands <= 0 ) 
v5 = ( unsigned __int8)recog_data_0.n_operands; 
if ( recog_data_0.n_operands <= 3u ) 
v6 = recog_data_0.n_operands & 0xFC; 
v8 = &matchp->use[( unsigned __int8)recog_data_0.n_operands + 26]; 
v8 = &matchp->use[( unsigned __int8)recog_data_0.n_operands + 26]; 
*( _OWORD *)&v8[v10] = -1LL; 
*( _OWORD *)&v8[v10] = -1LL; 
*( _OWORD *)&v8[v10 - 60] = -1LL; 
*( _OWORD *)&v8[v10 - 60] = -1LL; 
induction_1 *v11; // rbp 
induction_1 *v11; // rbp 
( machine_mode)BYTE2( v8)) ) 
v11 = ( induction_1 *)xmalloc( 0xA8uLL); 
v11 = ( induction_1 *)xmalloc( 0xA8uLL); 
v11, 
v11->mem = x; 
v10 = ( ( unsigned int)( mode_class_0[( unsigned int)v7] - 5) < 2) + 1; 
v11 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
result = gen_rtx_fmt_i0( REG, ( machine_mode)v7, v15); 
p_subblocks = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&result->block.subblocks; 
p_subblocks = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&result->block.subblocks; 
result = ( tree)p_subblocks->low; 
p_subblocks = &result->int_cst.int_cst; 
if ( ( *( _BYTE *)( v11.rtwint + 16) | 2) == 15 && *( tree_node **)( *( _QWORD *)( v11.rtwint + 8) + 128LL) == section_name ) 
v8 = *( int *)( ( char *)&allocno_0->reg + v7); 
v154 = ( unsigned __int64)*( _DWORD *)cfun->emit->x_regno_reg_rtx[*( int *)( ( char *)&allocno_0->reg + v7)] >> 16; 
v10 = ( unsigned __int8)BYTE2( *( _DWORD *)cfun->emit->x_regno_reg_rtx[*( int *)( ( char *)&allocno_0->reg + v7)]); 
if ( !*( int *)( ( char *)&allocno_0->calls_crossed + v7) ) 
v12 = v157 | *( HARD_REG_ELT_TYPE *)( ( char *)&allocno_0->hard_reg_conflicts + v7); 
v13 = *( HARD_REG_ELT_TYPE *)( ( char *)&allocno_0->regs_someone_prefers + v7) | v12 | ~regs_used_so_far; 
if ( !_bittest64( ( const __int64 *)&v17, v19) && ix86_hard_regno_mode_ok( v19, ( machine_mode)v10) ) 
v21 = ( ( unsigned int)( mode_class_0[v10] - 5) < 2) + 1; 
v25 = allocno_0; 
v27 = ~v17 & *( HARD_REG_ELT_TYPE *)( ( _BYTE *)&allocno_0->hard_reg_copy_preferences + v138); 
v8 = rtuint + ( ( unsigned int)( mode_class_0[( unsigned __int8)v7] - 5) < 2) + 1; 
v9 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v14] - 5) < 2) + 1; 
v8 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v12] - 5) < 2) + 1; 
rtx v121; // rax 
rtx *v123; // rax 
rtx *v139; // rcx 
rtx *v165; // rbx 
rtx *k; // rax 
rtx v30; // rax 
rtx memloc; // rbx 
rtx *fld; // r14 
rtx v37; // rax 
rtx v45; // rax 
rtx v16; // rbp 
rtx *v17; // rdx 
rtx v18; // rax 
rtx *v19; // rbx 
rtx v20; // rdi 
rtx *fld; // rdx 
rtx *v30; // rcx 
rtx *v46; // rdx 
rtx v58; // rax 
rtx *fld; // rcx 
rtx memrefloc[8]; // [rsp+8h] [rbp-40h] BYREF 
v18 = force_const_mem( ( machine_mode)*( ( unsigned __int8 *)v12 + 2), *( rtx *)&v12[1]); 
memrefloc[1] = ( rtx)loc; 
memrefloc[0] = v18; 
v12 = gen_rtx_fmt_ee( PLUS, ( machine_mode)*( ( unsigned __int8 *)v12 + 2), v12->fld[0].rtx, v18); 
fld = ( rtx *)v19->fld; 
find_reloads_address( mode, memrefloc, rtx, fld, ( rtx *)( unsigned int)opnum, type, ind_levels, 0LL); 
find_reloads_address( mode, memrefloc, rtx, fld, ( rtx *)( unsigned int)opnum, type, ind_levels, 0LL); 
( machine_mode)*( ( unsigned __int8 *)v18 + 2), 
if ( replace_reloads && recog_data_0.operand[opnum] != x ) 
( machine_mode)BYTE2( v8), 
result = gen_lowpart_common( ( machine_mode)*( ( unsigned __int8 *)x + 2), v21); 
( machine_mode)*( unsigned __int8 *)( reloads_subreg_address->fld[0].rtwint + 2)); 
result = gen_lowpart_common( ( machine_mode)*( ( unsigned __int8 *)reloads_subreg_address + 2), v23); 
( machine_mode)*( ( unsigned __int8 *)reloads_subreg_address + 2), 
if ( replace_reloads && recog_data_0.operand[opnum] != x ) 
( machine_mode)*( ( unsigned __int8 *)memloc + 2), 
( rtx_code)( unsigned __int16)*( _DWORD *)reg_rtx, 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)reg_rtx), 
( machine_mode)*( unsigned __int8 *)( v4->fld[0].rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)*loc + 2), 
rtx *v21; // rax 
rtx *v24; // rbx 
rtx v26; // rcx 
rtx *single_use_1; // rax 
v24 = ( rtx *)&v22[2 * v23]; 
v26 = *v24; 
v26 = *v24; 
single_use_1 = loc; 
if ( *v24 != dest ) 
|| *( _WORD *)v26 != 61 
|| ( single_use_1 = loc, v26->fld[0].rtint != dest->fld[0].rtint) ) 
|| ( single_use_1 = loc, v26->fld[0].rtint != dest->fld[0].rtint) ) 
rtx v25; // rbp 
rtx v30; // rax 
rtx *v33; // rax 
rtx v34; // rcx 
rtx *single_use; // rax 
rtx v40; // rax 
rtx v45; // rax 
rtx v46; // rbp 
rtx *result; // rax 
rtx v66; // rax 
v9 = ix86_register_move_cost( m1, ( reg_class)v5, class2); 
v4 = ix86_register_move_cost( m1, ( reg_class)v5, class2); 
v3->eh->exc_ptr = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v62 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 1); 
v60 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 0); 
v66 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 0); 
i = ( tree_node *)*( &global_trees + 11); 
elements = ( tree_node *)*( &global_trees + 11); 
v0 = initializer_stack_0; 
v1 = constructor_stack_0; 
if ( !constructor_stack_0 ) 
constructor_stack_0 = constructor_stack_0->next; 
constructor_stack_0 = constructor_stack_0->next; 
if ( constructor_range_stack_0 ) 
constructor_stack_0 = v0->constructor_stack; 
constructor_range_stack_0 = v0->constructor_range_stack; 
spelling_0 = v0->spelling; 
initializer_stack_0 = v0->next; 
fancy_abort( &off_7088B8[4], 3642, "finish_spills"); 
fancy_abort( &off_7088B8[4], 3721, "finish_spills"); 
|| ( v26.rtwint = ( __int64)v14->fld[0], v27 = *( tree_node **)( v26.rtwint + 128), v27 == integer_types[5]) 
v45 = *( tree_node **)v3; 
v14 = memory_address( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), fixed); 
v15 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v14); 
fancy_abort( &off_7088B8[4], 9527, "fixup_abnormal_edges"); 
if ( memory_address_p( ( machine_mode)v5, v10) ) 
return adjust_address_1( v4.rtx, ( machine_mode)v5, v6, 1, 1); 
v7 = adjust_address_1( v4.rtx, ( machine_mode)v5, v6, 1, 1); 
if ( **( _WORD **)( v4.rtwint + 16) == 54 && !memory_address_p( ( machine_mode)BYTE2( v3), x->fld[0].rtx) ) 
rtx v66; // rax 
rtx v68; // rax 
rtx v71; // rax 
rtx v75; // r14 
rtx v77; // r12 
rtx v81; // rax 
rtx v83; // rax 
rtx nonnote_insn; // rax 
rtx v31; // rdi 
rtx v32; // rax 
rtx v35; // [rsp+10h] [rbp-38h] 
v35 = no_share; 
no_share = v35; 
*( _OWORD *)p_pred = 0LL; 
fprintf( file, asc_6474F1, ( unsigned int)loop->num_nodes); 
*( _OWORD *)&loops->cfg.dfs_order = 0LL; 
*( _OWORD *)&loops->tree_root = 0LL; 
*( _OWORD *)&loops->num = 0LL; 
*( _OWORD *)&v58->outer = 0LL; 
v13 = ( tree_node *)*( &global_trees + 27); 
rtx v31; // rax 
rtx const_rtx; // r12 
rtx v42; // r14 
rtx v50; // rbx 
rtx v69; // rax 
rtx *v70; // rbp 
rtx v73; // r14 
rtx v100; // rax 
rtx v107; // rbp 
rtx v9; // rax 
rtx top; // rbx 
rtx v12; // r8 
rtx end; // rcx 
rtx exit_labels; // rcx 
rtx cont; // r8 
rtx v30; // rdx 
v9 = fncall( loop, rtx, v8, v35); 
v10 = ( unsigned __int16 *)v9; 
if ( *( _WORD *)v9 != 36 ) 
top = v9[1].fld[0].rtx; 
top = v9[1].fld[0].rtx; 
rtx v48; // rbx 
v48 = target->head; 
v48 = emit_label_before( v49, target->head); 
target->head = v48; 
v51 = v48->fld[0].rtint; 
v48 = target->head; 
v52 = gen_jump( v48); 
*( _QWORD *)&src->end[4] = v48; 
++*( _DWORD *)&v48[2]; 
v3 = gen_reg_rtx( ( machine_mode)BYTE2( v2)); 
rtx result; // rax 
rtx v15; // r13 
v3 = gen_reg_rtx( ( machine_mode)BYTE2( v4)); 
( machine_mode)*( unsigned __int8 *)( v2->fld[0].rtwint + 2), 
v15 = ( rtx)value[1]; 
switch ( ( unsigned __int16)*( _DWORD *)v15 ) 
if ( v15 == v3 ) 
if ( ( unsigned __int16)*( _DWORD *)v15 == 54 && v14 == optab_table[2] ) 
v15 = negate_rtx( ( machine_mode)BYTE2( v4), *( rtx *)&value[1]); 
v15 = negate_rtx( ( machine_mode)BYTE2( v4), *( rtx *)&value[1]); 
&& *( _WORD *)v15 == 54 
rtx v103; // rax 
rtx v119; // rax 
rtx v130; // [rsp+0h] [rbp-78h] 
rtx v132; // [rsp+8h] [rbp-70h] 
rtx v12; // r9 
v12 = y; 
v12 = x; 
v6 = (  struct rtx_def *)v12[1]; 
rtx = v12->fld[0].rtx; 
v15 = form_sum( v12, x->fld[0].rtx); 
return gen_rtx_fmt_ee( PLUS, v16, v12, x); 
return gen_rtx_fmt_ee( PLUS, v16, v12, x); 
v12 = v12->fld[0].rtx; 
v12 = v12->fld[0].rtx; 
v19 = gen_rtx_fmt_ee( PLUS, v16, v12, x); 
*( _OWORD *)&f->return_rtx = 0LL; 
*( _OWORD *)&f->x_nonlocal_labels = 0LL; 
*( _OWORD *)&f->x_nonlocal_goto_handler_labels = 0LL; 
*( _OWORD *)&f->x_cleanup_label = 0LL; 
*( _OWORD *)&f->x_save_expr_regs = 0LL; 
*( _OWORD *)&f->x_rtl_expr_chain = 0LL; 
*( _OWORD *)&f->x_tail_recursion_reentry = 0LL; 
*( _OWORD *)&f->x_context_display = 0LL; 
*( _OWORD *)&f->x_parm_birth_insn = 0LL; 
*( _OWORD *)&entry_exit_blocks[0].global_live_at_end = 0LL; 
v12 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v25 = ( ( unsigned int)( mode_class_0[( unsigned __int8)BYTE2( *( _DWORD *)reg_rtx)] - 5) < 2) + 1; 
free( uid_cuid_1); 
if ( !v2 || ( cannot_inline = ( const char *)&unk_6FC398, v2->int_cst.int_cst.low == *( &global_trees + 27)) ) 
cannot_inline = ( const char *)&unk_6FC398; 
return ( const char *)&unk_6FC408; 
else if ( in_section_0 != in_text ) 
in_section_0 = in_text; 
immediate_operand( recog_data_0.operand[1], VOIDmode); 
v9 = immediate_operand( recog_data_0.operand[1], VOIDmode); 
v21 = general_operand( recog_data_0.operand[0], QImode); 
reg_set_0 **v20; // r15 
reg_set_0 **v20; // r15 
rtx v27; // r11 
reg_set_0 *v33; // rsi 
reg_set_0 *v33; // rsi 
reg_set_0 *v43; // rdi 
reg_set_0 *v43; // rdi 
reg_set_0 *v48; // rsi 
reg_set_0 *v48; // rsi 
rtx v115; // rcx 
rtx v117; // rbp 
rtx *fld; // rbp 
rtx v124; // r14 
operand = insn_data_0[*( int *)( ( char *)&optab_table[0]->handlers[0].insn_code + v2)].operand; 
if ( !operand->predicate( x, ( machine_mode)*( ( unsigned __int16 *)operand + 8)) 
|| !operand[1].predicate( x, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) 
|| !operand[2].predicate( y, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
return insn_data_0[v3].genfun( x, x); 
operand = insn_data_0[v3].operand, 
operand->predicate( r0, ( machine_mode)*( ( unsigned __int16 *)operand + 8))) 
&& operand[1].predicate( r1, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) 
&& operand[2].predicate( c, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
return insn_data_0[v5].genfun( r0, r1); 
v22 = ( tree_node *)rtl[7]; 
if ( !gen_aux_info_record_compiled_from_record++ ) 
rtx result; // rax 
result = simplify_relational_operation( code, v9, rtx, v4); 
result = simplify_binary_operation( code, mode, op0, op1); 
if ( result ) 
return result; 
result = simplify_binary_operation( code, mode, v12, v4); 
if ( !result ) 
return result; 
if ( ( unsigned int)debug_info_level_0 <= DINFO_LEVEL_TERSE ) 
sprintf( v27, "*.%s%u", ( const char *)&off_66511C, v3->identifier.id.len >> 2); 
if ( call_insn_operand( operand0->fld[0].rtx, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)) ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)) ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
if ( call_insn_operand( operand1->fld[0].rtx, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)) ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)) ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v8 = copy_to_mode_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operand1->fld[0].rtx); 
v9 = copy_to_mode_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operand2->fld[0].rtx); 
v9 = insn_data_0[insn_code].genfun( op1, op2); 
v2 = ix86_expand_compare( ( rtx_code)*( _WORD *)operand0, 0LL, 0LL); 
if ( ( unsigned int)debug_info_level_0 >= DINFO_LEVEL_NORMAL ) 
if ( ( unsigned int)debug_info_level_0 < DINFO_LEVEL_NORMAL ) 
if ( ( unsigned int)debug_info_level_0 < DINFO_LEVEL_NORMAL ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v4 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v3); 
v12 = ( tree_node *)i[2]; 
result = simplify_gen_subreg( mode, x, ( machine_mode)v3, v5); 
sprintf( v18, "*.%s%u", ( const char *)&off_66511C, stmt->identifier.id.len >> 2); 
v2 = force_reg( ( machine_mode)BYTE2( v5), v2); 
rtx result; // rax 
__m256 element; // [rsp+20h] [rbp-78h] BYREF 
v8 = mode_class_0[mode]; 
return gen_rtx_fmt_e( ( rtx_code)( unsigned __int16)v3, mode, x->fld[0].rtx); 
return simplify_gen_subreg( mode, v2, ( machine_mode)v4, 0); 
*( _QWORD *)element.m256_f32 = v16; 
*( _QWORD *)element.m256_f32 = v13; 
slot_with_hash = htab_find_slot_with_hash( v17, &element, v13, INSERT); 
rtx = gen_rtx_fmt_w( CONST_INT, VOIDmode, *( __int64 *)element.m256_f32); 
*( _QWORD *)&element.m256_f32[4] = v21.r[2]; 
rtx result; // rax 
return gen_rtx_fmt_e( CLOBBER, ( machine_mode)v4, v9); 
result = rtx->fld[0].rtx; 
if ( ( unsigned __int16)*( _DWORD *)result == 66 ) 
if ( ( unsigned __int8)BYTE2( *( _DWORD *)result) == mode ) 
return result; 
result = gen_lowpart_common( mode, rtx); 
if ( result ) 
return result; 
return gen_rtx_fmt_ee( ( rtx_code)( unsigned __int16)*( _DWORD *)rtx, mode, rtx->fld[0].rtx, *( rtx *)&rtx[1]); 
result = simplify_gen_subreg( mode, rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v11); 
result = simplify_gen_subreg( mode, rtx, ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v11); 
if ( !result ) 
return gen_rtx_fmt_e( CLOBBER, ( machine_mode)v4, v9); 
v4 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)reg + 2)); 
if ( optab_table[30]->handlers[v2].insn_code != CODE_FOR_nothing || mode_class_0[v2] != MODE_CC ) 
return insn_data_0[optab_table[30]->handlers[v3].insn_code].genfun( v8, v10); 
return insn_data_0[optab_table[30]->handlers[v3].insn_code].genfun( v8, v10); 
operands[4] = gen_rtx_REG( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), 17); 
operands[4] = gen_rtx_REG( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), 17); 
v12 = gen_rtx( v10, ( machine_mode)v7, v11, v5); 
v12 = gen_rtx( v10, ( machine_mode)v7, v5, v11); 
v4 = gen_rtx_REG( ( machine_mode)v2, v3); 
v4 = gen_rtx_REG( ( machine_mode)v2, v3); 
if ( generating_concat_p && ( v2 = mode_class_0[mode], ( unsigned int)( v2 - 5) <= 1) ) 
rtx last_insn; // rbp 
rtx v12; // rax 
rtx v16; // r12 
rtx v20; // rax 
rtx v25; // rax 
rtx secondary_mem; // r14 
rtx v31; // r14 
last_insn = get_last_insn( ); 
&& ( v10 = gen_lowpart_common( ( machine_mode)v9, out)) != 0LL ) 
v19 = *( _OWORD *)&v2[v17 * 2 + 2]; 
*( _OWORD *)result[v17 + 1].elem = v19; 
v20 = *( _OWORD *)&v2[v17 * 2 + 6]; 
*( _OWORD *)result[v17 + 3].elem = v20; 
v21 = *( _OWORD *)&v2[v17 * 2 + 10]; 
*( _OWORD *)result[v17 + 5].elem = v21; 
v22 = *( _OWORD *)&v2[v17 * 2 + 14]; 
*( _OWORD *)result[v17 + 7].elem = v22; 
v25 = *( _OWORD *)&v2[v23 / 8 + 2]; 
v14 = *( _OWORD *)&v2[v12 * 2 + 2]; 
*( _OWORD *)result[v12 + 1].elem = v14; 
v15 = *( _OWORD *)&v2[v12 * 2 + 6]; 
*( _OWORD *)result[v12 + 3].elem = v15; 
v16 = *( _OWORD *)&v2[v12 * 2 + 10]; 
*( _OWORD *)result[v12 + 5].elem = v16; 
v17 = *( _OWORD *)&v2[v12 * 2 + 14]; 
*( _OWORD *)result[v12 + 7].elem = v17; 
v20 = *( _OWORD *)&v2[v18 / 8 + 2]; 
rtx result; // rax 
rtx v5; // rcx 
result = gen_rtx_fmt_E( SEQUENCE, VOIDmode, v3); 
v5 = cfun->emit->x_first_insn; 
if ( v5 ) 
*( _QWORD *)( result->fld[0].rtwint + v6) = v5; 
*( _QWORD *)( result->fld[0].rtwint + v6) = v5; 
v5 = v5[1].fld[0].rtx; 
v5 = v5[1].fld[0].rtx; 
while ( v5 ); 
return result; 
v6 = adjust_address_1( *operands, ( machine_mode)v5, 0LL, 1, 1); 
( machine_mode)v5); 
v12 = gen_rtx_fmt_ee( AND, ( machine_mode)v5, rtwint, v11); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v3 = gen_rtx_fmt_ee( MULT, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1], v2); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
ix86_split_fp_branch( ( rtx_code)*( _WORD *)*operands, operands[1], operands[2], operands[3], operands[4], 0LL); 
ix86_split_fp_branch( ( rtx_code)*( _WORD *)*operands, operands[1], operands[2], operands[3], operands[4], operands[5]); 
v1 = ix86_force_to_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2), operands[1]); 
v2 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v1); 
( rtx_code)( unsigned __int16)*( _DWORD *)operands[3], 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)operands[3]), 
ix86_free_from_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2)); 
v1 = ix86_force_to_memory( ( machine_mode)*( ( unsigned __int8 *)operands[2] + 2), operands[2]); 
v2 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v1); 
( rtx_code)( unsigned __int16)*( _DWORD *)operands[3], 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)operands[3]), 
ix86_free_from_memory( ( machine_mode)*( ( unsigned __int8 *)operands[2] + 2)); 
( rtx_code)( unsigned __int16)*( _DWORD *)v2, 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v2), 
v11 = gen_rtx( ( rtx_code)v1, ( machine_mode)v2, v10, const_int_rtx[64]); 
v1 = swap_condition( ( rtx_code)*( _WORD *)operands[1]); 
*( _WORD *)operands[1] = swap_condition( ( rtx_code)*( _WORD *)operands[1]); 
if ( const0_operand( operands[2], ( machine_mode)*( ( unsigned __int8 *)*operands + 2)) ) 
( rtx_code)( unsigned __int16)*( _DWORD *)operands[3], 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)operands[3]), 
v1 = ix86_force_to_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2), operands[1]); 
v2 = gen_rtx_fmt_e( FLOAT, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v1); 
ix86_free_from_memory( ( machine_mode)*( ( unsigned __int8 *)operands[1] + 2)); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
operands[2] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[2]); 
operands[3] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[3]); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
operands[2] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[2]); 
v1 = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[3]); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
operands[3] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[3]); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
v1 = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[3]); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
operands[3] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[3]); 
operands[4] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[4]); 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
operands[3] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[3]); 
v1 = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[4]); 
operands[1] = gen_lowpart( ( machine_mode)v4, operands[1]); 
operands[2] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[2]); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
operands[1] = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[1]); 
v1 = gen_lowpart( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), operands[2]); 
v8 = gen_rtx( ( rtx_code)*( _WORD *)operand1, VOIDmode, operand4, operand5); 
v8 = gen_rtx( ( rtx_code)*( _WORD *)operand1, VOIDmode, operand4, operand5); 
operand = insn_data_0[v3].operand, 
operand->predicate( r0, ( machine_mode)*( ( unsigned __int16 *)operand + 8))) 
&& operand[1].predicate( r1, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) 
&& operand[2].predicate( c, ( machine_mode)*( ( unsigned __int16 *)&operand[2] + 8)) ) 
return insn_data_0[v5].genfun( r0, r1); 
if ( ( unsigned int)debug_info_level_0 < DINFO_LEVEL_NORMAL ) 
if ( ( unsigned int)debug_info_level_0 >= DINFO_LEVEL_NORMAL ) 
if ( ( unsigned int)debug_info_level_0 < DINFO_LEVEL_NORMAL ) 
v2 = expand_simple_binop( ( machine_mode)v3, PLUS, operand0, v4, 0LL, 0, OPTAB_DIRECT); 
v2 = expand_simple_binop( ( machine_mode)v3, MINUS, pic_offset_table_rtx, operand0, 0LL, 1, OPTAB_DIRECT); 
v24 = ( tree_node *)v21[4]; 
if ( v24 == ( tree_node *)*( &global_trees + 27) ) 
v24 = ( tree_node *)v21[4]; 
v22 = ( tree_node *)xcalloc( 1uLL, 0x48uLL); 
v25 = ( tree_node *)xcalloc( 1uLL, 0x48uLL); 
rtx *v17; // rcx 
v17 = ( rtx *)( v13->fld[0].rtwint + 16); 
v17 = const_int_rtx + 520; 
*mult_val = *v17; 
if ( ( v2 & 0xFF0000) == 0 && v3 && ( mode_class_0[v3] | 2) != 3 ) 
|| mode_class_0[v3] == MODE_FLOAT && mode_size[v3] > mode_size[BYTE2( v2)] ) 
&& ( *( _WORD *)v9 == 70 || legitimate_address_p( ( machine_mode)BYTE2( v2), v9, 0)) ) 
if ( ( tree_node *)global_trees != attributes ) 
v5 = lang_hooks_0.get_alias_set( elements); 
v20 = ++new_alias_set_last_alias_set; 
v5 = lang_hooks_0.get_alias_set( v2->decl.section_name); 
v5 = ++new_alias_set_last_alias_set; 
x_arg_pointer_save_area = assign_stack_local_1( ( machine_mode)v3, mode_size[v3], 0, f); 
fatal_insn_not_found( insn, "insn-attrtab.c", 12189, "get_attr_athlon_decode"); 
return memory_operand( recog_data_0.operand[1], VOIDmode) != 0; 
return memory_operand( recog_data_0.operand[1], VOIDmode) != 0; 
return memory_operand( recog_data_0.operand[1], VOIDmode) != 0; 
fatal_insn_not_found( insn, "insn-attrtab.c", 11973, "get_attr_athlon_fpunits"); 
if ( register_operand( recog_data_0.operand[1], SImode) ) 
if ( immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) ) 
if ( immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( !register_operand( recog_data_0.operand[1], SImode) ) 
return 3 * ( immediate_operand( recog_data_0.operand[1], VOIDmode) == 0) + 1; 
if ( register_operand( recog_data_0.operand[1], SImode) ) 
if ( immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( register_operand( recog_data_0.operand[1], SImode) ) 
if ( !immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
v13 = recog_data_0.operand[3]; 
if ( mult_operator( recog_data_0.operand[3], SFmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 13438, "get_attr_i387"); 
v5 = recog_data_0.operand[3]; 
v5 = recog_data_0.operand[3]; 
v5 = recog_data_0.operand[3]; 
if ( get_attr_type( insn) == TYPE_FOP || mult_operator( recog_data_0.operand[3], TFmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 13072, "get_attr_imm_disp"); 
v18 = recog_data_0.operand[1]; 
if ( q_regs_operand( recog_data_0.operand[0], QImode) ) 
v18 = recog_data_0.operand[1]; 
if ( memory_displacement_operand( recog_data_0.operand[0], VOIDmode) ) 
v4 = recog_data_0.operand[1]; 
v7 = recog_data_0.operand[2]; 
v7 = recog_data_0.operand[2]; 
v7 = recog_data_0.operand[2]; 
v7 = recog_data_0.operand[2]; 
if ( !_bittest( &v20, ix86_cpu) || const1_operand( recog_data_0.operand[2], VOIDmode) || !_bittest( &v20, ix86_cpu) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 13642, "get_attr_length_address"); 
if ( !constant_call_address_operand( recog_data_0.operand[1], VOIDmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 14632, "get_attr_length_immediate"); 
if ( symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( ( unsigned int)( v20 - 7) < 3 || ( _DWORD)flag_pic && symbolic_operand( recog_data_0.operand[1], SImode) ) 
if ( ( unsigned int)( which_alternative - 2) <= 2 && aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( q_regs_operand( recog_data_0.operand[0], QImode) ) 
if ( symbolic_operand( recog_data_0.operand[1], DImode) ) 
if ( ( _DWORD)flag_pic && symbolic_operand( recog_data_0.operand[1], DImode) ) 
if ( !symbolic_operand( recog_data_0.operand[1], DImode) ) 
|| pic_symbolic_operand( recog_data_0.operand[2], DImode) ) 
v5 = recog_data_0.operand[2]; 
|| pic_symbolic_operand( recog_data_0.operand[2], SImode) ) 
|| pic_symbolic_operand( recog_data_0.operand[2], SImode) ) 
v5 = recog_data_0.operand[2]; 
if ( incdec_operand( recog_data_0.operand[2], HImode) || which_alternative == 2 ) 
v5 = recog_data_0.operand[2]; 
fatal_insn_not_found( insn, "insn-attrtab.c", 15861, "get_attr_memory"); 
if ( !memory_operand( recog_data_0.operand[0], VOIDmode) ) 
LODWORD( v3) = 2 * ( memory_operand( recog_data_0.operand[0], VOIDmode) != 0) + 1; 
if ( !memory_operand( recog_data_0.operand[1], VOIDmode) ) 
LODWORD( v3) = 2 * ( memory_operand( recog_data_0.operand[0], VOIDmode) != 0); 
|| ( LODWORD( v3) = 0, !symbolic_operand( recog_data_0.operand[1], SImode)) ) 
if ( !memory_operand( recog_data_0.operand[0], VOIDmode) 
|| ( LODWORD( v3) = 3, !memory_operand( recog_data_0.operand[1], VOIDmode)) ) 
if ( !memory_operand( recog_data_0.operand[0], VOIDmode) ) 
if ( !memory_operand( recog_data_0.operand[1], VOIDmode) 
|| !symbolic_operand( recog_data_0.operand[1], SImode) 
|| !memory_operand( recog_data_0.operand[2], VOIDmode)) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( memory_operand( recog_data_0.operand[0], VOIDmode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 17669, "get_attr_mode"); 
if ( ( unsigned int)( which_alternative - 2) <= 2 && aligned_operand( recog_data_0.operand[1], HImode) ) 
v19 = aligned_operand( recog_data_0.operand[1], HImode); 
if ( !q_regs_operand( recog_data_0.operand[0], QImode) ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 16766, "get_attr_modrm"); 
v4 = recog_data_0.operand[1]; 
v4 = recog_data_0.operand[0]; 
v7 = recog_data_0.operand[1]; 
if ( !register_operand( recog_data_0.operand[0], VOIDmode) ) 
if ( !immediate_operand( recog_data_0.operand[1], VOIDmode) ) 
if ( q_regs_operand( recog_data_0.operand[0], QImode) ) 
v7 = recog_data_0.operand[1]; 
if ( !memory_operand( recog_data_0.operand[1], VOIDmode) ) 
return !register_operand( recog_data_0.operand[1], SImode) && !register_operand( recog_data_0.operand[1], HImode); 
return !register_operand( recog_data_0.operand[1], SImode) && !register_operand( recog_data_0.operand[1], HImode); 
v5 = recog_data_0.operand[2]; 
v5 = recog_data_0.operand[2]; 
v5 = recog_data_0.operand[2]; 
fatal_insn_not_found( insn, "insn-attrtab.c", 19258, "get_attr_pent_pair"); 
v8 = recog_data_0.operand[1]; 
v8 = recog_data_0.operand[0]; 
if ( !( _DWORD)flag_pic || !symbolic_operand( recog_data_0.operand[1], SImode) ) 
v21 = recog_data_0.operand[1]; 
v5 = memory_operand( recog_data_0.operand[1], VOIDmode) != 0; 
if ( ( unsigned int)( which_alternative - 2) <= 2 && aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( q_regs_operand( recog_data_0.operand[0], QImode) ) 
if ( !( _DWORD)flag_pic || !symbolic_operand( recog_data_0.operand[1], DImode) ) 
v21 = recog_data_0.operand[1]; 
rtx v16; // rcx 
fatal_insn_not_found( insn, "insn-attrtab.c", 20323, "get_attr_prefix_0f"); 
if ( ( unsigned int)( which_alternative - 2) <= 2 && aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( !q_regs_operand( recog_data_0.operand[0], QImode) ) 
rtx = recog_data_0.operand[0]; 
if ( *( _WORD *)recog_data_0.operand[0] == 67 ) 
rtx = recog_data_0.operand[0]->fld[0].rtx; 
v16 = recog_data_0.operand[0]; 
v16 = recog_data_0.operand[0]; 
if ( *( _WORD *)recog_data_0.operand[0] == 67 ) 
v16 = recog_data_0.operand[0]->fld[0].rtx; 
v16 = recog_data_0.operand[0]->fld[0].rtx; 
v17 = insn_addresses_->data.i[v16->fld[0].rtint]; 
fatal_insn_not_found( insn, "insn-attrtab.c", 20460, "get_attr_prefix_data16"); 
fatal_insn_not_found( insn, "insn-attrtab.c", 20358, "get_attr_prefix_rep"); 
rtx v37; // rcx 
fatal_insn_not_found( insn, "insn-attrtab.c", 21978, "get_attr_type"); 
v26 = recog_data_0.operand[1]; 
if ( aligned_operand( recog_data_0.operand[1], HImode) ) 
if ( !register_operand( recog_data_0.operand[0], QImode) ) 
if ( q_regs_operand( recog_data_0.operand[0], QImode) ) 
v11 = swap_condition( ( rtx_code)*v9); 
result = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v5 = append_random_chars_value; 
if ( !append_random_chars_value ) 
append_random_chars_value = v5; 
v1[v6] = append_random_chars_letters[v5 % 0x3E]; 
v1[v6 + 1] = append_random_chars_letters[v5 / 0x3E 
v1[v6 + 2] = append_random_chars_letters[v5 / 0xF04 
v1[v6 + 3] = append_random_chars_letters[v5 / 0x3A2F8 
v1[v6 + 4] = append_random_chars_letters[v5 / 0xE17810 
v1[v6 + 5] = append_random_chars_letters[v5 / 0x369B13E0 
if ( v9 != 46 && ( sch_istable[v9] & 0x8C) == 0 ) 
result = get_frame_alias_set_set; 
if ( get_frame_alias_set_set == -1 ) 
result = new_alias_set_last_alias_set + 1; 
new_alias_set_last_alias_set = result; 
get_frame_alias_set_set = result; 
get_frame_alias_set_set = 0LL; 
hard_reg_initial_vals->entries = ( initial_value_pair_0 *)xmalloc( 0x50uLL); 
entries = ( initial_value_pair_0 *)xrealloc( hard_reg_initial_vals->entries, 16 * v9); 
result = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)reg + 2)); 
v8 = ( tree_node *)*( &global_trees + 15); 
arg0 = ( tree_node *)*( &global_trees + 17); 
return insn_data_0[code].name; 
rtx loc[2]; // [rsp+8h] [rbp-10h] BYREF 
loc[0] = last_value; 
return gen_lowpart_for_combine( ( machine_mode)*( ( unsigned __int8 *)x + 2), last_value); 
loc[0] = reg_last_set_value[rtuint]; 
if ( loc[0] ) 
last_value_validate = get_last_value_validate( loc, v6, *( ( _DWORD *)reg_last_set_label + rtuint), 0); 
result = loc[0]; 
loc[0] = copy_rtx( loc[0]); 
loc[0] = copy_rtx( loc[0]); 
if ( get_last_value_validate( loc, reg_last_set[rtuint], *( ( _DWORD *)reg_last_set_label + rtuint), 1) ) 
return loc[0]; 
v9 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v10] - 5) < 2) + 1; 
v8 = mode_class_0[mode]; 
( reload_type)v12, 
rtunion v3; // rax 
if ( ( *( _DWORD *)( v3.rtwint + 16) & 0x800FF) != 25 ) 
v5 = *( _QWORD *)( v3.rtwint + 40); 
v6 = *( _QWORD *)( v3.rtwint + 32); 
if ( v6 < 0 && ( *( _BYTE *)( *( _QWORD *)( v3.rtwint + 8) + 17LL) & 0x20) == 0 ) 
v6 = *( _QWORD *)( v3.rtwint + 32); 
*( _OWORD *)( v17 + 8) = 0LL; 
v8 = ( tree_node *)v17; 
result = get_varargs_alias_set_set; 
if ( get_varargs_alias_set_set == -1 ) 
result = new_alias_set_last_alias_set + 1; 
new_alias_set_last_alias_set = result; 
get_varargs_alias_set_set = result; 
get_varargs_alias_set_set = 0LL; 
v24 = ( page_group_0 *)&v18[v16 - v23]; 
v24 = ( page_group_0 *)( ( char *)v24 - G.pagesize); 
group = ( page_group_0 *)( page - 32); 
v29 = ( page_entry_0 *)xcalloc( 1uLL, n); 
if ( ( sch_istable[v3] & 4) != 0 ) 
page_group_0 *v6; // rbx 
page_group_0 *v6; // rbx 
v0 = _mm_sub_pd( ( __m128d)_mm_unpacklo_ps( ( __m128)G.allocated, ( __m128)xmm*(short *)0x675BE0), ( __m128d)xmm*(short *)0x675BF0); 
return object_size_table[( *( page_entry_0 ***)( ( char *)G.lookup + ( ( ( unsigned __int64)p >> 21) & 0x7F8)))[( ( unsigned __int64)p >> SLOBYTE( G.lg_pagesize)) & ~( -1 << ( 24 - LOBYTE( G.lg_pagesize)))]->order]; 
v1 = ( tree_node *)*( ( _QWORD *)&ggc_pending_trees->name + elements_used); 
rtl_op = first_rtl_op( ( tree_code)v27); 
v1 = ( *( page_entry_0 ***)( ( char *)G.lookup + ( ( ( unsigned __int64)p >> 21) & 0x7F8)))[( ( unsigned __int64)p >> SLOBYTE( G.lg_pagesize)) & ~( -1 << ( 24 - LOBYTE( G.lg_pagesize)))]; 
_OWORD *v10; // rax 
_OWORD *v10; // rax 
rtx regno_note; // rax 
rtx insns; // rax 
rtx v222; // rax 
if ( *( _BYTE *)( v3 + 9862847) || byte_98D96F[v3] ) 
v10 = xmalloc( 4 * v5); 
reg_allocno = v10; 
*v10 = -1LL; 
v10[1] = -1LL; 
v10[2] = -1LL; 
v10[3] = -1LL; 
v10[4] = -1LL; 
v10[5] = -1LL; 
v10[6] = -1LL; 
v8 = grokdeclarator( declarator, declspecs, ( decl_context)( 4 - ( width == 0LL)), 0); 
v10 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v8] - 5) < 2) + 1; 
( machine_mode)*( unsigned __int8 *)( *( _QWORD *)&v30[2 * v31 + 2] + 2LL), 
v11 = v4 + hash_expr_1( v43[1], ( machine_mode)*( ( unsigned __int8 *)v43[1] + 2), do_not_record_p); 
v49 = hash_expr_1( ( rtx)v48, ( machine_mode)*( unsigned __int8 *)( v48 + 2), do_not_record_p); 
v8 = v7 + hash_rtx( rtwint[1], ( machine_mode)*( ( unsigned __int8 *)rtwint[1] + 2), 0); 
v8 += hash_rtx( ( rtx)v36, ( machine_mode)*( unsigned __int8 *)( v36 + 2), 0); 
rtx v28; // rax 
*( ( _OWORD *)object_base + 1) = 0LL; 
if ( reg_note ) 
if ( *( _WORD *)reg_note->fld[0].rtwint == 66 ) 
v28 = ( rtx)insn[2]; 
if ( *( _WORD *)v28 != 47 ) 
v28 = single_set_2( insn, *( rtx *)&insn[2]); 
&& ( operand = insn_data_0[insn_code].operand, operand->predicate( 
( machine_mode)*( ( unsigned __int16 *)operand + 8))) 
&& operand[1].predicate( x, ( machine_mode)*( ( unsigned __int16 *)&operand[1] + 8)) ) 
*( _OWORD *)&v2->stack.chunk_size = 0LL; 
*( _OWORD *)&v2->stack.object_base = 0LL; 
*( _OWORD *)&v2->stack.chunk_limit = 0LL; 
*( _OWORD *)&v2->stack.alignment_mask = 0LL; 
*( _OWORD *)&v2->stack.freefun = 0LL; 
*( ( _OWORD *)&v2->stack + 5) = 0LL; 
rtx v55; // rax 
rtx v56; // rcx 
rtx v62; // rax 
rtx v63; // rcx 
rtx *v65; // rbx 
rtx v66; // rax 
rtx *v67; // rcx 
rtx *v68; // rdx 
rtx op; // [rsp+18h] [rbp-60h] BYREF 
rtx v75; // [rsp+20h] [rbp-58h] BYREF 
if ( ( mode_class_0[mode] | 2) != 3 ) 
*( _OWORD *)d.r = *( _OWORD *)&exp->block.vars; 
( machine_mode)( ( unsigned __int8)HIBYTE( WORD2( exp->common.type->block.abstract_origin)) >> 1), 
result[1] = ( rtx_def)_mm_load_si128( &v9); 
if ( mode && ( v2 & 0xFF0000) == 0 && ( mode_class_0[mode] | 2) != 3 ) 
rtx *v33; // rax 
rtx v37; // rsi 
v6 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), i); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
while ( v1 < ( const  struct builtin *)&off_65A430 ); 
cpp_define_builtin( pfile, ( &off_65A450)[v10]); 
*( _OWORD *)&cum->sse_words = 0LL; 
*( _OWORD *)&cum->words = 0LL; 
*( _OWORD *)&deps->pending_read_insns = 0LL; 
*( _OWORD *)&deps->pending_write_insns = 0LL; 
*( _OWORD *)&deps->pending_lists_length = 0LL; 
*( _OWORD *)&deps->last_function_call = 0LL; 
if ( init_flow_initialized ) 
init_flow_initialized = 1; 
rtx v20; // r13 
v20 = canon_rtx( rtx); 
if ( ( *( _DWORD *)v20 & 0x4000000) != 0 ) 
v21.rtwint = ( __int64)v20->fld[0]; 
if ( ( *( _DWORD *)v20 & 0xFF0000) == 3342336 ) 
if ( mode_size[*( ( unsigned __int8 *)v20 + 2)] > mode_size[*( unsigned __int8 *)( v22.rtwint + 2)] ) 
fld->rtwint = ( __int64)v20; 
if ( rtx_equal_p( v20->fld[0].rtx, *( rtx *)( v22.rtwint + 8)) ) 
v17 = alloc_EXPR_LIST( 0, v20, next); 
memset( ( char *)&stack_arg_under_construction + i, 0, v0); 
memset( &( &libiberty_nptr)[i / 8], 0, v0); 
*( _OWORD *)&cfun->x_temp_slots = 0LL; 
v2 = rtx_alloc( ( rtx_code)*( _WORD *)notes); 
v3 = lang_hooks_0.expand_constant( value); 
if ( v18 == ( tree_node *)*( &global_trees + 14) ) 
if ( v19 == ( tree_node *)*( &global_trees + 14) || v18 == ( tree_node *)*( &global_trees + 14) ) 
if ( v19 == ( tree_node *)*( &global_trees + 14) || v18 == ( tree_node *)*( &global_trees + 14) ) 
v29 = mode_class_0[v28]; 
v33 = mode_class_0[v32]; 
rtx v95; // r15 
rtx v104; // rbp 
rtx v118; // r12 
v14 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v13] - 5) < 2) + 1; 
v20 = _mm_add_epi64( _mm_shuffle_epi32( ( __m128i)x->fld[0].rtuint, 68), ( __m128i)xmm*(short *)0x641400); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641410); 
v26 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v27 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641430); 
v28 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E870); 
v16 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v14] - 5) < 2) + 1; 
*( _OWORD *)&v42[v43].const_rtx = 0LL; 
reg_eqv_table[v5] = ( reg_eqv_elem)-1LL; 
v50 = *( ( _DWORD *)uid_cuid_0 + v49[1]); 
if ( ( v50 > cse_basic_block_end || *( ( _DWORD *)uid_cuid_0 + *v49) < cse_basic_block_start) 
&& v50 > *( ( _DWORD *)uid_cuid_0 + *( int *)( reg_n_info->data.l[first_reg] + 4)) ) 
if ( v12 != ( ( unsigned int)( mode_class_0[v14] - 5) < 2) + 1 ) 
v13 = adjust_address_1( v13, ( machine_mode)v14, 0LL, 1, 1); 
v15 = gen_rtx_REG( ( machine_mode)*( ( unsigned __int8 *)v13 + 2), v7); 
fatal_insn_not_found( insn, "insn-attrtab.c", 29, "insn_current_length"); 
rtx v17; // rax 
rtx mem_set_list; // rbx 
rtx v19; // r14 
v17 = canon_rtx( rtx); 
mem_set_list = pbi->mem_set_list; 
if ( mem_set_list ) 
v19 = v17; 
v19 = v17; 
if ( anti_dependence( rtx, mem_set_list->fld[0].rtx) ) 
v23.rtwint = ( __int64)mem_set_list->fld[0]; 
if ( rtx_equal_p( v19->fld[0].rtx, *( rtx *)( v23.rtwint + 8)) ) 
if ( mode_size[*( ( unsigned __int8 *)v19 + 2)] <= mode_size[*( unsigned __int8 *)( v23.rtwint + 2)] ) 
fatal_insn_not_found( insn, "insn-attrtab.c", 356, "insn_default_length"); 
v5 = recog_data_0.operand[0]; 
v5 = recog_data_0.operand[0]; 
v5 = recog_data_0.operand[0]; 
rtx v45; // rax 
rtx v46; // rax 
rtx v47; // rax 
rtx v48; // rax 
rtx v51; // rax 
rtx v52; // rax 
rtx v53; // rax 
rtx *v55; // rax 
rtx v57; // rax 
rtx v58; // rax 
fatal_insn_not_found( insn, "insn-attrtab.c", 46, "insn_variable_length_p"); 
rtx v24; // rsi 
rtx v25; // rax 
rtx v27; // [rsp+10h] [rbp-38h] 
v24 = ( rtx)i1[2]; 
if ( *( _WORD *)v24 != 47 ) 
v24 = single_set_2( i1, v24); 
v24 = single_set_2( i1, v24); 
v24 = 0LL; 
v25 = ( rtx)i2[2]; 
if ( *( _WORD *)v25 != 47 ) 
rtx loc[4]; // [rsp+8h] [rbp-20h] BYREF 
loc[0] = rtx; 
loc[0] = copy_rtx( rtx); 
instantiate_virtual_regs_1( loc, 0LL, 0); 
if ( !memory_address_p( v5, loc[0]) ) 
if ( !memory_address_p( v7, loc[0]) ) 
instantiate_virtual_regs_1( loc, 0LL, 0); 
x->fld[0].rtx = loc[0]; 
rtx *v16; // r12 
rtx *v19; // rbp 
rtx *v48; // rax 
rtx *v51; // rbx 
rtx *v61; // rsi 
rtx arg0; // [rsp+10h] [rbp-58h] 
rtx value; // [rsp+20h] [rbp-48h] BYREF 
rtx v81; // [rsp+30h] [rbp-38h] 
v16 = ( rtx *)&v6[1]; 
value = ( rtx)v6[1]; 
instantiate_virtual_regs_1( &value, 0LL, 0); 
if ( *( _WORD *)value != 61 && *( _WORD *)value != 75 ) 
if ( *( _WORD *)value != 61 && *( _WORD *)value != 75 ) 
v8 = ( tree_node *)ggc_alloc( v7); 
&& *( _OWORD *)&c->block.vars >= *( _OWORD *)&initial->block.vars ) 
&& *( _OWORD *)&c->block.vars >= *( _OWORD *)&initial->block.vars ) 
&& *( _OWORD *)&c->block.vars >= *( _OWORD *)&initial->block.vars ) 
&& *( _OWORD *)&c->block.vars >= *( _OWORD *)&initial->block.vars ) 
v1 = mode_class_0[mode]; 
internal_error_function( msgid, ( va_list_0 *)va); 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&v1->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&v1->begin_diagnostic; 
LOBYTE( v27) = canon_hash( x, ( machine_mode)BYTE2( v3)); 
v57 = ( ( unsigned int)( mode_class_0[v56] - 5) < 2) + 1; 
v89 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v88] - 5) < 2) + 1; 
*( _OWORD *)&v2->loads = 0LL; 
v34 = ( unsigned int)v31 + ( ( unsigned int)( mode_class_0[( unsigned __int8)v32] - 5) < 2) + 1; 
&& ( v8 = ( unsigned int)reversed_comparison_code_parts( ( rtx_code)v7, v6[1], v6[2], v4)) != UNKNOWN ) 
v11 = gen_rtx_fmt_ee( v8, ( machine_mode)*( ( unsigned __int8 *)v6 + 2), v6[1], v6[2]); 
n_operands = recog_data_0.n_operands; 
v2 = ( __int64)*( &changes + n_operands--); 
if ( recog_data_0.n_operands <= 0 ) 
v2 = ( unsigned __int8)recog_data_0.n_operands + 1LL; 
v5 = *( ( _QWORD *)&changes_allocated + v2); 
fatal_insn( "unknown insn mode", insn, "i386.c", 9956, "ix86_attr_length_immediate_default"); 
if ( *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)op0 >> 14) & 0x3FC)) == MODE_FLOAT ) 
v6 = gen_rtx_REG( ( machine_mode)( ( ( v4 & 0x2000000 | 0x500000000uLL) - 1) >> 32), i); 
v7 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), pointer); 
v8 = adjust_address_1( v7, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), offset, 1, 1); 
v5 = gen_rtx_MEM( ( machine_mode)( ( ( v3 & 0x2000000 | 0x500000000uLL) - 1) >> 32), pointer); 
v6 = adjust_address_1( v5, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), offset, 1, 1); 
v7 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), i); 
if ( *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)v3 >> 14) & 0x3FC)) == MODE_FLOAT ) 
v38 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000) == 0) ^ 5), i); 
v13 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 2); 
v15 = gen_rtx_fmt_ee( PLUS, ( machine_mode)v14, global_rtl[4], v13); 
v18 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), global_rtl[4]); 
v23 = gen_rtx_fmt_ee( PLUS, ( machine_mode)v14, global_rtl[2], v13); 
*operands = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), *operands); 
v3 = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v3); 
*operands = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), *operands); 
v3 = force_reg( ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v3); 
v33 = reverse_condition_maybe_unordered( ( rtx_code)*( _WORD *)v31); 
v30 = swap_condition( ( rtx_code)*( _WORD *)v25); 
if ( *( _OWORD *)arg0 != 0LL ) 
v7 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
v8 = gen_reg_rtx( ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
v10 = gen_rtx_fmt_eee( IF_THEN_ELSE, ( machine_mode)*( ( unsigned __int8 *)*operands + 2), v5, operands[2], operands[3]); 
( machine_mode)*( ( unsigned __int8 *)*operands + 2), 
( machine_mode)*( ( unsigned __int8 *)*operands + 2), 
if ( symbolic_operand( v4, ( machine_mode)operands) ) 
v14 = mode_class_0[mode]; 
v14 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v13); 
v28 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), "_alloca"); 
if ( *( _OWORD *)arg1 != 0LL ) 
+ ( unsigned __int16)reverse_condition_maybe_unordered( ( rtx_code)*( unsigned __int16 *)arg1[0]) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
result = construct_container( ( machine_mode)v2, valtype, 1, 6, 8, x86_64_int_return_registers, 0); 
v3 = mode_class_0[v2]; 
return gen_rtx_REG( ( machine_mode)v2, v4); 
v2 = mode_class_0[mode]; 
v1 = mode_class_0[mode]; 
|| ( v3 = ( unsigned __int8)BYTE2( *( _DWORD *)x), v4 = mode_class_0[v3], v4 > 8) 
v19 = mode_class_0[v18]; 
v14 = force_reg( ( machine_mode)BYTE2( v6), v4); 
v12 = mode_class_0[BYTE2( v6)]; 
v30 = mode_class_0[v29]; 
v5 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v10 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v5 = classify_argument( ( machine_mode)v2, type, v9, 0); 
if ( ( _DWORD)v2 != 51 && ( ( unsigned int)( mode_class_0[v2] - 7) > 1 || int_size_in_bytes( type) != 8) ) 
if ( ( _DWORD)v4 == 6 || ( _DWORD)v4 == 18 || ( unsigned int)( mode_class_0[v4] - 7) <= 1 ) 
v10 = *( _OWORD *)&cum->words; 
*( _OWORD *)&v41.sse_words = *( _OWORD *)&cum->sse_words; 
*( _OWORD *)&v41.sse_words = *( _OWORD *)&cum->sse_words; 
*( _OWORD *)&v41.words = v10; 
v12 = *( _OWORD *)&cum->words; 
*( _OWORD *)&v41.sse_words = *( _OWORD *)&cum->sse_words; 
*( _OWORD *)&v41.sse_words = *( _OWORD *)&cum->sse_words; 
rtx v12; // rax 
rtx x; // [rsp+0h] [rbp-68h] BYREF 
rtx v30[2]; // [rsp+8h] [rbp-60h] BYREF 
rtx parts; // [rsp+18h] [rbp-50h] BYREF 
v12 = copy_rtx( *operands); 
*operands = v12; 
*( ( _BYTE *)v12 + 2) = 5 - ( ( target_flags & 0x2000000) == 0); 
v5 = ix86_split_to_parts( operands[1], &parts, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
v5 = ix86_split_to_parts( operands[1], &parts, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
ix86_split_to_parts( *operands, &x, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
ix86_split_to_parts( *operands, &x, ( machine_mode)*( ( unsigned __int8 *)*operands + 2)); 
rtx v12; // rax 
rtx operands; // [rsp+18h] [rbp-70h] BYREF 
operands = operand; 
operands = pool_constant; 
split_di( &operands, 1, parts, parts + 1); 
operands = v13; 
rtx v17; // r13 
rtx v52; // r15 
rtx memref; // [rsp+30h] [rbp-98h] 
tree v22; // rax 
tree v23; // rax 
tree v24; // rax 
tree v25; // rax 
tree v26; // [rsp+0h] [rbp-48h] 
tree v27; // [rsp+8h] [rbp-40h] 
tree v28; // [rsp+10h] [rbp-38h] 
return force_reg( ( machine_mode)v3, exp); 
v5 = gen_reg_rtx( ( machine_mode)v3); 
( machine_mode)BYTE2( v3), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
v7 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v5] - 5) < 2) + 1; 
v20 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
rtx *v31; // rbx 
rtx last_value; // rax 
v10 = *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)x >> 14) & 0x3FC)); 
v12 = *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)val >> 14) & 0x3FC)); 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
( machine_mode)*( ( unsigned __int8 *)v7 + 2), 
( machine_mode)BYTE2( v24)); 
( machine_mode)*( ( unsigned __int8 *)v7 + 2), 
( machine_mode)BYTE2( v16), 
&& *( const mode_class *)( ( char *)mode_class_0 + ( ( *( ( _DWORD *)&type->type + 15) >> 7) & 0x1FC)) == MODE_INT ) 
rtx v31; // rax 
rtx v40; // r14 
rtx v43; // r12 
rtx v44; // rax 
rtx v28; // rbx 
rtx v36; // rbx 
v26 = gen_rtx_fmt_Ei( UNSPEC, ( machine_mode)v15, v16, 15); 
v27 = gen_rtx_fmt_e( CONST, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v26); 
v28 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v27); 
v28 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v27); 
*( ( _BYTE *)v28 + 3) |= 4u; 
v29 = ix86_GOT_alias_set_set; 
if ( ix86_GOT_alias_set_set == -1 ) 
ix86_GOT_alias_set_set = v29; 
set_mem_alias_set( v28, v29); 
v2 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
v30 = gen_movsi( v2, v28); 
v17 = gen_rtx_fmt_Ei( UNSPEC, ( machine_mode)v15, v16, 6); 
v18 = gen_rtx_fmt_e( CONST, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v17); 
if ( ( sch_istable[v9] & 4) == 0 ) 
v7 = 2 * ( ( sch_istable[v9] & 4) == 0) + 8; 
if ( ( sch_istable[v18] & 4) == 0 && ( ( sch_istable[v18] & 0x100) == 0 || v7 != 16) ) 
if ( ( sch_istable[v18] & 4) == 0 && ( ( sch_istable[v18] & 0x100) == 0 || v7 != 16) ) 
v19 = hex_value[v18]; 
if ( ( sch_istable[( unsigned __int8)v34] & 4) == 0 ) 
while ( ( sch_istable[( unsigned __int8)v34] & 4) != 0 ) 
if ( ( sch_istable[v47] & 4) == 0 ) 
v38 = ( char *)&unk_64CEF8; 
v38 = ( char *)&unk_64CF08; 
v84 = ( tree_node *)*( &global_trees + 11); 
rtx insns; // rax 
rtx rtwint; // rbx 
v20 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 2); 
diddle_return_value( mark_reg_0, global_live_at_start); 
*( _OWORD *)&regs_ever_live[32] = 0LL; 
*( _OWORD *)&regs_ever_live[16] = 0LL; 
*( _OWORD *)regs_ever_live = 0LL; 
insns = get_insns( ); 
if ( insns ) 
rtwint = insns; 
rtwint = insns; 
v18.rtwint = ( __int64)rtwint[1].fld[0]; 
if ( ( unsigned __int16)*( _DWORD *)rtwint == 36 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v82 = mem_loc_descriptor( rtx, ( machine_mode)BYTE2( v26)); 
rtx v16; // r14 
rtx v25; // rax 
rtx v26; // rax 
rtx v32; // rax 
rtx v42; // rax 
rtx v43; // rcx 
*( _OWORD *)&loc->offset = 0LL; 
v15 = *( tree_node **)&f[2 * v7 + 2]; 
values = *( tree_node **)&f[2 * v8 + 2]; 
v10 = *( tree_node **)&f[2 * v7 + 2]; 
v11 = ( tree_node **)&f[2 * v7 + 4]; 
LODWORD( v1) = file_table_0.table; 
if ( !LODWORD( file_table_0.table) 
|| strcmp( file_name, *( const char **)( cfa_temp.offset + 8LL * LODWORD( file_table_0.table))) ) 
LODWORD( file_table_0.table) = v1; 
LODWORD( file_table_0.table) = v1; 
fwrite( *( ( const void **)&ptr + ( __int16)v6), 7uLL, 1uLL, file); 
rtx v65; // rax 
rtx v77; // rax 
rtx v78; // rax 
rtx v79; // rbp 
rtx v81; // rax 
rtx v89; // rax 
rtx v91; // rdi 
rtx v107; // rdx 
rtx reg; // [rsp+8h] [rbp-70h] 
v12 = expand_mult_add( x, reg, v10, v11, ( machine_mode)*( ( unsigned __int8 *)reg + 2), 1); 
v11 = expand_mult_add( v8, reg, v9, v10, ( machine_mode)*( ( unsigned __int8 *)reg + 2), 1); 
v14 = expand_mult_add( v9, reg, v12, v13, ( machine_mode)*( ( unsigned __int8 *)reg + 2), 1); 
rtx v10; // rdx 
rtx v12; // rbx 
rtx x_forced_labels; // rax 
rtx v22; // rbx 
rtx v28; // rax 
rtx v31; // r15 
rtx last_insn; // rax 
rtx k; // rax 
v23 = gen_rtx_REG( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), i); 
rtx compound_operation; // rax 
rtx v27; // rbp 
rtx v28; // rax 
rtx *rtwint; // rcx 
rtx v56; // rax 
rtx *v62; // rbp 
rtx *intoa; // [rsp+8h] [rbp-40h] 
*p_rtl = adjust_address_1( rtl, ( machine_mode)( unsigned __int8)supercontext, 0LL, 0, 1); 
v30 = gen_rtx_fmt_s( SYMBOL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), low); 
if ( !ix86_hard_regno_mode_ok( v11, ( machine_mode)v14) ) 
v18 = gen_rtx_fmt_i0( REG, ( machine_mode)LOBYTE( decl->block.supercontext), reg_number); 
v22 = ( ( unsigned int)( mode_class_0[v20] - 5) < 2) + 1; 
rtx v12; // r9 
rtx v18; // rdx 
rtx v21; // rbp 
rtx v28; // rax 
rtx v29; // r12 
rtx result; // rax 
result = adjust_address_1( v3, ( machine_mode)*( ( unsigned __int8 *)ad + 2), 0LL, 0, 1); 
result = adjust_address_1( v3, ( machine_mode)*( ( unsigned __int8 *)ad + 2), 0LL, 0, 1); 
if ( result == reg_equiv_memory_loc[regno] ) 
return copy_rtx( result); 
return result; 
v3 = ( tree_node *)ggc_alloc( v2); 
v6 = transp_0[src->index]; 
tree v27; // r12 
tree v28; // rax 
v2 = ( tree_node *)ggc_alloc( v1); 
rtx v5; // r14 
rtx v7; // r14 
rtx v8; // r15 
v5 = gen_rtx_fmt_u00( LABEL_REF, VOIDmode, v4); 
mark_jump_label( v5, rtx, 0); 
*( _QWORD *)( *( _QWORD *)&rtx[2] + 32LL) = v5->fld[0].rtwint; 
if ( reg_note ) 
v7 = reg_note; 
v7 = reg_note; 
if ( ( unsigned int)( mode_class_0[v4] - 5) < 2 ) 
if ( ( unsigned int)( mode_class_0[v9] - 5) >= 2 ) 
v3 = ( ( unsigned int)( mode_class_0[( unsigned __int8)BYTE2( *( _DWORD *)cfun->emit->x_regno_reg_rtx[regno])] - 5) < 2) 
v17 = ( ( unsigned int)( mode_class_0[v15] - 5) < 2) + 1; 
v22 = _mm_add_epi32( _mm_shuffle_epi32( _mm_cvtsi32_si128( v17), 80), ( __m128i)xmm*(short *)0x641440); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v27 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
if ( ( unsigned int)( mode_class_0[v3] - 5) >= 2 ) 
v6 = ( ( unsigned int)( mode_class_0[v5] - 5) < 2) + 1; 
v10 = _mm_add_epi32( _mm_shuffle_epi32( _mm_cvtsi32_si128( rtuint), 80), ( __m128i)xmm*(short *)0x641460); 
v12 = _mm_shuffle_pd( ( __m128d)hard_regs_live, ( __m128d)xmm*(short *)0x675E90, 2); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641470); 
v17 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v18 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641480); 
v19 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641490); 
v20 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x6414A0); 
v8 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
rtx mem_set_list; // rbx 
rtx v28; // r14 
rtx *p_mem_set_list; // rax 
rtx *v32; // r13 
rtx v33; // rbp 
rtx v35; // r14 
rtx *v37; // rax 
rtx v39; // r12 
rtx v40; // rbx 
rtx *reg_next_use; // rsi 
mark_set_1( pbi, ( rtx_code)( unsigned __int16)v12, *( ( rtx *)v11 + 1), v7, insn, pbi->flags); 
mark_set_1( pbi, ( rtx_code)( unsigned __int16)v8, v4->fld[0].rtx, v7, insn, pbi->flags); 
? ( v7 = ( ( unsigned int)( mode_class_0[v6] - 5) < 2) + 1) 
rtx v16; // rbp 
rtx *v17; // rax 
rtx v21; // r12 
rtx v22; // r14 
rtx *v23; // rax 
rtx *v30; // rbx 
rtx v32; // [rsp+8h] [rbp-60h] 
rtx v33; // [rsp+10h] [rbp-58h] 
rtx v35; // [rsp+10h] [rbp-58h] 
rtx *listp; // [rsp+18h] [rbp-50h] 
listp = &pbi->mem_set_list; 
v32 = insn; 
v15 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v14] - 5) < 2) + 1; 
v21 = _mm_add_epi64( _mm_shuffle_epi32( ( __m128i)x->fld[0].rtuint, 68), ( __m128i)xmm*(short *)0x641400); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641410); 
v27 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v28 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641430); 
v29 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E870); 
v30 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E880); 
if ( mode_class_0[BYTE2( v2)] == MODE_FLOAT 
|| *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)x->fld[0].rtwint >> 14) & 0x3FC)) == MODE_FLOAT ) 
if ( *( const mode_class *)( ( char *)mode_class_0 + v3) == MODE_FLOAT ) 
if ( *( const mode_class *)( ( char *)mode_class_0 + ( ( v2 >> 14) & 0x3FC)) == MODE_FLOAT 
v19 = ( const char *)&off_6FF833; 
induction_1 *giv; // rcx 
induction_1 *giv; // rcx 
induction_1 *v24; // r15 
induction_1 *v24; // r15 
rtx mult_val; // rax 
rtx v28; // rax 
rtx add_val; // rcx 
induction_1 *biv; // r8 
induction_1 *biv; // r8 
rtx v32; // rsi 
rtx v37; // rdi 
rtx v38; // rcx 
if ( ( sch_istable[v9] & 4) == 0 ) 
if ( ( sch_istable[( unsigned __int8)v9] & 4) != 0 ) 
while ( ( sch_istable[( unsigned __int8)v9] & 4) != 0 ); 
rtx pool_constant_mark; // rbp 
pool_constant_mark = get_pool_constant_mark( rtx, &pmarked); 
if ( ( unsigned __int16)*( _DWORD *)pool_constant_mark == 68 ) 
if ( ( *( _DWORD *)pool_constant_mark & 0x4000000) == 0 ) 
get_pool_constant_mark( pool_constant_mark, &pmarked); 
rtx = pool_constant_mark; 
pool_constant_mark = rtx; 
pool_constant_mark = rtx; 
v13->dw_loc_oprnd1.v.val_offset = ( unsigned __int64)pool_constant_mark; 
v14->data.l[elements_used] = ( __int64)pool_constant_mark; 
v16 = mem_loc_descriptor( rtx->fld[0].rtx, ( machine_mode)BYTE2( v3)); 
rtx v10; // rax 
rtx v11; // rbx 
rtx v16; // rbx 
rtx constptr; // [rsp+10h] [rbp-38h] BYREF 
v11 = gen_reg_rtx( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v8)); 
v11 = gen_reg_rtx( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v8)); 
v8 = force_operand( v8, v11); 
if ( v8 != v11 ) 
emit_move_insn( v11, v8); 
constptr = const_int_rtx[64]; 
v14 = eliminate_constant_term( v8, &constptr); 
rtx v21; // r14 
rtx v27; // rbp 
rtx v42; // r15 
v21 = x->fld[0].rtx; 
v21 = ( rtx)x[1]; 
v21 = canon_rtx( x); 
v27 = y->fld[0].rtx; 
v6 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v7] - 5) < 2) + 1; 
v15 = ( tree_node *)ggc_alloc( v14); 
rtx v9; // r12 
rtx v14; // rax 
v9 = ( rtx)v5[1]; 
if ( *( _WORD *)v9 == 37 ) 
v9 = a->end; 
*( _OWORD *)&b->pred = 0LL; 
v14 = delete_insn( v5); 
v14 = v5[1].fld[0].rtx; 
v5 = v14; 
end = v9; 
while ( v9 != end ) 
v18 = v9->fld[0].rtint; 
v9 = v9[1].fld[0].rtx; 
v9 = v9[1].fld[0].rtx; 
warning_with_decl( newdecl, aWeakDeclaratio_0); 
if ( in_section_0 != in_const ) 
in_section_0 = in_const; 
v8 = operand_sub*(short *)0xforce( x, i, mode); 
genfun = insn_data_0[insn_code].genfun; 
rtx v24; // rcx 
rtx v30; // rsi 
rtx v32; // rdx 
rtx *v59; // rbx 
rtx v60; // rax 
rtx v64; // [rsp+10h] [rbp-58h] 
rtx *v68; // [rsp+28h] [rbp-40h] 
rtx v69; // [rsp+30h] [rbp-38h] BYREF 
v68 = pnotes; 
v64 = to_insn; 
to_insn = v64; 
pnotes = v68; 
rtx i; // r13 
rtx v9; // rbp 
rtx v12; // rax 
rtx regno_note; // rax 
rtx ( *v38)[59]; // rcx 
for ( i = pat + 1; ; i = ( rtx)( v6 + 2) ) 
for ( i = pat + 1; ; i = ( rtx)( v6 + 2) ) 
v6 = *( int **)i; 
v7 = **( _DWORD **)i; 
( machine_mode)( unsigned __int8)BYTE2( *v8), 
( machine_mode)BYTE2( v7))] 
*( _QWORD *)i = v6; 
v9 = rtx; 
warning( ( &off_702390)[code - 1], v3); 
if ( in_section_0 != in_named || strcmp( name, in_named_name) ) 
in_section_0 = v6; 
return ++new_alias_set_last_alias_set; 
*( ( _OWORD *)object_base + 1) = 0LL; 
v15 = gen_rtx_fmt_ee( code, ( machine_mode)*( ( unsigned __int8 *)if_info->cond + 2), cmp_a, cmp_b); 
v16 = gen_rtx_fmt_eee( IF_THEN_ELSE, ( machine_mode)*( ( unsigned __int8 *)x + 2), v15, vtrue, vfalse); 
if ( !general_operand( cmp_a, ( machine_mode)*( ( unsigned __int8 *)cmp_a + 2)) 
|| !general_operand( cmp_b, ( machine_mode)*( ( unsigned __int8 *)cmp_b + 2)) ) 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
v11 = gen_rtx_fmt_ee( v10, ( machine_mode)*( ( unsigned __int8 *)x + 2), cond->fld[0].rtx, *( rtx *)&cond[1]); 
rtx reg_equal_equiv_note; // rax 
rtx v23; // rax 
rtx v25; // rax 
rtx v27; // rax 
rtx v29; // rax 
rtx v30; // rax 
rtx jump; // rbx 
rtx v33; // rbp 
rtx v34; // rax 
rtx v18; // rax 
rtx i; // rbx 
if ( ( _WORD)v7 != 61 || *( const mode_class *)( ( char *)mode_class_0 + ( ( v7 >> 14) & 0x3FC)) != MODE_INT ) 
if ( ( _WORD)v17 == 61 && *( const mode_class *)( ( char *)mode_class_0 + ( ( v17 >> 14) & 0x3FC)) == MODE_INT ) 
v18 = canonicalize_condition( jump, ( rtx)v5, v14, earliest, v16); 
if ( v18 ) 
v11 = v18; 
for ( i = *earliest; i != jump; i = i[1].fld[0].rtx ) 
for ( i = *earliest; i != jump; i = i[1].fld[0].rtx ) 
for ( i = *earliest; i != jump; i = i[1].fld[0].rtx ) 
v5 = mode_class_0[v4]; 
if ( mode == VOIDmode || ( v4 & 0xFF0000) != 0 || ( mode_class_0[mode] | 2) == 3 ) 
rtx v42; // rbp 
rtx v46; // rdx 
rtx v47; // rsi 
rtx v50; // rax 
rtx v62; // rax 
rtx v64; // rbp 
rtx v68; // rax 
rtx v71; // rax 
rtx v80; // rax 
v8 = mode_class_0[v7]; 
v10 = mode_class_0[mode]; 
+ ( ( *( _DWORD *)x >> 13) & 0x7F8)) | rtwint & nonzero_bits( x, ( machine_mode)v7)); 
v60 = nonzero_bits( x->fld[0].rtx, ( machine_mode)v7); 
if ( mode_class_0[mode] == MODE_INT ) 
rtx epilogue_delay_list; // rbx 
epilogue_delay_list = cfun->epilogue_delay_list; 
if ( epilogue_delay_list ) 
while ( !can_throw_external( epilogue_delay_list) ) 
epilogue_delay_list = ( rtx)epilogue_delay_list[1]; 
if ( !epilogue_delay_list ) 
rtx last_value; // rax 
v10 = mode_class_0[( int)v2]; 
v11 = mode_class_0[v5]; 
last_value = get_last_value( v7); 
if ( last_value ) 
v3 = *( _DWORD *)last_value; 
v5 = ( unsigned __int8)BYTE2( *( _DWORD *)last_value); 
LODWORD( v2) = ( unsigned __int8)BYTE2( *( _DWORD *)last_value); 
v7 = last_value; 
|| ( v65 = nonzero_bits( v7, ( machine_mode)v2), _bittest64( &v65, ( unsigned __int8)( v12 - 1))) ) 
v23 = num_sign_bit_copies( v38, ( machine_mode)v2); 
v27 = num_sign_bit_copies( *( rtx *)&v7[1], ( machine_mode)v2); 
v30 = num_sign_bit_copies( v7->fld[0].rtx, ( machine_mode)v2); 
if ( *( _OWORD *)&t1 == 0LL || t1 && readonly_fields_p( t1) ) 
rtx v7; // r14 
_OWORD *v16; // rax 
_OWORD *v16; // rax 
v5 = simplify_gen_binary( PLUS, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), rtx, offset); 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)memref + 2), v5) 
v6 = force_reg( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)rtx), rtx); 
v5 = simplify_gen_binary( PLUS, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v6, offset); 
v7 = change_address_1( memref, VOIDmode, v5, 1); 
v16 = ggc_alloc( 0x28uLL); 
*slot = v16; 
*( ( _QWORD *)v16 + 4) = v21; 
v16[1] = v20; 
*v16 = v17; 
*( _QWORD *)&v7[1] = v14; 
return v7; 
rtx v20; // [rsp+0h] [rbp-38h] BYREF 
v20 = v3; 
v20 = y; 
if ( ( unsigned __int16)v6 == 75 && ( constant_term_loc = find_constant_term_loc( &v20)) != 0LL ) 
v20 = *constant_term_loc; 
*v15 = v20; 
return offsettable_address_p( 0, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)op), op->fld[0].rtx) != 0; 
timevar_push( TV_DUMP_0); 
timevar_pop( TV_DUMP_0); 
( machine_mode)*( unsigned __int8 *)( v9.rtwint + 2), 
( machine_mode)BYTE2( v4)); 
( machine_mode)*( unsigned __int8 *)( v13.rtwint + 2), 
( machine_mode)BYTE2( v6)); 
rtx v5; // rbp 
v5 = modify_mem_list[*( int *)( basic_block_for_insn->data.l[rtint] + 88)]; 
if ( v5 ) 
v6 = uid_cuid_1; 
v7 = *( ( _DWORD *)uid_cuid_1 + rtint); 
v8.rtwint = ( __int64)v5->fld[0]; 
v5 = ( rtx)v5[1]; 
if ( !v5 ) 
v6 = uid_cuid_1; 
rtx v6; // rbx 
v6 = modify_mem_list[*( int *)( basic_block_info->data.l[current_bb] + 88)]; 
if ( v6 ) 
v7 = uid_cuid_1; 
v8 = *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
v9.rtwint = ( __int64)v6->fld[0]; 
gcse_mem_operand = rtx; 
gcse_mems_conflict_p = 0; 
if ( gcse_mems_conflict_p ) 
v6 = ( rtx)v6[1]; 
if ( !v6 ) 
v7 = uid_cuid_1; 
if ( reg_avail_info_0[rtx->fld[0].rtuint].last_bb == current_bb ) 
return reg_avail_info_0[rtx->fld[0].rtuint].last_set < *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
return reg_avail_info_0[rtx->fld[0].rtuint].last_set < *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
return reg_avail_info_0[rtx->fld[0].rtuint].first_set >= *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
return reg_avail_info_0[rtx->fld[0].rtuint].first_set >= *( ( _DWORD *)uid_cuid_1 + insn->fld[0].rtint); 
v26 = mode_class_0[BYTE2( v24)]; 
if ( v26 != mode_class_0[v27] ) 
*( _OWORD *)htab = 0LL; 
*( _OWORD *)ptr = 0LL; 
lang_hooks_0.tree_inlining.add_pending_fn_decls( &va, v6); 
v22 = ( tree_node *)*( &global_trees + 12); 
v22 = ( tree_node *)*( &global_trees + 11); 
transp_0 = sbitmap_vector_alloc( n_basic_blocks, 1u); 
comp_0 = sbitmap_vector_alloc( n_basic_blocks, 1u); 
sbitmap_vector_ones( transp_0, n_basic_blocks); 
transp_0[v3]->elms[0] &= ~1uLL; 
note_stores( *( ( rtx *)v6 + 4), reg_becomes_live_0, &to); 
v26 = (  struct simple_bitmap_def *)comp_0; 
sbitmap_vector_zero( comp_0, n_basic_blocks); 
v29 = comp_0; 
sbitmap_not( v26, transp_0[v32-- - 2]); 
v33 = pre_edge_lcm( ( FILE *)v26, 1, transp_0, comp_0, antic, ( sbitmap *)v23, &insert_0, &delete); 
v33 = pre_edge_lcm( ( FILE *)v26, 1, transp_0, comp_0, antic, ( sbitmap *)v23, &insert_0, &delete); 
v33 = pre_edge_lcm( ( FILE *)v26, 1, transp_0, comp_0, antic, ( sbitmap *)v23, &insert_0, &delete); 
if ( ( insert_0[v36 - 2]->elms[0] & 1) != 0 ) 
v45 = transp_0[index]; 
free( transp_0); 
free( comp_0); 
free( insert_0); 
rtx v38; // rax 
rtx v40; // rax 
rtx v41; // rbp 
rtx v42; // rax 
rtx v45; // rax 
rtx v48; // rbp 
rtx v49; // rax 
rtx v53; // rax 
rtx v54; // rax 
elements = lang_hooks_0.expand_constant( exp); 
v8 = ( tree_node *)high[4]; 
rtx v30; // rsi 
rtx v31; // rax 
rtx v36; // rsi 
rtx v37; // rax 
rtx v39; // rax 
rtx paddressp[32]; // [rsp+30h] [rbp-1A8h] BYREF 
v4 = ( int *)paddressp; 
if ( !LODWORD( paddressp[0]) ) 
fprintf( ( FILE *)asm_out_file, &off_6474F4[1], ( unsigned int)insn_counter); 
v23 = sch_istable[( unsigned __int8)v10]; 
if ( ( sch_istable[*( ( unsigned __int8 *)v2 + 2)] & 4) == 0 ) 
insn_data_0[debug_insn[2].fld[0].rtint].name); 
if ( insn_data_0[rtint].n_alternatives >= 2 ) 
fprintf( ( FILE *)asm_out_file, off_671883, ( unsigned int)( which_alternative + 1)); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v6 = lang_hooks_0.expand_constant( exp); 
if ( in_section_0 == in_text ) 
fprintf( ( FILE *)asm_out_file, ( const char *)&off_71EE08 + 4, "\t.zero\t", ( unsigned int)v5); 
if ( in_section_0 != in_text ) 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
rtl = gen_rtx_MEM( ( machine_mode)v13, v14); 
if ( in_section_0 != in_data ) 
if ( in_section_0 == in_const ) 
in_section_0 = in_const; 
if ( in_section_0 != in_const ) 
if ( in_section_0 == in_data ) 
in_section_0 = in_data; 
if ( in_section_0 != in_const ) 
in_section_0 = in_const; 
assemble_real( ( machine_mode)v16, v7->align, *( realvaluetype *)&constant[1]); 
*( _OWORD *)&varasm->x_first_pool = 0LL; 
values = ( tree_node *)high[3]; 
if ( in_section_0 == in_text ) 
fprintf( ( FILE *)asm_out_file, ( const char *)&off_71EE08 + 4, "\t.zero\t", ( unsigned int)v27); 
if ( in_section_0 == in_text ) 
fprintf( ( FILE *)asm_out_file, ( const char *)&off_71EE08 + 4, "\t.zero\t", ( unsigned int)length); 
if ( in_section_0 == in_text ) 
fprintf( ( FILE *)asm_out_file, ( const char *)&off_71EE08 + 4, "\t.zero\t", ( unsigned int)v50); 
if ( in_section_0 == in_text ) 
fprintf( ( FILE *)asm_out_file, ( const char *)&off_71EE08 + 4, "\t.zero\t", ( unsigned int)v15); 
sprintf( digit_buffer, &off_6474F4[1], *v11); 
sprintf( digit_buffer, ( const char *)&off_6660B2, *v12); 
sprintf( digit_buffer, off_6660B6, *v49); 
sprintf( digit_buffer, &off_66C47B[1], *v21); 
sprintf( digit_buffer, off_66B3AF, *v39); 
v11 = ( tree_node *)v10; 
v13 = *( _OWORD *)&buffer->state.diagnostic_count[4]; 
v2 = *( _OWORD *)&buffer->state.prefix; 
v3 = *( _OWORD *)&buffer->state.indent_skip; 
v4 = *( _OWORD *)&buffer->state.cursor; 
v12 = *( _OWORD *)buffer->state.diagnostic_count; 
*( _OWORD *)&buffer->state.prefix = 0LL; 
buffer->state.format_args = ( va_list_0 *)va; 
*( _OWORD *)&buffer->state.diagnostic_count[4] = v13; 
*( _OWORD *)buffer->state.diagnostic_count = v12; 
if ( !parmlist_tags_warning_already ) 
if ( !parmlist_tags_warning_already ) 
parmlist_tags_warning_already = 1; 
if ( ( sch_istable[v18] & 0x88) == 0 ) 
while ( v4 == 46 || ( sch_istable[v4] & 0x204) != 0 ); 
if ( ( _DWORD)v4 != 46 && ( sch_istable[( unsigned __int8)v4] & 0x204) == 0 ) 
if ( ( sch_istable[( unsigned __int8)v20] & 0x88) != 0 ) 
if ( ( sch_istable[( unsigned __int8)v8] & 0x400) != 0 ) 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
( va_list_0 *)va, 
v3 = spelling_0; 
if ( spelling_base < spelling_0 ) 
v3 = spelling_0; 
diagnostic_for_decl( decl, msgid, ( va_list_0 *)va, flag_pedantic_errors == 0); 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
if ( !peep2_insn_data_0[v5].insn ) 
reg_set_to_hard_reg_set( &v48, peep2_insn_data_0[v5].live_before); 
if ( !peep2_insn_data_0[v5].insn ) 
reg_set_to_hard_reg_set( v49, peep2_insn_data_0[v5].live_before); 
v13 = peep2_find_free_register_search_ofs + v12 - 53; 
if ( peep2_find_free_register_search_ofs + v12 <= 52 ) 
v13 = peep2_find_free_register_search_ofs + v12; 
peep2_find_free_register_search_ofs = 0; 
v19 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v23 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v30 = _mm_add_epi32( _mm_shuffle_epi32( v29, 80), ( __m128i)xmm*(short *)0x641440); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v35 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
insn = peep2_insn_data_0[v1].insn; 
if ( !peep2_insn_data_0[v3].insn ) 
v6 = ( ( unsigned int)( mode_class_0[( unsigned __int8)BYTE2( *( _DWORD *)reg)] - 5) < 2) + 1; 
if ( !peep2_insn_data_0[v3].insn ) 
recog_data_0.insn = 0LL; 
result = gen_peephole2_1291( insn, recog_data_0.operand); 
result = gen_peephole2_1290( insn, recog_data_0.operand); 
result = gen_peephole2_1295( insn, recog_data_0.operand); 
result = gen_peephole2_1296( insn, recog_data_0.operand); 
result = gen_peephole2_1297( insn, recog_data_0.operand); 
result = gen_peephole2_1276( insn, recog_data_0.operand); 
result = gen_peephole2_1275( insn, recog_data_0.operand); 
result = gen_peephole2_1280( insn, recog_data_0.operand); 
result = gen_peephole2_1281( insn, recog_data_0.operand); 
result = gen_peephole2_1282( insn, recog_data_0.operand); 
recog_data_0.operand[0] = v63; 
recog_data_0.operand[3] = ( rtx)v232; 
if ( rtx_equal_p( v233, recog_data_0.operand[0]) ) 
recog_data_0.operand[1] = v234; 
peep2_insn_data_0[0].live_before = bitmap_initialize( &head); 
peep2_insn_data_0[1].live_before = bitmap_initialize( &v64); 
peep2_insn_data_0[2].live_before = bitmap_initialize( &v65); 
peep2_insn_data_0[3].live_before = bitmap_initialize( &v66); 
peep2_insn_data_0[4].live_before = bitmap_initialize( &v67); 
peep2_insn_data_0[0].insn = 0LL; 
peep2_insn_data_0[1].insn = 0LL; 
peep2_insn_data_0[2].insn = 0LL; 
peep2_insn_data_0[3].insn = 0LL; 
peep2_insn_data_0[4].insn = global_rtl[0]; 
bitmap_copy( peep2_insn_data_0[4].live_before, v1); 
peep2_insn_data_0[v7].insn = ( rtx)end; 
bitmap_copy( peep2_insn_data_0[peep2_current].live_before, v1); 
insn = peep2_insn_data_0[v16].insn; 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v45.rtwint), 
if ( *( _WORD *)peep2_insn_data_0[v49].insn == 34 ) 
rtx v19; // rbp 
rtx arg0; // [rsp+8h] [rbp-40h] BYREF 
rtx p; // [rsp+18h] [rbp-30h] BYREF 
p = x; 
add_double( *( _QWORD *)&x[1], x[1].fld[0].rtwint, v3, v3 >> 63, ( unsigned __int64 *)&arg0, &i1); 
return immed_double_const( ( __int64)arg0, i1, VOIDmode); 
v19 = force_const_mem( v16, v18); 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v19 + 2), v19->fld[0].rtx) ) 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v19 + 2), v19->fld[0].rtx) ) 
if ( !memory_address_p( ( machine_mode)*( ( unsigned __int8 *)v19 + 2), v19->fld[0].rtx) ) 
return v19; 
if ( !find_constant_term_loc( &p) ) 
v1 = constructor_stack_0; 
v1 = constructor_stack_0; 
if ( constructor_range_stack_0 ) 
v10 = spelling_0; 
v11 = ( unsigned __int64)( ( char *)spelling_0 - ( char *)spelling_base) >> 4; 
spelling_0 = v10; 
spelling_0 = v10 + 1; 
spelling_0 = &spelling_base[constructor_depth]; 
constructor_range_stack_0 = v1->range_stack; 
spelling_0 = &spelling_base[depth]; 
constructor_stack_0 = v1->next; 
if ( constructor_stack_0 ) 
if ( *( const mode_class *)( ( char *)mode_class_0 + ( ( **( ( _DWORD **)aux + 6) >> 14) & 0x3FC)) == MODE_FLOAT 
*( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)v20 >> 14) & 0x3FC)) == MODE_FLOAT) ) 
hitrate = 10000 - predictor_info_0[predictor].hitrate; 
hitrate = predictor_info_0[predictor].hitrate; 
*( _OWORD *)&v0->x_nonlocal_labels = 0LL; 
*( _OWORD *)&v0->x_nonlocal_goto_handler_labels = 0LL; 
*( _OWORD *)&v1->original_arg_vector = 0LL; 
*( _OWORD *)&v1->x_temp_slots = 0LL; 
*( _OWORD *)&v1->x_rtl_expr_chain = 0LL; 
if ( recog_data_0.n_operands > 0 ) 
n_alternatives = recog_data_0.n_alternatives; 
v3 = recog_data_0.constraints[v2]; 
v8 = ( reg_class *)( v6 + v5 + 9987960); 
n_alternatives = recog_data_0.n_alternatives; 
while ( v4 < recog_data_0.n_alternatives ); 
while ( v2 < recog_data_0.n_operands ); 
*( _OWORD *)v45 = 0LL; 
sprintf( s, &off_6474F4[1], *( _DWORD *)&x[1]); 
v5 = ( char *)off_6F024B; 
*( ( _QWORD *)&v47 + 1) = &unk_710228; 
v5 = ( char *)&off_6F2CB0; 
v5 = ( char *)&off_6F2C9E; 
v5 = ( _BYTE *)( &off_6F2CB0 + 5); 
v5 = ( char *)&off_6F2CA7; 
v9 = &off_710234; 
v9 = &off_710234; 
v9 = &off_710238; 
v9 = &off_710238; 
v9 = &off_710240; 
fprintf( outfile, off_6763D0, name->int_cst.int_cst.low); 
v8 = ( const char *)&unk_700E66; 
v18 = ( char *)&off_67C877; 
v18 = ( char *)&off_6F2CE7; 
v18 = ( char *)&off_6F2CE1; 
put_condition_code( ( rtx_code)v13, v14, v15, v20, v5); 
v26 = mode_class_0[v12]; 
v7 = ( const char *)&unk_67B048; 
v7 = ( const char *)&unk_67B048; 
if ( *( _OWORD *)&out.base != 0LL ) 
fprintf( file, off_67B096, scale); 
fprintf( file, off_67B09A, scale); 
fprintf( file, off_67AF83, ( unsigned int)( rtint - 29)); 
rtx v26; // rbx 
rtx v43; // rcx 
rtx *v45; // rax 
rtx tmp_rtx; // [rsp+30h] [rbp-30h] 
tmp_rtx = rtx_first; 
if ( tmp_rtx ) 
rtx = tmp_rtx[1].fld[0].rtx; 
in_bb_p = ( print_rtl_graph_with_bb::bb_state *)v15; 
tmp_rtx = rtx; 
if ( ( v14[rtint] & 0x80000000) != 0 && ( *( _WORD *)tmp_rtx == 35 || *( _WORD *)tmp_rtx == 37) ) 
if ( ( v14[rtint] & 0x80000000) != 0 && ( *( _WORD *)tmp_rtx == 35 || *( _WORD *)tmp_rtx == 37) ) 
tmp_rtx->fld[0].rtuint, 
v26 = tmp_rtx; 
rtx i; // rbx 
for ( i = cfun->epilogue_delay_list; i; i = ( rtx)i[1] ) 
for ( i = cfun->epilogue_delay_list; i; i = ( rtx)i[1] ) 
for ( i = cfun->epilogue_delay_list; i; i = ( rtx)i[1] ) 
print_rtl_single( outf, i->fld[0].rtx); 
rtx v49; // rax 
fprintf( outfile, off_6474F4, *( _DWORD *)&in_rtx[2]); 
fprintf( outfile, off_6474F4, *( _DWORD *)&in_rtx[3]); 
v35 = print_rtx_hi_name[v24]; 
v35 = print_rtx_qi_name[v24]; 
v35 = print_rtx_hi_name[v39]; 
fprintf( outfile, off_67AF83, ( unsigned int)( v24 - 29)); 
fprintf( outfile, off_6474F4, rtuint); 
fprintf( outfile, off_6763D0, note_insn_name[v22]); 
fprintf( outfile, off_6474F4); 
if ( ( sch_istable[*( unsigned __int8 *)v32] & 4) != 0 ) 
sprintf( v73, off_710215); 
sprintf( v73, off_71F3EB, *( _DWORD *)&x[1]); 
sprintf( v73, off_710211, *( unsigned int *)( x->fld[0].rtwint + 8)); 
v2 = convert_to_mode( ( machine_mode)v4, size, 1); 
v15 = gen_rtx_fmt_ee( MINUS, ( machine_mode)v12, v13, v14); 
v18 = gen_rtx_fmt_ee( MINUS, ( machine_mode)v12, v13, v17); 
v25 = force_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v25); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
v4 = constructor_stack_0; 
if ( constructor_stack_0->replacement_value ) 
v4 = constructor_stack_0; 
if ( constructor_stack_0->replacement_value ) 
if ( constructor_stack_0->implicit ) 
if ( !constructor_stack_0->implicit ) 
if ( general_operand( v2, ( machine_mode)*( ( unsigned __int8 *)reaching_reg + 2)) ) 
v6 = copy_to_mode_reg( ( machine_mode)BYTE2( v7), copy); 
fatal_insn( "Attempt to delete prologue/epilogue insn:", insn, "flow.c", 1615, "propagate_one_insn"); 
rtx v8; // rax 
rtx v11; // rax 
rtx v12; // r15 
rtx v14; // r15 
rtx v15; // rax 
rtx v16; // r12 
v5 = gen_reg_rtx( ( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
v11 = protect_from_queue( x->fld[0].rtx, 0); 
if ( v11 != x->fld[0].rtx ) 
v12 = v11; 
v12 = v11; 
rtx x; // [rsp+8h] [rbp-C0h] BYREF 
rtx data[2]; // [rsp+10h] [rbp-B8h] BYREF 
data[0] = ( rtx)&v11; 
x = insns; 
data[1] = rtx; 
for_each_rtx( &x, insns_for_mem_walk, data); 
for_each_rtx( &x, insns_for_mem_walk, data); 
rtx = x; 
x = rtx; 
for ( v6.rtwint = ( __int64)v1[3].fld[0]; ; v6.rtwint = ( __int64)data[0][1] ) 
data[0] = v6.rtx; 
rtx v40; // rax 
rtx v48; // r14 
rtx v51; // r14 
rtx v69; // rax 
rtx v74; // rax 
rtx v78; // rax 
rtx v82; // rax 
( machine_mode)( unsigned __int8)BYTE2( *v16), 
( machine_mode)BYTE2( v14)); 
v7 = subreg_regno_offset( v8, ( machine_mode)( unsigned __int8)BYTE2( *v6), v3[4], ( machine_mode)BYTE2( v4)); 
v7 = subreg_regno_offset( v8, ( machine_mode)( unsigned __int8)BYTE2( *v6), v3[4], ( machine_mode)BYTE2( v4)); 
rtx v2; // r14 
rtx v3; // rax 
v2 = 0LL; 
v3 = 0LL; 
v3 = rtx; 
if ( !v2 ) 
v3 = rtx; 
if ( rtx[2] != v2[2] ) 
v3 = rtx; 
if ( rtint != v2[2].fld[0].rtint ) 
v3 = v2; 
v3 = v2; 
v2 = v3; 
v2 = v3; 
v5 = convert_modes( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), ptr_mode, size, 1); 
v9 = copy_to_mode_reg( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), v5); 
while ( constructor_stack_0->implicit ) 
v7->next = constructor_stack_0; 
v11 = spelling_0; 
v13 = ( unsigned __int64)( ( char *)spelling_0 - ( char *)spelling_base) >> 4; 
constructor_stack_0 = v7; 
v7->range_stack = constructor_range_stack_0; 
constructor_range_stack_0 = 0LL; 
spelling_0 = v11; 
v11 = spelling_0; 
v18 = ( unsigned __int64)( ( char *)spelling_0 - ( char *)spelling_base) >> 4; 
spelling_0 = v11; 
spelling_0 = v11 + 1; 
rtx v62; // rax 
*( _OWORD *)&v2->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&v2->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&emit->x_first_insn = 0LL; 
rtx v5; // rax 
*( _OWORD *)&v1->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&v1->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&emit->x_first_insn = 0LL; 
v5 = rtx; 
v6->x_last_insn = v5; 
*( _OWORD *)&v0->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&v0->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&emit->x_first_insn = 0LL; 
*( _OWORD *)&v3->x_first_insn = *( _OWORD *)&v6->first; 
*( _OWORD *)&v3->x_first_insn = *( _OWORD *)&v6->first; 
*( _OWORD *)&v1->parm_flag = 0LL; 
*( _OWORD *)&v1->this_block = 0LL; 
*( _OWORD *)&v1->shadowed = 0LL; 
*( _OWORD *)&v1->names = 0LL; 
v11 = ( _BYTE *)( &off_67B8E8 + 4); 
v11 = ( _BYTE *)( &off_67B8E8 + 7); 
v11 = ( const char *)&off_67B8E8; 
v11 = off_66C47B + 2; 
v11 = ( const char *)&unk_67B8F2; 
put_reg_into_stack( v11, rtl, decl->common.type, v12, ( machine_mode)v8, v15, 0, ( unsigned __int8)v4, 0LL); 
LODWORD( v5) = ( int)( ( double)( *( int *)( ( char *)&qty_0->size + v5) 
* *( int *)( ( char *)&qty_0->freq + v5) 
* floor_log2_wide( *( int *)( ( char *)&qty_0->n_refs + v5))) 
/ ( double)( *( int *)( ( char *)&qty_0->death + v5) - *( int *)( ( char *)&qty_0->birth + v5)) 
/ ( double)( *( int *)( ( char *)&qty_0->death + v5) - *( int *)( ( char *)&qty_0->birth + v5)) 
- ( int)( ( double)( qty_0[v6].size * qty_0[v6].freq * floor_log2_wide( qty_0[v6].n_refs)) 
- ( int)( ( double)( qty_0[v6].size * qty_0[v6].freq * floor_log2_wide( qty_0[v6].n_refs)) 
- ( int)( ( double)( qty_0[v6].size * qty_0[v6].freq * floor_log2_wide( qty_0[v6].n_refs)) 
/ ( double)( qty_0[v6].death - qty_0[v6].birth) 
/ ( double)( qty_0[v6].death - qty_0[v6].birth) 
fprintf( di_0->stream, off_71BF36, v15, 6552963LL); 
fprintf( di_0->stream, asc_71BF35, 25LL, 6552963LL); 
aka = ( tree_node *)prev_try->aka; 
v16 = ( tree_node *)v13->aka; 
if ( ( sch_istable[( unsigned __int8)ch_0] & 0xC00) == 0 ) 
while ( ( sch_istable[( unsigned __int8)v8] & 0xC00) == 0 ) 
if ( ( sch_istable[( unsigned __int8)v10] & 0xC00) == 0 ) 
while ( v12 != -1 && ( sch_istable[( unsigned __int8)v12] & 1) != 0 ); 
if ( in_section_0 != in_const ) 
in_section_0 = in_const; 
*( _OWORD *)x.r = *( _OWORD *)&expr->block.vars; 
*( _OWORD *)x.r = *( _OWORD *)&expr->block.vars; 
*( _OWORD *)retstr->r = 0LL; 
ereal_from_int( retstr, low, high, ( machine_mode)v5); 
ereal_from_uint( retstr, low, high, ( machine_mode)v5); 
*( _OWORD *)v15.r = a7; 
*( _OWORD *)a1 = *( _OWORD *)v8->r; 
*( _OWORD *)x.r = *( _OWORD *)&expr->block.vars; 
*( _OWORD *)( v2 + 88) = 0LL; 
constructor_stack_0 = (  struct constructor_stack *)v2; 
constructor_depth = ( unsigned __int64)( ( char *)spelling_0 - ( char *)spelling_base) >> 4; 
rttree = ( tree_node *)*( &global_trees + 11); 
v10 = ( tree_node *)*( &global_trees + 17); 
rtx i; // rax 
for ( i = cfun->expr->x_forced_labels; i; i = ( rtx)i[1] ) 
for ( i = cfun->expr->x_forced_labels; i; i = ( rtx)i[1] ) 
for ( i = cfun->expr->x_forced_labels; i; i = ( rtx)i[1] ) 
v5.rtwint = ( __int64)i->fld[0]; 
recog_data_0.insn = 0LL; 
recog_data_0.operand[0] = ( rtx)v18; 
recog_data_0.operand[0] = ( rtx)v18; 
recog_data_0.operand[0] = v21; 
recog_data_0.operand[0] = rtx; 
recog_data_0.operand[1] = v8; 
recog_data_0.operand[1] = v7; 
recog_data_0.operand[0] = ( rtx)v13; 
recog_data_0.operand[1] = ( rtx)v4; 
recog_data_0.operand[2] = v6; 
|| ( recog_data_0.operand[0] = v9, !ix86_match_ccmode( insn, CCGCmode)) 
|| ( result = 213LL, recog_data_0.operand[2]->fld[0].rtint == 0x80000000) ) 
recog_data_0.operand[2] = v6; 
recog_data_0.operand[0] = ( rtx)v12; 
&& rtx_equal_p( *( rtx *)( v182 + 8), recog_data_0.operand[1]) 
&& rtx_equal_p( *( rtx *)( v182 + 16), recog_data_0.operand[2]) 
v15 = ix86_binary_operator_ok( MINUS, SImode, recog_data_0.operand); 
recog_data_0.operand[0] = ( rtx)v12; 
&& rtx_equal_p( *( rtx *)( v14 + 8), recog_data_0.operand[1]) 
&& rtx_equal_p( *( rtx *)( v14 + 16), recog_data_0.operand[2]) 
v15 = ix86_binary_operator_ok( MINUS, SImode, recog_data_0.operand); 
recog_data_0.operand[1] = v16; 
recog_data_0.operand[2] = v17; 
recog_data_0.operand[0] = v186; 
if ( *( _WORD *)recog_data_0.operand[1] == 66 ) 
v187 = recog_data_0.operand[2]; 
rtx v940; // rcx 
rtx v1003; // rcx 
recog_data_0.operand[0] = v5; 
recog_data_0.operand[1] = ( rtx)v8; 
v11 = rtx_equal_p( *( rtx *)( v10 + 16), recog_data_0.operand[0]); 
recog_data_0.operand[1] = v190; 
recog_data_0.operand[2] = v191; 
if ( *( _WORD *)recog_data_0.operand[1] == 66 && *( _WORD *)v191 == 66 ) 
recog_data_0.operand[1] = v196; 
recog_data_0.operand[2] = v197; 
recog_data_0.operand[1] = v201; 
recog_data_0.operand[2] = v202; 
recog_data_0.operand[1] = v9.rtx; 
recog_data_0.operand[2] = v98; 
recog_data_0.operand[0] = v100; 
if ( !rtx_equal_p( *( rtx *)( v101 + 8), recog_data_0.operand[1]) ) 
recog_data_0.operand[1] = v9.rtx; 
recog_data_0.operand[2] = v11; 
recog_data_0.operand[0] = v14; 
if ( !rtx_equal_p( *( rtx *)( v15 + 8), recog_data_0.operand[1]) ) 
recog_data_0.operand[1] = v21.rtx; 
recog_data_0.operand[2] = v105; 
recog_data_0.operand[0] = v107; 
if ( rtx_equal_p( *( rtx *)( v108 + 8), recog_data_0.operand[1]) ) 
recog_data_0.operand[1] = v21.rtx; 
recog_data_0.operand[2] = v24; 
recog_data_0.operand[0] = v27; 
if ( rtx_equal_p( *( rtx *)( v28 + 8), recog_data_0.operand[1]) ) 
recog_data_0.operand[2] = v7.rtx; 
recog_data_0.operand[3] = v108; 
recog_data_0.operand[0] = v110; 
if ( rtx_equal_p( *( rtx *)( v111 + 8), recog_data_0.operand[2]) ) 
recog_data_0.operand[1] = v114; 
if ( rtx_equal_p( *( rtx *)( v115 + 8), recog_data_0.operand[3]) ) 
recog_data_0.operand[2] = v7.rtx; 
recog_data_0.operand[3] = v10; 
recog_data_0.operand[0] = v13; 
if ( !rtx_equal_p( *( rtx *)( v14 + 8), recog_data_0.operand[2]) ) 
recog_data_0.operand[1] = v17; 
if ( !rtx_equal_p( *( rtx *)( v18 + 8), recog_data_0.operand[3]) ) 
rtx opc; // [rsp+8h] [rbp-40h] 
recog_data_0.operand[0] = rtx; 
recog_data_0.operand[1] = v6; 
recog_data_0.operand[0] = rtx; 
recog_data_0.operand[1] = v8; 
recog_data_0.operand[1] = v8; 
recog_data_0.operand[0] = rtx; 
recog_data_0.operand[1] = v22; 
v23 = *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)v22 >> 14) & 0x3FC)); 
recog_data_0.operand[1] = v12; 
recog_data_0.operand[1] = v11; 
frees = undobuf_0.frees; 
if ( undobuf_0.frees ) 
undobuf_0.frees = undobuf_0.frees->next; 
undobuf_0.frees = undobuf_0.frees->next; 
frees->next = undobuf_0.undos; 
undobuf_0.undos = frees; 
reg_dead_regno = v33; 
reg_dead_endregno = v33 + 1; 
reg_dead_flag = 0; 
if ( reg_dead_flag ) 
if ( reg_dead_flag != 1 ) 
if ( find_regno_note( nonnote_insn, REG_DEAD, reg_dead_regno) ) 
v41 = reg_dead_regno; 
if ( reg_dead_regno < reg_dead_endregno ) 
if ( reg_dead_regno < reg_dead_endregno ) 
if ( ++v41 >= reg_dead_endregno ) 
v35 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v34] - 5) < 2) + 1; 
reg_dead_endregno = v40 + v33; 
reg_dead_flag = 0; 
v19 = costs_0; 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
p_int_cst = &exp->int_cst.int_cst; 
p_int_cst = &exp->int_cst.int_cst; 
pointer = p_int_cst; 
v43 = ( tree_node *)v42[4]; 
v27 = lang_hooks_0.expand_constant( exp); 
rtx *v11; // r10 
rtx *v16; // rax 
rtx *v20; // r8 
rtx *v24; // rdi 
v8 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v9] - 5) < 2) + 1; 
v11 = reg_last_death; 
v16 = &reg_last_death[v7 + 14]; 
*( __m128i *)&( &v11[v7])[v15] = si128; 
*( __m128i *)&v16[v15 - 12] = si128; 
*( __m128i *)&v16[v15 - 10] = si128; 
*( __m128i *)&v16[v15 - 8] = si128; 
*( __m128i *)&v16[v15 - 6] = si128; 
*( __m128i *)&v16[v15 - 4] = si128; 
*( __m128i *)&v16[v15 - 2] = si128; 
rtx v15; // rbx 
rtx v18; // rax 
rtx v19; // rax 
rtx v20; // r8 
rtx v22; // r11 
rtx *v23; // rsi 
rtx v41; // r12 
rtx v54; // rcx 
rtx v15; // rax 
rtx v16; // rbx 
rtx v23; // rax 
rtx v24; // rbp 
rtx v33; // rax 
rtx v34; // rbx 
reg_set_0 **v6; // rcx 
reg_set_0 **v6; // rcx 
reg_set_table = ( reg_set_0 **)xrealloc( reg_set_table, ( unsigned int)( 8 * ( regno + 100))); 
v6 = reg_set_table; 
v6[regno] = ( reg_set_0 *)object_base; 
v6[regno] = ( reg_set_0 *)object_base; 
v91 = recog_data_0.operand[v90 / 4]; 
v13 = recog_data_0.operand[v7]; 
rtx last_value; // rax 
rtx v11; // r13 
rtx *v15; // rcx 
rtx *v19; // rbp 
rtx *v20; // r8 
rtx orig; // [rsp+0h] [rbp-48h] BYREF 
rtx v39; // [rsp+10h] [rbp-38h] 
orig = value; 
v6 = ( ( unsigned int)( mode_class_0[v7] - 5) < 2) + 1; 
v39 = reg; 
last_value = get_last_value( reg); 
if ( last_value ) 
v11 = last_value; 
v11 = last_value; 
rtx v33; // rsi 
rtx v38; // rbx 
rtx v44; // rax 
rtx v46; // rbx 
rtx v48; // rax 
rtx v51; // r15 
rtx nonnote_insn; // rax 
rtx v55; // rax 
*( _OWORD *)&e->flags = 0LL; 
*( _OWORD *)&e->insns = 0LL; 
*( _OWORD *)&e->src = 0LL; 
*( _OWORD *)&e->pred_next = 0LL; 
rtx *v22; // rbx 
v22 = ( rtx *)&x[1]; 
if ( loc == v22 ) 
x = *v22; 
LODWORD( result) = ( ( unsigned int)( mode_class_0[( unsigned __int8)result] - 5) < 2) + 1; 
LODWORD( result) = ( ( unsigned int)( mode_class_0[v26] - 5) < 2) + 1; 
rtx *v14; // rbx 
rtx *v20; // rax 
v14 = ( rtx *)&v5[1]; 
if ( v14 == loc ) 
v5 = *v14; 
( machine_mode)( unsigned __int8)BYTE2( *rtwint), 
( machine_mode)BYTE2( v6)); 
result = ( ( unsigned int)( mode_class_0[v32] - 5) < 2) + 1; 
result = ( ( unsigned int)( mode_class_0[( unsigned __int8)v28] - 5) < 2) + 1; 
v20 = ( rtx *)&v5->fld[v17]; 
if ( v20 != loc ) 
v14 = ( rtx *)v5->fld; 
v21 = refers_to_regno_p( regno, endregno, *v20, loc); 
if ( reg_pref_0 ) 
return reg_pref_0[regno].altclass; 
v11 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v10] - 5) < 2) + 1; 
v18 = ( ( unsigned int)( mode_class_0[v17] - 5) < 2) + 1; 
v5 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v10 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
v16 = ( ( unsigned int)( mode_class_0[v14] - 5) < 2) + 1; 
v22 = _mm_add_epi32( _mm_shuffle_epi32( _mm_cvtsi32_si128( v20), 80), ( __m128i)xmm*(short *)0x641440); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v27 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
v39 = ( ( unsigned int)( mode_class_0[v37] - 5) < 2) + 1; 
v44 = _mm_add_epi32( _mm_shuffle_epi32( _mm_cvtsi32_si128( v39), 80), ( __m128i)xmm*(short *)0x641440); 
( machine_mode)*( unsigned __int8 *)( v9.rtwint + 2), 
( machine_mode)BYTE2( v4)); 
v11 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v14] - 5) < 2) + 1; 
( machine_mode)*( unsigned __int8 *)( v15.rtwint + 2), 
( machine_mode)BYTE2( v4)); 
v16 = ( ( unsigned int)( mode_class_0[v17] - 5) < 2) + 1; 
if ( reg_pref_0 ) 
return reg_pref_0[regno].prefclass; 
if ( reg_note ) 
v30 = *( _DWORD *)reg_note->fld[0].rtwint - 58; 
v47 = ( ( unsigned int)( *( const mode_class *)( ( char *)mode_class_0 + ( ( v45 >> 14) & 0x3FC)) - 5) < 2) + ( _DWORD)rtint; 
costs_0 = v4; 
memset( costs_0, 0, n); 
reg_pref_0 = reg_pref_buffer; 
fprintf( dump, " %s:%i", dump_regclass_reg_class_names[j], ( unsigned int)costs_0[v18].cost[j]); 
fprintf( dump, " %s:%i", dump_regclass_reg_class_names[j], ( unsigned int)costs_0[v18].cost[j]); 
fprintf( dump, " MEM:%i\n", ( unsigned int)costs_0[v17].mem_cost); 
v28 = costs_0[v21].cost[v27 + 23]; 
mem_cost = costs_0[v23].mem_cost; 
if ( costs_0[v23].cost[v33] < mem_cost ) 
if ( costs_0[v23].cost[v33 + 1] < mem_cost ) 
if ( dump && ( v26 != reg_pref_0[v21].prefclass || v29 != reg_pref_0[v21].altclass) ) 
if ( dump && ( v26 != reg_pref_0[v21].prefclass || v29 != reg_pref_0[v21].altclass) ) 
fprintf( dump, " pref %s\n", dump_regclass_reg_class_names[v26]); 
v31 = dump_regclass_reg_class_names[v26]; 
fprintf( dump, " pref %s, else %s\n", v31, dump_regclass_reg_class_names[v29]); 
v22 = reg_pref_0; 
reg_pref_0[v21].prefclass = v26; 
*( _OWORD *)init_cost.cost = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[4] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[4] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[8] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[8] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[12] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[12] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[16] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[16] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[20] = xmm*(short *)0x703E20; 
*( _OWORD *)&init_cost.cost[20] = xmm*(short *)0x703E20; 
reg_pref_0 = 0LL; 
if ( mode_class_0[v3] == MODE_FLOAT ) 
rtx v53; // rax 
rtx v54; // rsi 
rtx v62; // rax 
rtx v64; // rax 
rtx v81; // rax 
rtx real_insn; // rbx 
rtx v84; // rax 
rtx *v87; // rcx 
rtx v92; // rax 
v5 = ( ( unsigned int)( mode_class_0[mode] - 5) < 2) + 1; 
rtx *v158; // r13 
rtx *v162; // r14 
rtx *v164; // rax 
rtx *v172; // rbx 
rtx *loc; // [rsp+48h] [rbp-210h] 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705060); 
v3 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705010); 
v4 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v5 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705050); 
v7 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705040); 
v8 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705000); 
v9 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x704FF0); 
rtx v29; // r14 
rtx v36; // rax 
rtx v48; // rbx 
rtx j; // rbx 
rtx *v85; // rax 
rtx v87; // rbp 
rtx v106; // rax 
rtx v313; // rcx 
rtx *v317; // rbp 
rtx *v318; // rbx 
rtx v324; // rbp 
rtx v339; // rbx 
rtx *v34; // rax 
v10 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v8] - 5) < 2) + 1; 
fancy_abort( &off_7088B8[4], 8994, "reload_combine_note_use"); 
fancy_abort( &off_7088B8[4], 9015, "reload_combine_note_use"); 
if ( ( unsigned int)( mode_class_0[v18] - 5) < 2 ) 
v34 = ( rtx *)( v31 + 10488712); 
*v34 = v14; 
else if ( !rtx_equal_p( v14, *v34) ) 
rtx nonnote_insn; // rax 
rtx v34; // r11 
rtx v44; // rax 
rtx v45; // rbp 
rtx offset; // rax 
rtx *v47; // rbp 
rtx **v51; // r12 
rtx *v53; // rbp 
rtx v84; // rbx 
rtx *v99; // rsi 
rtx v102; // rax 
if ( recog_data_0.n_alternatives && recog_data_0.n_operands ) 
if ( recog_data_0.n_alternatives && recog_data_0.n_operands ) 
fatal_insn_not_found( insn, &off_7088B8[4], 8371, "reload_cse_simplify_operands"); 
v2 = 4LL * recog_data_0.n_alternatives; 
n_operands = recog_data_0.n_operands; 
if ( recog_data_0.n_operands > 0 ) 
v7 = recog_data_0.operand[v5]; 
v10 = recog_data_0.operand_mode[v5]; 
n_operands = recog_data_0.n_operands; 
if ( v5 >= recog_data_0.n_operands ) 
if ( recog_data_0.n_operands > 0 ) 
n_alternatives = recog_data_0.n_alternatives; 
v19 = ( int *)( ( char *)equiv_regs - ( ( 4LL * recog_data_0.n_alternatives + 15) & 0xFFFFFFFFFFFFFFF0LL)); 
v20 = recog_data_0.constraints[v15]; 
mode = recog_data_0.operand_mode[v15]; 
( machine_mode)BYTE2( v7), 
*( ( reg_class *)&regclass_map + v5->fld[0].rtuint), 
v8 = ix86_memory_move_cost( ( machine_mode)BYTE2( v7), class2, 1); 
v10 = cselib_lookup( v5, ( machine_mode)*( unsigned __int8 *)( set->fld[0].rtwint + 2), 0); 
( machine_mode)BYTE2( v15), 
*( ( reg_class *)&regclass_map + loc->fld[0].rtuint), 
v12 = ( ( unsigned int)( mode_class_0[v7] - 5) < 2) + 1; 
fancy_abort( &off_7088B8[4], 4527, "reload_reg_free_p"); 
fancy_abort( &off_7088B8[4], 4663, "reload_reg_reaches_end_p"); 
fancy_abort( &off_7088B8[4], 4739, "reloads_conflict"); 
*( _OWORD *)( ( char *)&replacements[0].where + 3 * v7) = *( _OWORD *)&v4->where; 
*( _OWORD *)( ( char *)&replacements[0].where + 3 * v7) = *( _OWORD *)&v4->where; 
*( _OWORD *)&e->flags = 0LL; 
*( _OWORD *)&e->insns = 0LL; 
*( _OWORD *)&e->src = 0LL; 
*( _OWORD *)&e->pred_next = 0LL; 
rtx *v36; // rcx 
rtx *reg_loc; // rax 
reg_loc = done_renames->reg_loc; 
v39 = *reg_loc; 
if ( *reg_loc != done_renames->old_reg ) 
*reg_loc = done_renames->new_reg; 
v36 = ( rtx *)( ( ( *v39 >> 13) & 0x7F8) + 472 * v40 + 10534240); 
v36 = &ssa_rename_to_pseudo[( unsigned int)( v40 - 53)]; 
*v36 = done_renames->prev_reg; 
if ( use == sibcall_use_normal_0 ) 
else if ( use == sibcall_use_sibcall_0 ) 
if ( use != sibcall_use_tail_recursion_0 ) 
( machine_mode)mode, 
LOBYTE( v28) = replace_oldest_value_addr( v16, INDEX_REGS, ( machine_mode)mode, insn, fld); 
LOBYTE( v9) = replace_oldest_value_addr( ( rtx *)&v24[2 * v25], a2, ( machine_mode)mode, insn, vd) | ( unsigned __int8)v9 & 1; 
v27 = replace_oldest_value_addr( ( rtx *)( v26 + *( ( _QWORD *)v7 + v20)), a2, ( machine_mode)mode, insn, vd); 
v23 = replace_oldest_value_addr( ( rtx *)v7 + v20, a2, ( machine_mode)mode, insn, vd); 
rtx *fld; // r15 
rtx v9; // rax 
rtx v10; // rax 
fld = loc; 
v9 = eliminate_regs( rtx, mem_mode, usage); 
rtx = v9; 
if ( v9 == *fld ) 
if ( v9 == *fld ) 
v10 = reg_equiv_constant[rtuint]; 
if ( !v10 ) 
v10 = reg_equiv_mem[rtuint]; 
if ( !v10 ) 
v10 = gen_rtx_MEM( ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v11); 
v10 = gen_rtx_MEM( ( machine_mode)*( ( unsigned __int8 *)rtx + 2), v11); 
v10 = cfun->emit->x_regno_reg_rtx[rtuint]; 
rtx v16; // rsi 
v16 = reg_map[v15]; 
if ( !v16 || *( _WORD *)v16 != 63 ) 
if ( !v16 || *( _WORD *)v16 != 63 ) 
return simplify_gen_subreg( ( machine_mode)BYTE2( v8), v16, ( machine_mode)BYTE2( v14), *( _DWORD *)&x[1]); 
return simplify_gen_subreg( ( machine_mode)BYTE2( v8), v16, ( machine_mode)BYTE2( v14), *( _DWORD *)&x[1]); 
return simplify_gen_subreg( ( machine_mode)BYTE2( v8), v16, ( machine_mode)BYTE2( v14), *( _DWORD *)&x[1]); 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
if ( !count_error_warning_message ) 
count_error_warning_message = 1; 
v6 = *( _OWORD *)&diagnostic_buffer->state.prefix; 
v7 = *( _OWORD *)&diagnostic_buffer->state.indent_skip; 
v8 = *( _OWORD *)&diagnostic_buffer->state.cursor; 
v9 = *( _OWORD *)diagnostic_buffer->state.diagnostic_count; 
v10 = *( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4]; 
*( _OWORD *)&diagnostic_buffer->state.cursor = *( _OWORD *)&dc->message; 
*( _OWORD *)&diagnostic_buffer->state.cursor = *( _OWORD *)&dc->message; 
rtx last_insn; // rax 
rtx j; // rdi 
rtx v31; // rbx 
last_insn = get_last_insn( ); 
if ( !last_insn ) 
j = 0LL; 
v31 = 0LL; 
if ( *( _WORD *)last_insn != 32 ) 
if ( *( _WORD *)last_insn == 37 ) 
if ( last_insn[2].fld[0].rtint == -89 ) 
j = last_insn; 
j = last_insn; 
if ( last_insn->fld[0].rtint == epilogue->data.i[v51] ) 
while ( ( sch_istable[v7] & 4) != 0 ); 
sprintf( v10, &off_6474F4[1], v7); 
v8 = *( const char *const *)( ( char *)resolve_unique_section_prefixes[v4] 
timevar_push( TV_REST_OF_COMPILATION_0); 
timevar_push( TV_INTEGRATION_0); 
timevar_pop( TV_INTEGRATION_0); 
timevar_push( TV_DUMP_0); 
timevar_pop( TV_DUMP_0); 
timevar_push( TV_INTEGRATION_0); 
timevar_pop( TV_INTEGRATION_0); 
timevar_push( TV_JUMP_0); 
timevar_push( TV_DUMP_0); 
timevar_pop( TV_DUMP_0); 
timevar_pop( TV_JUMP_0); 
timevar_push( TV_JUMP_0); 
timevar_push( TV_DUMP_0); 
timevar_pop( TV_DUMP_0); 
timevar_pop( TV_JUMP_0); 
timevar_push( TV_JUMP_0); 
timevar_pop( TV_JUMP_0); 
timevar_push( TV_DUMP_0); 
v52 = TV_DUMP_0; 
timevar_push( TV_VARCONST_0); 
timevar_pop( TV_VARCONST_0); 
timevar_push( TV_SYMOUT_0); 
timevar_pop( TV_SYMOUT_0); 
timevar_push( TV_SYMOUT_0); 
timevar_pop( TV_SYMOUT_0); 
*( _OWORD *)sequence_result = 0LL; 
*( _OWORD *)&sequence_result[2] = 0LL; 
rtx v4; // rax 
rtx line_note; // rbp 
rtx v10; // rax 
v4 = head; 
while ( *( _WORD *)v4 != 37 || v4[2].fld[0].rtint <= 0 ) 
while ( *( _WORD *)v4 != 37 || v4[2].fld[0].rtint <= 0 ) 
v4 = ( rtx)v4[1]; 
if ( !v4 ) 
v4 = 0LL; 
v4 = v2; 
line_note = v4; 
line_note = v4; 
line_note = h_i_d[rtint].line_note; 
if ( !line_note || line_note == v4 || v4 && line_note[2].fld[0].rtint == v4[2].fld[0].rtint && line_note[2] == v4[2] ) 
if ( !line_note || line_note == v4 || v4 && line_note[2].fld[0].rtint == v4[2].fld[0].rtint && line_note[2] == v4[2] ) 
if ( !line_note || line_note == v4 || v4 && line_note[2].fld[0].rtint == v4[2].fld[0].rtint && line_note[2] == v4[2] ) 
( rtx_code)*( _WORD *)comparison, 
v6 = mode_class_0[v5]; 
rtx v3; // r9 
rtx v4; // rcx 
v3 = head; 
if ( *( _WORD *)v3 != 37 ) 
v4 = v3; 
v4 = v3; 
v5 = (  struct rtx_def *)v3[1]; 
v6 = v3; 
v4 = v6[1].fld[0].rtx; 
if ( v4 == rtx ) 
v6 = v4; 
if ( *( _WORD *)v4 != 37 ) 
v5[1].fld[0].rtwint = ( __int64)v4; 
if ( v4 ) 
*( _QWORD *)&v4[1] = v5; 
if ( v4 != rtx ) 
v4 = rtx; 
rtx v3; // r9 
rtx v5; // r11 
rtx v6; // rdx 
rtx v8; // rcx 
v3 = 0LL; 
v5 = head; 
if ( *( _WORD *)v5 != 37 ) 
v6 = v5; 
v6 = v5; 
v7 = ( __int64)v5[1]; 
v8 = v5; 
v8 = v5; 
v6 = v8[1].fld[0].rtx; 
v6 = v8[1].fld[0].rtx; 
*( _QWORD *)( v7 + 24) = v6; 
rtx v16; // r14 
if ( mode_class_0[v15] == MODE_INT ) 
v16 = v14->int_cst.rtl; 
if ( v16 ) 
if ( integer_zerop( v16[6].fld[0].rttree) ) 
if ( !compare_tree_int( *( tree *)&v16[7], v12->string.length - 1LL) ) 
v12 = mode_class_0[BYTE2( v9)]; 
v5 = cselib_lookup( x, ( machine_mode)BYTE2( v4), 0); 
v7 = cselib_lookup( y, ( machine_mode)BYTE2( v6), 0); 
v5 = gen_lowpart_for_combine( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)rtx), x); 
v7 = gen_lowpart_for_combine( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)v6), y); 
if ( *( _OWORD *)&x == 0LL ) 
rtx real_insn; // rbx 
real_insn = next_real_insn( rtx); 
return real_insn == next_real_insn( v2->fld[0].rtx); 
( machine_mode)*( unsigned __int8 *)( v16.rtwint + 2), 
( machine_mode)BYTE2( v4)); 
*( _OWORD *)( object_base + 40) = 0LL; 
*( _OWORD *)( object_base + 24) = 0LL; 
*( _OWORD *)( object_base + 8) = 0LL; 
*( _OWORD *)( object_base + 72) = 0LL; 
*( _OWORD *)( object_base + 120) = 0LL; 
*( _OWORD *)( object_base + 104) = 0LL; 
*( _OWORD *)( object_base + 88) = 0LL; 
*( _OWORD *)( object_base + 56) = 0LL; 
pfile->buffer = ( cpp_buffer_0 *)object_base; 
rtx rtl; // r12 
rtx v17; // rax 
safe_from_p_save_expr_list = 0LL; 
for ( i = safe_from_p_save_expr_list; i; i = i->common.chain ) 
p_int_cst = &exp->int_cst.int_cst; 
exp = ( tree)p_int_cst->low; 
rtl = exp->decl.rtl; 
if ( !rtl ) 
if ( *( _WORD *)rtl == 63 ) 
rtl = rtl->fld[0].rtx; 
rtl = rtl->fld[0].rtx; 
if ( *( _WORD *)rtl == 61 ) 
if ( rtl->fld[0].rtint < 0x35u ) 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641440); 
v14 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v15 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
v26 = ( ( unsigned int)( mode_class_0[BYTE2( v25)] - 5) < 2) + 1; 
v15 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
v14 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641440); 
rtx v4; // rax 
rtx *elem; // rbp 
rtx v15; // r13 
rtx v16; // rax 
elem = v9->elem; 
*elem = rtl; 
v15 = gen_realpart( ( machine_mode)*( unsigned __int8 *)( rtl->fld[0].rtwint + 2), rtl); 
v15 = gen_realpart( ( machine_mode)*( unsigned __int8 *)( rtl->fld[0].rtwint + 2), rtl); 
v16 = gen_imagpart( ( machine_mode)*( ( unsigned __int8 *)v15 + 2), rtl); 
v16 = gen_imagpart( ( machine_mode)*( ( unsigned __int8 *)v15 + 2), rtl); 
v16 = gen_imagpart( ( machine_mode)*( ( unsigned __int8 *)v15 + 2), rtl); 
if ( *( _WORD *)v15 == 61 ) 
parmdecl_map[v15->fld[0].rtuint] = arguments; 
rtx v5; // rdx 
if ( *( _WORD *)head != 37 || ( v5 = head, head[2].fld[0].rtint <= 0) ) 
v5 = v4; 
v4 = v5; 
( save_level)( x_block_stack->next == 0LL), 
rtx v6; // r15 
rtx v9; // r14 
rtx v10; // rbp 
rtx nonnote_insn; // rax 
rtx v13; // r12 
rtx v14; // rax 
rtx v18; // rax 
rtx v27; // rax 
rtx scan_start; // rax 
rtx v51; // rbp 
rtx *v67; // rdi 
rtx *v68; // rdi 
rtx ops[16]; // [rsp+10h] [rbp-EC8h] BYREF 
v5 = ix86_memory_move_cost( ( machine_mode)*( unsigned __int8 *)( v3->fld[0].rtwint + 2), GENERAL_REGS, 1); 
costs_0[v7].mem_cost -= frequency * v5; 
&& recog_data_0.n_operands >= 3 
&& *recog_data_0.constraints[1] == 48 
&& !*( ( _BYTE *)recog_data_0.constraints[1] + 1) ) 
if ( ( v8 = *( _DWORD *)recog_data_0.operand[1], ( unsigned __int16)( *( _DWORD *)recog_data_0.operand[1] - 54) <= 0xEu) 
if ( ( v8 = *( _DWORD *)recog_data_0.operand[1], ( unsigned __int16)( *( _DWORD *)recog_data_0.operand[1] - 54) <= 0xEu) 
&& ( v9 = 24599, _bittest( &v9, *( _DWORD *)recog_data_0.operand[1] - 54)) 
if ( !rtx_equal_p( recog_data_0.operand[0], recog_data_0.operand[1]) 
scan_rtx_address( insn, ( rtx *)v9->fld, GENERAL_REGS, action, ( machine_mode)type); 
scan_rtx( insna, ( rtx *)v9->fld, a3, action, ( op_type)( 2 * ( v7 != OP_IN)), earlyclobber); 
v15 = ( ( unsigned int)( mode_class_0[v13] - 5) < 2) + 1; 
v29 = ( ( unsigned int)( *( const mode_class *)( ( char *)mode_class_0 + ( ( *( _DWORD *)v26 >> 14) & 0x3FC)) - 5) < 2) + 1; 
v14 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v13] - 5) < 2) + 1; 
v33 = ( v14 - 1 < 0) ^ __OFADD__( -1, v14) | ( v14 == 1); 
v33 = ( v14 - 1 < 0) ^ __OFADD__( -1, v14) | ( v14 == 1); 
cselib_lookup( rtwint->fld[0].rtx, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 1); 
rtx v7; // r12 
rtx pending_read_insns; // rbp 
rtx *v9; // rbx 
rtx v10; // rbx 
rtx pending_write_insns; // rbp 
rtx *v12; // rbx 
rtx v13; // rbx 
rtx i; // rbx 
rtx v18; // rax 
rtx v19; // rbx 
deps_0 *v12; // rbp 
deps_0 *v12; // rbp 
v12 = deps; 
reg_last = v12->reg_last; 
add_dependence( i22, n->fld[0].rtx, ( reg_note)0); 
add_dependence( i22, ii->fld[0].rtx, ( reg_note)0); 
rtx v15; // rbx 
rtx *v41; // rdx 
rtx v44; // r15 
rtx v55; // r12 
rtx v56; // rcx 
rtx v61; // r15 
rtx v62; // rbp 
rtx v64; // rcx 
rtx v66; // rbx 
deps_0 *v14; // rax 
deps_0 *v14; // rax 
deps_0 *v19; // rax 
deps_0 *v19; // rax 
deps_0 *v51; // r14 
deps_0 *v51; // r14 
deps_0 *v67; // rcx 
deps_0 *v67; // rcx 
deps_0 *v189; // r14 
deps_0 *v189; // r14 
deps_0 *v202; // r9 
deps_0 *v202; // r9 
rtx headp; // [rsp+8h] [rbp-140h] BYREF 
deps_0 *v304; // [rsp+40h] [rbp-108h] 
deps_0 *v304; // [rsp+40h] [rbp-108h] 
deps_0 deps; // [rsp+A0h] [rbp-A8h] BYREF 
rtx tailp[7]; // [rsp+110h] [rbp-38h] BYREF 
if ( ( v5 & 0x4000) == 0 && ( unsigned int)debug_info_level_0 >= DINFO_LEVEL_NORMAL ) 
rtx v4; // rbx 
v4 = rtx; 
if ( active_insn_p( v4) && ( **( _DWORD **)&v4[2] & 0xFFFE) != 44 ) 
if ( active_insn_p( v4) && ( **( _DWORD **)&v4[2] & 0xFFFE) != 44 ) 
insn_scopes->data.l[v4->fld[0].rtint] = v3; 
else if ( *( _WORD *)v4 == 37 ) 
rtint = v4[2].fld[0].rtint; 
v3 = ( __int64)v4[2]; 
delete_insn( v4); 
v5 = *( tree_node **)( low + 128); 
if ( v5 == ( tree_node *)*( &global_trees + 24) ) 
if ( initial != ( tree_node *)global_trees ) 
if ( initial != ( tree_node *)global_trees ) 
if ( constructor_range_stack_0 ) 
while ( constructor_stack_0->implicit ) 
memset( dc, 0, sizeof( diagnostic_context_0)); 
*( _OWORD *)&dc->begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&dc->begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
if ( v13 || constructor_range_stack_0 ) 
v14->prev = constructor_range_stack_0; 
v14->stack = constructor_stack_0; 
if ( constructor_range_stack_0 ) 
constructor_range_stack_0->next = v14; 
constructor_range_stack_0 = v14; 
if ( constructor_range_stack_0 ) 
v2->prev = constructor_range_stack_0; 
v2->stack = constructor_stack_0; 
if ( constructor_range_stack_0 ) 
constructor_range_stack_0->next = v2; 
constructor_range_stack_0 = v2; 
_OWORD *v16; // rax 
_OWORD *v16; // rax 
v16 = ggc_alloc( 0x28uLL); 
*slot = v16; 
*( ( _QWORD *)v16 + 4) = v20; 
v17 = *( _OWORD *)arg0; 
v16[1] = v19; 
*v16 = v17; 
_OWORD *v15; // rax 
_OWORD *v15; // rax 
v15 = ggc_alloc( 0x28uLL); 
*slot = v15; 
*( ( _QWORD *)v15 + 4) = v19; 
v15[1] = v18; 
*v15 = v16; 
_OWORD *v42; // rax 
_OWORD *v42; // rax 
v7 = *( tree_node **)( v6 + 8); 
if ( lang_hooks_0.honor_readonly ) 
v47 = ( tree_node *)*( &global_trees + 15); 
v42 = ggc_alloc( 0x28uLL); 
*slot = v42; 
_OWORD *v16; // rax 
_OWORD *v16; // rax 
v16 = ggc_alloc( 0x28uLL); 
*slot = v16; 
*( ( _QWORD *)v16 + 4) = v20; 
v17 = *( _OWORD *)arg0; 
v16[1] = v19; 
*v16 = v17; 
_OWORD *v16; // rax 
_OWORD *v16; // rax 
v16 = ggc_alloc( 0x28uLL); 
*slot = v16; 
*( ( _QWORD *)v16 + 4) = v20; 
v17 = *( _OWORD *)arg0; 
v16[1] = v19; 
*v16 = v17; 
v6 = ( tree_node *)*( &global_trees + 17); 
if ( !in || !*( ( _BYTE *)in + 2) || ix86_hard_regno_mode_ok( v6, ( machine_mode)*( ( unsigned __int8 *)in + 2)) ) 
if ( !out || ix86_hard_regno_mode_ok( v6, ( machine_mode)*( ( unsigned __int8 *)out + 2)) ) 
reg_note = gen_rtx_fmt_ee( EXPR_LIST, ( machine_mode)kind, datum, insn[3].fld[0].rtx); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641400); 
v5 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641410); 
v6 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v7 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641430); 
v11 = ( ( unsigned int)( mode_class_0[v10] - 5) < 2) + 1; 
rtx v14; // rax 
v14 = sge_plus_constant( x->fld[0].rtx, v2); 
v14 = sge_plus_constant( v6, v2); 
v12 = v14; 
v13 = ( tree_node *)p_chain[3]; 
while ( ( tree_node *)v14[3] != v13 ) 
v16 = *( _OWORD *)&orig[v14 + 1].fld[0].rtwint; 
*( _OWORD *)&result[v14].fld[0].rtwint = *( _OWORD *)&orig[v14].fld[0].rtwint; 
*( _OWORD *)&result[v14].fld[0].rtwint = *( _OWORD *)&orig[v14].fld[0].rtwint; 
*( _OWORD *)&result[v14 + 1].fld[0].rtwint = v16; 
v17 = *( _OWORD *)&orig[v14 + 3].fld[0].rtwint; 
*( _OWORD *)&result[v14 + 2].fld[0].rtwint = *( _OWORD *)&orig[v14 + 2].fld[0].rtwint; 
*( _OWORD *)&result[v14 + 2].fld[0].rtwint = *( _OWORD *)&orig[v14 + 2].fld[0].rtwint; 
*( _OWORD *)&result[v14 + 3].fld[0].rtwint = v17; 
v18 = *( _OWORD *)&orig[v14 + 5].fld[0].rtwint; 
rtx v62; // rbx 
rtx v84; // [rsp+18h] [rbp-C0h] 
v84 = first; 
if ( v84 ) 
rtx = v84; 
v62 = v84; 
v8 = ( tree_node *)*( &global_trees + 5); 
v8 = ( tree_node *)*( &global_trees + 4); 
v8 = ( tree_node *)*( &global_trees + 3); 
v8 = ( tree_node *)*( &global_trees + 2); 
v8 = ( tree_node *)*( &global_trees + 1); 
if ( section_name == ( tree_node *)*( &global_trees + 10) ) 
if ( section_name == ( tree_node *)*( &global_trees + 9) ) 
if ( section_name == ( tree_node *)*( &global_trees + 8) ) 
if ( section_name == ( tree_node *)*( &global_trees + 7) ) 
if ( section_name == ( tree_node *)*( &global_trees + 6) ) 
p_int_cst = &low->int_cst.int_cst; 
low = ( tree)p_int_cst->low; 
p_int_cst = &low->int_cst.int_cst; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&low->block.subblocks; 
p_int_cst = ( tree_int_cst::$A887AD9C3C6C8CC7716950D571F57C9D *)&low->block.subblocks; 
if ( section_name == ( tree_node *)*( &global_trees + 24) ) 
v15 = simplify_and_const_int( 0LL, ( machine_mode)BYTE2( v12), v5->fld[0].rtx, v10); 
v16 = simplify_and_const_int( 0LL, ( machine_mode)*( ( unsigned __int8 *)v5 + 2), *( rtx *)&v5[1], v10); 
rtx v9; // r12 
rtx v13; // rax 
rtx v14; // r12 
rtx v17; // r13 
rtx v20; // rax 
rtx result; // rax 
rtx v4; // rbx 
rtx v38; // r15 
rtx v40; // r12 
rtx compound_operation; // rbx 
rtx v48; // r15 
rtx v52; // rax 
rtx v53; // rbx 
rtx v24; // rax 
rtx v25; // r12 
rtx v27; // rax 
rtx *v40; // rbp 
rtx result; // rax 
rtx v49; // rax 
rtx v51; // rax 
rtx v52; // rax 
rtx *v64; // rbp 
rtx v89; // [rsp+60h] [rbp-C8h] BYREF 
rtx *v96; // [rsp+F0h] [rbp-38h] 
v89 = op1; 
v25 = &v89; 
v29 = &v89; 
v96 = v29; 
v29 = v96 + 2; 
v27 = &v89; 
*( _OWORD *)&base[16 * v61] = *( ( _OWORD *)v27 - 1); 
rtx pool_constant; // r13 
rtx v27; // rax 
rtx *v61; // rcx 
rtx *v68; // rcx 
v4 = simplify_unary_operation( ( rtx_code)v5, outermode, v3, v21); 
return gen_rtx_fmt_e( ( rtx_code)v5, outermode, v3); 
v4 = simplify_relational_operation( ( rtx_code)v5, v23, v24, v25); 
v27 = swap_condition( ( rtx_code)v5); 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
( rtx_code)( unsigned __int16)*( _DWORD *)x, 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2)); 
( rtx_code)( unsigned __int16)*( _DWORD *)x, 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
return simplify_relational_operation( ( rtx_code)( unsigned __int16)*( _DWORD *)x, v5, v4.rtx, ( rtx)v6); 
return simplify_binary_operation( ( rtx_code)v1, v2, v8, rtx); 
( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)x), 
rtx v7; // rbx 
rtx v25; // rax 
rtx v61; // rax 
rtx v71; // rax 
v11 = simplify_subreg( outermode, v16.rtx, ( machine_mode)v17, v20); 
v11 = simplify_subreg( outermode, *v28, ( machine_mode)*( ( unsigned __int8 *)*v28 + 2), byte % v26); 
if ( v12 == v35 && v12 < v9 && mode_class_0[outermode] == MODE_INT ) 
if ( mode_class_0[outermode] != MODE_INT ) 
decode_rtx_const( ( machine_mode)*( ( unsigned __int8 *)x + 2), x->fld[0].rtx, &value); 
decode_rtx_const( ( machine_mode)*( ( unsigned __int8 *)v1 + 2), *( rtx *)&v1[1], &v3); 
v17 = mode_class_0[mode]; 
v19 = mode_class_0[mode]; 
v24 = simplify_relational_operation( ( rtx_code)*( _WORD *)v8, op0_mode, v21.rtx, ( rtx)v23); 
rtx result; // rax 
result = 0LL; 
result = gen_rtx_CONST_INT( VOIDmode, v32); 
return result; 
if ( !size_int_type_wide_size_htab ) 
size_int_type_wide_size_htab = htab_create( 0x400uLL, size_htab_hash, size_htab_eq, 0LL); 
ggc_add_deletable_htab( size_int_type_wide_size_htab, 0LL, 0LL); 
size_int_type_wide_new_const = make_node( INTEGER_CST); 
ggc_add_tree_root( &size_int_type_wide_new_const, 1); 
v3 = size_int_type_wide_new_const; 
*( _OWORD *)&size_int_type_wide_new_const->block.vars = number; 
*( _OWORD *)&size_int_type_wide_new_const->block.vars = number; 
v5 = size_int_type_wide_new_const; 
*( ( _DWORD *)&size_int_type_wide_new_const->common + 4) = *( ( _DWORD *)&size_int_type_wide_new_const->common + 4) & 0xFFF3FFFF | ( ( v4 & 1) << 18) | ( ( v4 & 1) << 19); 
*( ( _DWORD *)&size_int_type_wide_new_const->common + 4) = *( ( _DWORD *)&size_int_type_wide_new_const->common + 4) & 0xFFF3FFFF | ( ( v4 & 1) << 18) | ( ( v4 & 1) << 19); 
slot = htab_find_slot( size_int_type_wide_size_htab, v5, INSERT); 
v7 = ( tree_node *)*slot; 
v7 = size_int_type_wide_new_const; 
*slot = size_int_type_wide_new_const; 
size_int_type_wide_new_const = make_node( INTEGER_CST); 
if ( !cpp_trigraph_map[v7] ) 
v9 = cpp_trigraph_map[cur[1]]; 
v14 = sch_istable[cur[v12++]]; 
cpp_trigraph_map[v7]); 
v3 = *( _OWORD *)&diagnostic_buffer->state.indent_skip; 
v4 = *( _OWORD *)&diagnostic_buffer->state.cursor; 
v5 = *( _OWORD *)diagnostic_buffer->state.diagnostic_count; 
v17 = *( _OWORD *)&diagnostic_buffer->state.prefix; 
v21 = *( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4]; 
diagnostic_buffer->state.format_args = ( va_list_0 *)va; 
*( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4] = v21; 
*( _OWORD *)v12->state.diagnostic_count = v20; 
v19 = *( _OWORD *)&v9[v18]; 
v20 = *( _OWORD *)&v9[v18 + 16]; 
v21 = *( _OWORD *)&base[v18 + 16]; 
*( _OWORD *)&v9[v18] = *( _OWORD *)&base[v18]; 
*( _OWORD *)&v9[v18] = *( _OWORD *)&base[v18]; 
*( _OWORD *)&v9[v18 + 16] = v21; 
*( _OWORD *)&base[v18] = v19; 
*( _OWORD *)&base[v18 + 16] = v20; 
v22 = *( _OWORD *)&v9[v18 + 32]; 
v6 = ( ( unsigned int)( mode_class_0[( unsigned __int8)BYTE2( *( _DWORD *)cfun->emit->x_regno_reg_rtx[v3])] - 5) < 2) 
*( _OWORD *)&result->left = 0LL; 
rtx v61; // rax 
rtx v134; // rax 
rtx v149; // rax 
recog_data_0.operand[0] = v5.rtx; 
recog_data_0.operand[1] = ( rtx)v53; 
return gen_split_1133( recog_data_0.operand); 
recog_data_0.operand[1] = ( rtx)v8; 
return gen_split_1135( recog_data_0.operand); 
recog_data_0.operand[0] = v5.rtx; 
recog_data_0.operand[0] = ( rtx)v5; 
recog_data_0.operand[1] = ( rtx)v7; 
return gen_split_881( recog_data_0.operand); 
return gen_split_881( recog_data_0.operand); 
return gen_split_881( recog_data_0.operand); 
v154 = *( unsigned __int16 *)recog_data_0.operand[0]; 
rtuint = recog_data_0.operand[0]->fld[0].rtuint; 
else if ( v154 != 61 || v12 || recog_data_0.operand[0]->fld[0].rtint > 3u ) 
if ( !reg_overlap_mentioned_p( recog_data_0.operand[0], ( rtx)v7) ) 
return gen_split_882( recog_data_0.operand); 
recog_data_0.operand[1] = ( rtx)v7; 
v162 = true_regnum( recog_data_0.operand[0]); 
if ( v162 != true_regnum( recog_data_0.operand[1]) ) 
return gen_split_883( recog_data_0.operand); 
recog_data_0.operand[0] = ( rtx)v5; 
recog_data_0.operand[1] = ( rtx)v16; 
return gen_split_879( recog_data_0.operand); 
recog_data_0.operand[1] = ( rtx)v16; 
return gen_split_885( recog_data_0.operand); 
recog_data_0.operand[0] = ( rtx)v6; 
recog_data_0.operand[1] = v85; 
recog_data_0.operand[2] = v86; 
recog_data_0.operand[3] = v88; 
if ( rtx_equal_p( *( rtx *)( v89 + 8), recog_data_0.operand[1]) ) 
v90 = rtx_equal_p( *( rtx *)( v89 + 16), recog_data_0.operand[2]); 
return gen_split_1003( recog_data_0.operand); 
recog_data_0.operand[1] = v10; 
recog_data_0.operand[2] = v11; 
recog_data_0.operand[3] = v14; 
if ( rtx_equal_p( *( rtx *)( v15 + 8), recog_data_0.operand[1]) ) 
v16 = rtx_equal_p( *( rtx *)( v15 + 16), recog_data_0.operand[2]); 
return gen_split_1005( recog_data_0.operand); 
recog_data_0.operand[0] = ( rtx)v6; 
recog_data_0.operand[1] = v55; 
recog_data_0.operand[2] = v60; 
if ( dead_or_set_p( insn, recog_data_0.operand[1]) 
&& !reg_mentioned_p( recog_data_0.operand[1], recog_data_0.operand[0]) ) 
rtx v8; // rax 
v8 = split_insn( v3); 
if ( v8 ) 
if ( *( _WORD *)v8 == 35 ) 
v8 = ( rtx)v8[1]; 
while ( *( _WORD *)v8 == 35 ); 
v6 = v8; 
recog_data_0.insn = 0LL; 
recog_data_0.operand[0] = ( rtx)v5; 
return gen_split_1179( recog_data_0.operand); 
recog_data_0.operand[0] = ( rtx)v5; 
return gen_split_1178( recog_data_0.operand); 
v20 = ( tree_node *)high; 
v20 = ( tree_node *)low; 
rtx v5; // rdx 
rtx v6; // rax 
v5 = *startp; 
v6 = v5; 
v6 = v5; 
v5 = v5[1].fld[0].rtx; 
v5 = v5[1].fld[0].rtx; 
if ( *( _WORD *)v6 == 37 && ( unsigned int)( v6[2].fld[0].rtint + 98) <= 5 ) 
if ( *( _WORD *)v6 == 37 && ( unsigned int)( v6[2].fld[0].rtint + 98) <= 5 ) 
if ( v6 == v2 ) 
v2 = v5; 
v7 = ( __int64)v6[1]; 
*( _QWORD *)&v6[1] = v8; 
v6[1].fld[0].rtwint = ( __int64)v2; 
*( _QWORD *)( v8 + 24) = v6; 
*( _QWORD *)( v6[1].fld[0].rtwint + 16) = v6; 
rtx insn; // rdi 
rtx v39; // rbx 
rtx v50; // rax 
rtx v51; // rbx 
edge_info_0 = ( edge *)xmalloc( 8LL * edges->num_edges); 
edge_info_0[edge_index] = succ->succ_next; 
v11 = edge_info_0; 
v34 = edge_info_0; 
insn = uses->ref->insn; 
&& *( _WORD *)insn == 32 
if ( insn 
&& ( v19 = ( __int64)insn[2], *( _WORD *)v19 == 47) 
v20 = basic_block_for_insn->data.bb[insn->fld[0].rtint]; 
visit_phi_node( insn, v20); 
rtx v41; // rcx 
rtx v69; // rax 
rtx v79; // rbp 
rtx v82; // rax 
rtx v83; // rbx 
rtx insn_after_basic_block_note; // rax 
rtx v88; // rbx 
rtx v91; // rax 
rtx mm; // rax 
v17 = ( tree_node *)ggc_alloc( 0x28uLL); 
*( _OWORD *)&v17->common.chain = 0LL; 
v5 = ( tree_node *)ggc_alloc( 0x28uLL); 
*( _OWORD *)&v5->common.chain = 0LL; 
v3 = mode_class_0[v2]; 
*( _OWORD *)&v25->parm_flag = 0LL; 
*( _OWORD *)&v25->this_block = 0LL; 
*( _OWORD *)&v25->shadowed = 0LL; 
*( _OWORD *)&v25->names = 0LL; 
v4->constructor_stack = constructor_stack_0; 
v4->constructor_range_stack = constructor_range_stack_0; 
v4->spelling = spelling_0; 
v4->next = initializer_stack_0; 
initializer_stack_0 = v4; 
constructor_stack_0 = 0LL; 
constructor_range_stack_0 = 0LL; 
spelling_0 = 0LL; 
spelling_0 = v12 + 1; 
*( _OWORD *)&v0->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&v0->first = *( _OWORD *)&emit->x_first_insn; 
*( _OWORD *)&emit->x_first_insn = 0LL; 
return lang_hooks_0.staticp( arg); 
set_diagnostic_context( &v3, msgid, ( va_list_0 *)va, ( const char *)input_filename, lineno, 1); 
v11 = ( tree_node *)*( &global_trees + 15); 
v12 = expand_expr( valist, 0LL, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), EXPAND_NORMAL); 
tree v8; // rax 
v8 = build( MODIFY_EXPR, type, valist, tree); 
*( ( _BYTE *)&v8->block.common + 17) |= 1u; 
expand_expr( v8, const_int_rtx[64], VOIDmode, EXPAND_NORMAL); 
rtx v15; // r12 
rtx v22; // rax 
rtx v25; // rax 
rtx v47; // rax 
rtx v17; // rax 
v17 = adjust_automodify_address_1( to, v8, data->to_addr, data->offset, 1); 
v17 = adjust_address_1( to, v8, data->offset, 1, 1); 
v18 = v17; 
mode_alignment = get_mode_alignment( ( machine_mode)BYTE2( v12)); 
rtx v40; // rdx 
rtx k; // rbx 
rtx m; // rbx 
rtx v50; // rax 
rtx v62; // r13 
if ( !direct_store[mode] && ( unsigned int)( mode_class_0[mode] - 5) > 1 ) 
v22 = convert_modes( mode, ( machine_mode)v25, v22, 1); 
v41 = immed_double_const( v44, v46, ( machine_mode)( unsigned __int8)v24); 
v33 = gen_lowpart( ( machine_mode)( unsigned __int8)v24, value); 
v33 = convert_to_mode( ( machine_mode)( unsigned __int8)v24, value, 1); 
v6 = ( tree_node *)v5; 
if ( *( ( _BYTE *)&type->block.common + 16) == 18 && !type->int_cst.rtl && v6 != ( tree_node *)global_trees ) 
v8 = *( tree_node **)( v7->int_cst.int_cst.low + 32); 
v7 = ( tree_node *)p_chain[4]; 
v7 = ( tree_node *)p_chain[4]; 
v24 = *( tree_node **)( v22[4] + 128LL); 
rtx fixed_bit_field; // rax 
rtx v32; // [rsp+20h] [rbp-48h] 
v32 = v13; 
fixed_bit_field = gen_rtx_CONST_INT( VOIDmode, ( ( unsigned __int64)v13->fld[0].rtwint >> v18) & ~( -1LL << v26)); 
fixed_bit_field = extract_fixed_bit_field( *(short *)0xmode, v13, 0LL, v26, v18, 0LL, 1); 
v28 = fixed_bit_field; 
rtx = operand_sub*(short *)0xforce( rtx, v21, v19); 
v13 = v32; 
rtx v201; // rax 
rtx v208; // rax 
rtx v212; // rax 
rtx v213; // rax 
rtx v218; // rax 
rtx v223; // rax 
rtx v227; // rbp 
rtx *v228; // rax 
rtx v233; // rbp 
v9 = ( tree_node *)v7; 
v11 = ( tree_node *)high; 
if ( check_mode && !ix86_hard_regno_mode_ok( v5, ( machine_mode)( unsigned __int8)BYTE2( *rtwint)) ) 
( machine_mode)*( unsigned __int8 *)( v1.rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2)); 
v5 = ( ( unsigned int)( mode_class_0[xmode] - 5) < 2) + 1; 
v6 = ( ( unsigned int)( mode_class_0[ymode] - 5) < 2) + 1; 
return gen_rtx_fmt_e( CLOBBER, ( machine_mode)BYTE2( v9), const_int_rtx[64]); 
( machine_mode)BYTE2( v42), 
( machine_mode)*( unsigned __int8 *)( v8->fld[0].rtwint + 2)); 
( machine_mode)BYTE2( v42), 
( machine_mode)*( unsigned __int8 *)( v8->fld[0].rtwint + 2), 
rtx *v23; // r12 
rtx *v25; // rax 
rtx v34; // rax 
rtx v36; // rax 
rtx **v44; // r10 
rtx v46; // rcx 
rtx ( *v50)[59]; // rcx 
rtx *v52; // rdx 
rtx v57; // rcx 
rtx ( *v61)[59]; // rcx 
rtx *v62; // rdx 
rtx v67; // rcx 
rtx *v13; // rbp 
rtx v15; // r12 
rtx v22; // rax 
rtx v23; // rax 
rtx v37; // rbx 
rtx v44; // rax 
rtx *v50; // rsi 
rtx *v61; // rax 
rtx v8; // rax 
rtx v9; // rsi 
rtx v10; // rcx 
&& ( v10 = reg_equiv_constant[v5]) != 0LL ) 
rtx = v10; 
v9 = ( rtx)addr[1]; 
|| ( v9 = reg_equiv_constant[rtuint]) == 0LL ) 
v8 = subst_indexed_address( addr->fld[0].rtx); 
if ( v8 != rtx ) 
rtx = v8; 
v9 = v3; 
v9 = subst_indexed_address( v3); 
if ( v9 == v3 ) 
if ( *( _WORD *)v9 == 75 ) 
fld = (  struct rtx_def **)v9->fld; 
v10 = _mm_add_epi32( _mm_load_si128( ( const __m128i *)&xmm*(short *)0x7058E0), v9); 
v11 = _mm_add_epi32( _mm_load_si128( ( const __m128i *)&xmm*(short *)0x7058F0), v9); 
v12 = _mm_add_epi32( _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705900), v9); 
v13 = _mm_add_epi32( v9, ( __m128i)xmm*(short *)0x705910); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705920); 
v17 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x705930); 
v24 = _mm_add_epi64( _mm_shuffle_epi32( ( __m128i)v7, 68), ( __m128i)xmm*(short *)0x675E90); 
v26 = _mm_shuffle_pd( ( __m128d)reg_set, ( __m128d)xmm*(short *)0x675E90, 2); 
v30 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E880); 
v31 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x64E870); 
v32 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
rtx v7; // rbx 
rtx v10; // rbx 
rtx regno_note; // rax 
rtx *v40; // r12 
rtx *v44; // rbx 
rtx v45; // rbp 
rtx v46; // rax 
rtx v67; // rax 
rtx *v78; // rbx 
*( _WORD *)pat = swap_condition( ( rtx_code)v2); 
rtx m; // rax 
rtx v42; // r14 
rtx v48; // rbp 
rtx v54; // rbp 
rtx v76; // rdi 
rtx v77; // rax 
rtx v78; // rax 
rtx v88; // r12 
rtx v94; // rax 
rtx v8; // rax 
v8 = delete_insn( rtx); 
v8 = rtx[1].fld[0].rtx; 
rtx = v8; 
v1 = stack_0; 
if ( &timevars[timevar] != stack_0->timevar ) 
stack_0 = stack_0->next; 
stack_0 = stack_0->next; 
v3 = stack_0; 
if ( stack_0 ) 
v4 = stack_0->timevar; 
v4->elapsed.user = ( float)( v2 - start_time.user) + stack_0->timevar->elapsed.user; 
v3 = stack_0; 
stack_0 = v5; 
rtx v11; // r14 
v11 = assign_stack_local_1( BLKmode, v9, 0, v10); 
node->int_cst.int_cst.high = ( __int64)v11; 
rtx = v11->fld[0].rtx; 
v13 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32)); 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), 
*( _OWORD *)&result->decl.common.type = 0LL; 
rtx base_term; // rax 
base_term = find_base_term( rtx); 
|| ( v26 = *( _DWORD *)base_term, ( _WORD)v26 != 67) && ( ( _WORD)v26 != 68 || ( v26 & 0x4000000) == 0) ) 
if ( !base_term 
if ( base_alias_check( rtx, v19, ( machine_mode)*( ( unsigned __int8 *)v4 + 2), mem_mode) ) 
( machine_mode)*( unsigned __int8 *)( x->fld[0].rtwint + 2), 
( machine_mode)*( ( unsigned __int8 *)x + 2)); 
rtx v13; // r13 
rtx v16; // rbp 
rtx *v38; // r9 
rtx v50; // rbp 
rtx v52; // r15 
rtx *v54; // r13 
rtx v60; // rax 
rtx v63; // rcx 
rtx v72; // rcx 
rtx v74; // rbp 
rtx v9; // rax 
rtx v12; // rax 
rtx v19; // [rsp+10h] [rbp-68h] 
rtx v21; // [rsp+20h] [rbp-58h] 
rtx x; // [rsp+28h] [rbp-50h] BYREF 
v19 = cfun->emit->x_regno_reg_rtx[regno]; 
v9 = ( rtx)rtx[2]; 
if ( *( _WORD *)v9 != 47 ) 
v9 = single_set_2( rtx, *( rtx *)&rtx[2]); 
if ( v9 ) 
v10.rtwint = ( __int64)v9->fld[0]; 
rtx v40; // r12 
rtx v41; // rbx 
rtx v48; // r15 
rtx v49; // rax 
rtx *v51; // rsi 
rtx reg_equal_equiv_note; // r15 
rtx v63; // rax 
rtx v88; // rbx 
rtx nonnote_insn; // rax 
v48 = pc_set( src->end); 
rtx *p_x_last_insn; // r12 
( machine_mode)*( unsigned __int8 *)( v24.rtwint + 2), 
( machine_mode)*( unsigned __int8 *)( v24.rtwint + 2), 
( machine_mode)v20, 
( machine_mode)v21, 
emit_cmp_and_jump_insns( v17, v23, GTU, 0LL, ( machine_mode)v20, 1, default_label); 
v17 = convert_modes( ( machine_mode)v25, VOIDmode, v17, 1); 
v28 = gen_rtx_fmt_ee( MULT, ( machine_mode)v26, v17, v27); 
v29 = gen_rtx_fmt_u00( LABEL_REF, ( machine_mode)( ( ( target_flags & 0x2000000 | 0x500000000uLL) - 1) >> 32), table_label); 
v30 = gen_rtx_fmt_ee( PLUS, ( machine_mode)v26, v28, v29); 
v31 = memory_address_noforce( ( machine_mode)( ( ( target_flags & 0x2000000) == 0 || ( _DWORD)flag_pic != 0) ^ 5), v30); 
v32 = gen_reg_rtx( ( machine_mode)( ( ( target_flags & 0x2000000) == 0 || ( _DWORD)flag_pic != 0) ^ 5)); 
v33 = gen_rtx_MEM( ( machine_mode)( ( ( target_flags & 0x2000000) == 0 || ( _DWORD)flag_pic != 0) ^ 5), v31); 
if ( *( _OWORD *)&args1 != 0LL ) 
elements = *( tree_node **)( v3 + 32); 
rtx v16; // rax 
rtx v18; // rax 
rtx rtwint; // rdx 
rtx *fld; // rcx 
rtx v52; // rbx 
rtx v97; // rax 
v3 = ( tree_node *)ggc_alloc( 0x28uLL); 
*( _OWORD *)&v3->common.chain = 0LL; 
if ( section_name == ( tree_node *)*( &global_trees + 5) ) 
if ( section_name == ( tree_node *)*( &global_trees + 4) ) 
if ( section_name == ( tree_node *)*( &global_trees + 3) ) 
if ( section_name == ( tree_node *)*( &global_trees + 2) ) 
if ( section_name == ( tree_node *)*( &global_trees + 1) ) 
v3 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v4] - 5) < 2) + 1; 
rtx address; // rcx 
address = temp_slot_from_address->address; 
if ( address ) 
if ( *( _WORD *)address != 3 ) 
address = gen_rtx_fmt_ee( EXPR_LIST, VOIDmode, temp_slot_from_address->address, 0LL); 
v11->address = address; 
v2 = gen_rtx_fmt_ee( EXPR_LIST, VOIDmode, v2, address); 
v6 = ( ( unsigned int)( mode_class_0[( unsigned __int8)v5] - 5) < 2) + 1; 
if ( v4 == ( tree_node *)global_trees ) 
v8 = ( change_t_0 *)xrealloc( changes, 32LL * v9); 
rtx v41; // rax 
rtx v42; // r14 
rtx *fld; // r15 
rtx *v45; // r8 
rtx *v47; // rbx 
rtx v51; // [rsp+10h] [rbp-68h] 
rtx *v56; // [rsp+40h] [rbp-38h] 
|| ( v51 = v4, v12 = to, ( ( v8 ^ v11) & 0xFFFFFF) == 0) && ( v22 = rtx_equal_p( v4, from), to = v12, v4 = v51, v22) ) 
if ( memory_address_p( ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)ref), rtwint) ) 
if ( in_section_0 != in_const ) 
in_section_0 = in_const; 
if ( in_section_0 != in_data ) 
if ( in_section_0 != in_const ) 
else if ( in_section_0 != in_data ) 
in_section_0 = in_data; 
v21 = *( _OWORD *)&args->gp_offset; 
if ( memchr( &unk_71F3A8, v16, 4uLL) ) 
while ( memchr( &unk_71F3A8, v16, 4uLL) ); 
v16 = *( _OWORD *)&diagnostic_buffer->state.diagnostic_count[4]; 
v2 = *( _OWORD *)&diagnostic_buffer->state.prefix; 
v3 = *( _OWORD *)&diagnostic_buffer->state.indent_skip; 
v4 = *( _OWORD *)&diagnostic_buffer->state.cursor; 
v15 = *( _OWORD *)diagnostic_buffer->state.diagnostic_count; 
*( _OWORD *)&diagnostic_buffer->state.prefix = 0LL; 
v1->state.format_args = ( va_list_0 *)va; 
*( _OWORD *)&v1->state.diagnostic_count[4] = v16; 
fatal_insn( "wrong insn in the fallthru edge", end, "cfgrtl.c", 1717, "verify_flow_info"); 
fatal_insn( "flow control insn inside a basic block", ( rtx)v31, "cfgrtl.c", 1829, "verify_flow_info"); 
fatal_insn( "insn outside basic block", insns, "cfgrtl.c", 1887, "verify_flow_info"); 
fatal_insn( "return not followed by barrier", insns, "cfgrtl.c", 1895, "verify_flow_info"); 
rtx reg; // rdi 
rtx v24; // rdi 
rtx v31; // rdi 
rtx const_value; // rdx 
rtx v47; // rcx 
rtx v56; // rdx 
rtx v57; // rcx 
rtx v68; // rcx 
rtx v69; // r8 
sprintf( &v8[v9], " %-33s", &unk_710117); 
v19 = statement_code_p( ( tree_code)v10); 
if ( ( _DWORD)v10 != 2 && !v19 && !lang_hooks_0.tree_inlining.tree_chain_matters_p( ( tree_node *)*v8) ) 
if ( ( _DWORD)v10 != 2 && !v19 && !lang_hooks_0.tree_inlining.tree_chain_matters_p( ( tree_node *)*v8) ) 
if ( statement_code_p( ( tree_code)*( ( unsigned __int8 *)*v8 + 16)) && ( *( ( _BYTE *)*v8 + 19) & 4) == 0 ) 
rtl_op = first_rtl_op( ( tree_code)v10); 
if ( !statement_code_p( ( tree_code)v10) ) 
v22 = lang_hooks_0.tree_inlining.walk_subtrees( ( tree_node **)v8, &subtrees, func, v20, htab_); 
v22 = lang_hooks_0.tree_inlining.walk_subtrees( ( tree_node **)v8, &subtrees, func, v20, htab_); 
if ( !general_operand( x, ( machine_mode)( unsigned __int8)BYTE2( *( _DWORD *)x)) ) 
v5 = want_to_gcse_p_test_insn; 
if ( !want_to_gcse_p_test_insn ) 
want_to_gcse_p_test_insn = make_insn_raw( v7); 
want_to_gcse_p_test_insn[1] = 0LL; 
ggc_add_rtx_root( &want_to_gcse_p_test_insn, 1); 
v5 = want_to_gcse_p_test_insn; 
v8 = want_to_gcse_p_test_insn; 
*( _QWORD *)( *( _QWORD *)&want_to_gcse_p_test_insn[2] + 16LL) = x; 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
v3 = spelling_0; 
if ( spelling_base < spelling_0 ) 
v3 = spelling_0; 
diagnostic_for_decl( decl, msgid, ( va_list_0 *)va, 1); 
s.args_ptr = ( va_list_0 *)va; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
*( _OWORD *)&s.begin_diagnostic = *( _OWORD *)&global_dc->begin_diagnostic; 
v10 = gen_lowpart( ( machine_mode)*( ( unsigned __int8 *)op + 2), v8); 
v11 = force_reg( ( machine_mode)BYTE2( v6), op); 
if ( **( _WORD **)&this_insn_0[2] == 39 ) 
if ( multiple_sets( this_insn_0) ) 
v5 = *( unsigned int **)( *( _QWORD *)&this_insn_0[2] + 8LL); 
v5 = *( unsigned int **)( *( _QWORD *)&this_insn_0[2] + 8LL); 
v13 = ( ( unsigned int)( mode_class_0[v11] - 5) < 2) + 1; 
v21 = _mm_add_epi32( _mm_shuffle_epi32( _mm_cvtsi32_si128( v19), 80), ( __m128i)xmm*(short *)0x641440); 
v23 = ( __m128i)_mm_shuffle_pd( ( __m128d)regs_live, ( __m128d)xmm*(short *)0x675E90, 2); 
si128 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641420); 
v27 = _mm_load_si128( ( const __m128i *)&xmm*(short *)0x641450); 
rtx base_term; // rax 
|| ( base_term = find_base_term( v19)) == 0LL 
|| ( v29 = *( _DWORD *)base_term, ( _WORD)v29 != 67) && ( ( _WORD)v29 != 68 || ( v29 & 0x4000000) == 0) ) 
( machine_mode)*( ( unsigned __int8 *)x + 2), 
( machine_mode)*( ( unsigned __int8 *)mem + 2)) ) 
if ( ( ( _DWORD)v4 == 16 || ( _DWORD)v4 == 22 || ( mode_class_0[v4] | 4) == 5) && computed >= 33 ) 
*( _OWORD *)&reg_alloc_order[v3] = xmm*(short *)0x67A650; 
*( _OWORD *)&reg_alloc_order[v3] = xmm*(short *)0x67A650; 
*( _OWORD *)&reg_alloc_order[v12 + 4] = xmm*(short *)0x67A660; 
*( _OWORD *)&reg_alloc_order[v12 + 4] = xmm*(short *)0x67A660; 
*( _OWORD *)&reg_alloc_order[v3] = xmm*(short *)0x67A670; 
*( _OWORD *)&reg_alloc_order[v3] = xmm*(short *)0x67A670; 
*( _OWORD *)&reg_alloc_order[v3 + 4] = xmm*(short *)0x67A680; 
*( _OWORD *)&reg_alloc_order[v3 + 4] = xmm*(short *)0x67A680; 
*( _OWORD *)&reg_alloc_order[v14] = xmm*(short *)0x67A690; 
*( _OWORD *)&reg_alloc_order[v14] = xmm*(short *)0x67A690; 
*( _OWORD *)&reg_alloc_order[v14 + 4] = xmm*(short *)0x67A6A0; 
*( _OWORD *)&reg_alloc_order[v14 + 4] = xmm*(short *)0x67A6A0; 
if ( xexit_cleanup ) 
sprintf( xstrerror_buf, aUndocumentedEr, ( unsigned int)errnum); 
if ( ( unsigned int)v1 <= 0xFF && ( sch_istable[( unsigned __int8)v1] & 0xAC) != 0 ) 
timevar_push( TV_LEX_0); 
v1 = cpp_type2name( ( cpp_ttype)v0); 
timevar_pop( TV_LEX_0); 
